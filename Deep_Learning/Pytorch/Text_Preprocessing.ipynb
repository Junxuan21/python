{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cB6bFrLchQ1W"
   },
   "source": [
    "## Text Preprocessing Practice in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "iwiA9ndchT42",
    "outputId": "96da4553-e2d6-4205-e7a3-602a069c3847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re                      # Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAweA95FhT-b"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df_pd = pd.read_csv(\"C:/Your/files/textdata.csv\", encoding = 'utf-8', header = None)\n",
    "df_np = np.asarray(pd.read_csv(\"C:/Your/files/textdata.csv\", encoding ='utf-8', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTj84AzehmDi"
   },
   "source": [
    "- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TAUeklBPhmvD",
    "outputId": "49c8c255-4a45-45bb-981f-bdafccde6bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Hello world!', 'I am craving for coffee before school!']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \" Hello world! I am craving for coffee before school! \"\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJqD8JK-hokA"
   },
   "outputs": [],
   "source": [
    "def tokenization_s(sentences): # same can be achieved for words tokens\n",
    "    s_new = []\n",
    "    for sent in (sentences[:][0]): #For NumpY = sentences[:]\n",
    "        s_token = sent_tokenize(sent)\n",
    "        if s_token != '':\n",
    "            s_new.append(s_token)\n",
    "            \n",
    "    return s_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kPUeTJfh3w2"
   },
   "source": [
    "- Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xap_8FvJh4RV"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "\n",
    "    for x in (text[:][0]): \n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
    "        new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "        new_text = new_text.lower() # lower case, .upper() for upper          \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "            \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2WIiJRsiAAO"
   },
   "source": [
    "- Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i7fyT2BniAVK",
    "outputId": "1820a72d-cc82-436e-a392-9a9d83cfe4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'kind', 'of', 'coffee', 'do', 'you', 'like', '?']\n"
     ]
    }
   ],
   "source": [
    "sentences = \" What kind of coffee do you like? \"\n",
    "words = word_tokenize(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50MDkgs6iCfA"
   },
   "outputs": [],
   "source": [
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:][0]):  # for NumPy = words[:]\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "            \n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AEYUr3PCiGcD",
    "outputId": "1d3a6106-5d35-435e-cb21-cbe140f194e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization_w(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UflsivfciTmk"
   },
   "source": [
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFg8JwajiUEt"
   },
   "outputs": [],
   "source": [
    "# use Snowball Stemmer from nltk.stem library\n",
    "snowball = SnowballStemmer(language = 'english')\n",
    "def stemming(words):\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in (words[:][0])]\n",
    "    new.append(stem_words)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yNyYRj_fidf0",
    "outputId": "3ab1a3c6-9eb4-4095-c194-41c26f9bc013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['y']]\n"
     ]
    }
   ],
   "source": [
    "test = ['You like Boba. Why not we go for it today!']\n",
    "test_pd = pd.DataFrame(test)  # makes this into a panda data frame\n",
    "clean_test = preprocess(test_pd) # removes punctuation, see above\n",
    "clean_words = tokenization_w(clean_test) # word tokenization\n",
    "stem_test = stemming(clean_words) # stemming similar words\n",
    "print(stem_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AblMPFDUiomA"
   },
   "source": [
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVWT5-FMiftg"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(words):\n",
    "    new = []\n",
    "    lem_words = [lemmatizer.lemmatize(x) for x in (words[:][0])]\n",
    "    new.append(lem_words)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4SYQXPI5iMfh",
    "outputId": "d1a1de98-39ad-45ed-9967-8843b1acd75c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['y']]\n"
     ]
    }
   ],
   "source": [
    "lemtest = lemmatization(clean_words)\n",
    "print(lemtest)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TextPreprocessing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
