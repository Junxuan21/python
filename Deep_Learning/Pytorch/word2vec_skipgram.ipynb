{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_slc0gil",
    "id": "1C259CBE4A394E02B3454D9680B0536A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word Embedding\n",
    "\n",
    "\n",
    "In this notebook, I will present and practice a simple, probabilistic method by [Mikolov et al., 2013] : [**Word2Vec**](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf). Word2Vec and other word embedding tools is to solve OneHot to represent word as well as the relationships. It represent every word as a fixed length vector and through traning, these vectors can have semantic meanings. In order to compute the language model, we need to calculate the probability of words and the conditional probability of a word given other few words.\n",
    "\n",
    "Word2vec is a software package that actually includes :\n",
    "\n",
    "- 2 algorithms: **[Continuous Bag-Of-Words (CBOW)](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)** and **[Skip-Gram](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)**\n",
    "\n",
    "**CBOW** aims to predict a center word from the surrounding context in terms of word vectors. It assumes the center $w_c$ is generated by context $P(w_c\\mid \\mathcal{W}_o)$, where $\\mathcal{W}_o$ is the set of context words.\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5mjt4r02n.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "\n",
    "\n",
    "**Skip-Gram** does the opposite, assume the surrounding words $w_o$ are generated by center $w_c$, that is $P(w_o\\mid w_c)$ and predicts the distribution (probability) of context words from a center word.\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5mjsq84o9.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "\n",
    "- 2 training methods: **[Negative Sampling](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)** and **[Hierarchical Softmax](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)**\n",
    "\n",
    "**Negative Sampling** defines an objective by sampling negative examples, while **H-softmax** defines an objective using an efficient. In practice, hierarchical softmax tends to be better for infrequent words, while negative sampling works better for frequent words and lower dimensional vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAAAD7AB4E5A4DFA8689C13D4CA97029",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### This notebook will focus on Skip-Gram implementation and Negative Sampling to train Word2Vec model using PTB dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "graffitiCellId": "id_y7ocw2l",
    "id": "8627003642CB441780806CBC552BFAC1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ube5b27",
    "id": "DD9999F086964C808616928EC7B736C0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### PTB dataset\n",
    "\n",
    "Simply putÔºåWord2Vec can learn how to map discrete words to vectors in continuous space from a large corpus while keeping its semantic meaning. \n",
    "\n",
    "So to train our Word2Vec model, in this notebook I will use a NLP corpus called PTB [PTB (Penn Tree Bank)](https://catalog.ldc.upenn.edu/LDC99T42). PTB is a commonly-used small size corpus, coming from articles on Wall Street Journal, including training set, test set and validation set. I will train the word embedding on PTB train set.\n",
    "\n",
    "- load dataset : take train file `ptb.train.txt` as an exampleÔºö\n",
    "```\n",
    "aer banknote berlitz calloway centrust cluett fromstein gitano guterman ...\n",
    "pierre  N years old will join the board as a nonexecutive director nov. N \n",
    "mr.  is chairman of  n.v. the dutch publishing group \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "graffitiCellId": "id_9374ybr",
    "id": "FF5B1C79764A4EA8AA61C3DE984CBA0D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 42068\n",
      "# tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "# tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
      "# tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "with open('/home/kesci/input/ptb_train1020/ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines() # separated by line\n",
    "    raw_dataset = [st.split() for st in lines] # split the words in sentence by space\n",
    "\n",
    "print('# sentences: %d' % len(raw_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5037DA4EE89D42EE9E21DD7567CF3C93",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_whcovuv",
    "id": "4694FB2B910840BB8C65162A14881EB0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This part illustrates the process how to split text data into tokens, and then map these discrete tokens to index in our vocab. \n",
    " \n",
    " - Split sentence into tokens\n",
    " \n",
    " For each sentence, we first split it into a list of tokens. A token is a data point the model will train and predict. \n",
    " \n",
    " \n",
    " - Create word index\n",
    "\n",
    " The string type of the token is inconvenient to be used by models, which take numerical inputs. So we need a dictionary, often called vocabulary as well, to map string tokens into numerical indices starting from 0. To do so, we first count the unique tokens in all documents, called corpus, and then assign a numerical index to each unique token according to its frequency. In our vocab, we filter out low-freq words to reduce the complexity. \n",
    " \n",
    " In some cases, we also use special tokens, which are ‚Äú<unk>‚Äù, ‚Äú<pad>‚Äù, ‚Äú<bos>‚Äù and ‚Äú<eos>‚Äù "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8C55AA7F69547C68C7434006ADA4B04",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print length and first 5 words of first 3 sents\n",
    "# eos token as '' Ôºåunk token as '' , replace number as 'N'\n",
    "for st in raw_dataset[:3]:\n",
    "    print('# tokens:', len(st), st[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "graffitiCellId": "id_u6zhq97",
    "id": "70DD6E74F6854C289BA21B367D2397B4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 887100'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = collections.Counter([tk for st in raw_dataset for tk in st]) \n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items())) # filter low-freq words apprear less than 5 times\n",
    "\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)} # map token word with index\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in raw_dataset] # create vocab with idx of token words in raw_dataset \n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "'# tokens: %d' % num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_4zjy016",
    "id": "845942F28174462A87ADDBAE82957D15",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Subsampling\n",
    "\n",
    "Subsampling is a technique to remove some high-freq words like \"the\", \"a\", \"in\" in text data to improve model performance. As in some cases, these words just add unnecessary noise to the model and can distort it from finding a true signal. \n",
    "\n",
    "The detail of subsampling in word2vec is that the random removal of tokens is done before the corpus is processed into word-context pairs. (i.e. before word2vec is actually run) In practice, it is similar to filter out stop words, by performing a resampling to get a dataset, in which all words have a certain prob dropped out in training, which is:\n",
    "\n",
    "$$\n",
    "\n",
    "P(w_i)=\\max(1-\\sqrt{\\frac{t}{f(w_i)}},0)\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "where $f(w_i)$ is the frequency of word $w_i$ in subsampling  (a.k.a ${count(ùë§ùëñ)}$/ ${total_Count}$ ) and $t$ is a hyperparameter, a chosen threshold (set at $10^{‚àí4}$ in this notebook). So, it is only possible to discard $w_i$ in subsampling only when $f(w_i) > t$, and the higher freq of $w_i$, the higher the probability of being discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "graffitiCellId": "id_yg9kj6g",
    "id": "4B82B59FCC244E11AA7335909F6B588A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 375548\n",
      "# the: before=50770, after=2131\n",
      "# join: before=45, after=45\n"
     ]
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    '''\n",
    "    @params:\n",
    "        idx: subscript of the word token\n",
    "    @return: True/False drop the token or not\n",
    "    '''\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "print('# tokens: %d' % sum([len(st) for st in subsampled_dataset]))\n",
    "\n",
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (token, \n",
    "    sum([st.count(token_to_idx[token]) for st in dataset]), \n",
    "    sum([st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
    "\n",
    "print(compare_counts('the'))\n",
    "print(compare_counts('join'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B31A3D7FD11749DD8C82A617D95A79C6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can see that after subsampling, the freq of word \"the\" has reduced greatly and that of \"join\" hasn't changed. And we didn't throw away token from vocab, instead just discard randomly in the sentences. So this function works well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_u88x2eb",
    "id": "18BE417013E049A3B5A3EC7B607EE554",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Extract center word and contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "graffitiCellId": "id_a0ayzaz",
    "id": "56C4719FE9A64B468F9B0081DD25ABBC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [1, 3]\n",
      "center 3 has contexts [2, 4]\n",
      "center 4 has contexts [2, 3, 5, 6]\n",
      "center 5 has contexts [3, 4, 6]\n",
      "center 6 has contexts [5]\n",
      "center 7 has contexts [8, 9]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    '''\n",
    "    @params:\n",
    "        dataset: all token from original dataset by index\n",
    "        max_window_size: max context window size\n",
    "    @return:\n",
    "        centers: list of center words\n",
    "        contexts: list of context words matched with centers\n",
    "    '''\n",
    "    centers, contexts = [], []\n",
    "    \n",
    "    for st in dataset:\n",
    "        if len(st) < 2:  # at least 2 words to create center-context\n",
    "            continue\n",
    "        centers += st\n",
    "        \n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size) # random choose context window size\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)  # remove center from context words\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    \n",
    "    return centers, contexts \n",
    "\n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n",
    "\n",
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_161ief2",
    "id": "3853063E40E14F7DA7DCA410A0A3C617",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "*Note: The batch size data processing requires negtive sampling, which will be implemented later in this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_s7nai85",
    "id": "E0C8301D7DB340CDABD81A23520A340D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Skip-Gram Model\n",
    "\n",
    "In this model, each word is represented as two $d$ -dim vector to calculate condition probability. \n",
    "\n",
    "If the word has an index of $i$ in the vocab, when it is the center, the vector is $\\boldsymbol{v}_i\\in\\mathbb{R}^d$Ôºåwhile when it is in the context, the vector is  $\\boldsymbol{u}_i\\in\\mathbb{R}^d$ \n",
    "\n",
    "Let the index of the center word $w_c$ in the vocab is $c$Ôºåcontext $w_o$ indices are $o$ , the condition probability of generating context $w_o$ given center $w_c$ is:\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "P(w_o\\mid w_c)=\\frac{\\exp(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}{\\sum_{i\\in\\mathcal{V}}\\exp(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}\n",
    "\n",
    "\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_j38gtz0",
    "id": "C7B5BB9D5FD54489BF6E3F42E31B835F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- PyTorch built-in embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "graffitiCellId": "id_7he6kmh",
    "id": "68DF20EE03824200887D18AA3363E7F9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.3367,  1.2066,  0.1861, -0.7905],\n",
      "        [-1.4386,  0.2864,  0.8527,  1.6524],\n",
      "        [ 0.4732,  0.1352, -0.5633, -0.3785],\n",
      "        [ 1.5265, -1.0352,  1.1603,  1.3615],\n",
      "        [-0.7230, -1.0227, -0.8679, -0.5138],\n",
      "        [ 0.6433,  0.9655,  0.2737,  0.1145],\n",
      "        [-0.2912, -1.1775, -1.3287, -0.1709],\n",
      "        [ 1.7273,  0.9296, -0.8355, -0.1213],\n",
      "        [-1.3989, -0.0632,  1.0090, -1.7358],\n",
      "        [ 0.5273, -1.2383,  0.7594, -0.8828]], requires_grad=True)\n",
      "tensor([[[-1.4386,  0.2864,  0.8527,  1.6524],\n",
      "         [ 0.4732,  0.1352, -0.5633, -0.3785],\n",
      "         [ 1.5265, -1.0352,  1.1603,  1.3615]],\n",
      "\n",
      "        [[-0.7230, -1.0227, -0.8679, -0.5138],\n",
      "         [ 0.6433,  0.9655,  0.2737,  0.1145],\n",
      "         [-0.2912, -1.1775, -1.3287, -0.1709]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# used to get token by index\n",
    "embed = nn.Embedding(num_embeddings=10, embedding_dim=4)  # define a layer with num of token and embedding dim\n",
    "print(embed.weight)\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)\n",
    "print(embed(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_hevvqgv",
    "id": "8421DAEC55314EBE80D2EC59EAB73099",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- PyTorch built-in batch matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_8i4omp4",
    "id": "12B88D54095F48F58397626D8F1935E3",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones((2, 1, 4))\n",
    "Y = torch.ones((2, 4, 6))\n",
    "print(torch.bmm(X, Y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D24A576D8539475987487C2B6B57D504",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now let's define forward function for our Skip Gram Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_x6q9jp9",
    "id": "AAA4F7E268764809836AAADD7FD2A8AE",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    '''\n",
    "    @params:\n",
    "        center: center indexÔºåshape (n, 1) int tensor\n",
    "        contexts_and_negatives: indices of contexts and negativesÔºåshape (n, m) int tensor\n",
    "        embed_v: embedding layer for center\n",
    "        embed_u: embedding layer for contexts\n",
    "    \n",
    "    @return:\n",
    "        pred: batch matrix multiplication of center and contexts to calculate prob later p(w_o|w_c)\n",
    "    '''\n",
    "    v = embed_v(center) # shape of (n, 1, d)\n",
    "    u = embed_u(contexts_and_negatives) # shape of (n, m, d)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) # reshape bmm (n, 1, d) to (n, d, m)\n",
    "    \n",
    "    return pred # shape of (n, 1, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_rb8yuyq",
    "id": "8FE236CF785F474F9BBB4BB3D88AA4CC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Negative Sampling\n",
    "\n",
    "As Softmax computation for all tokens in a large NLP corpus would be too expensive, we will solve this issue using negative sampling on our skip-gram model. For every training step, instead of looping over the entire vocabulary, we can just sample several negative examples! We \"sample\" from a noise distribution (Pn(w)) whose probabilities match the ordering of the frequency of the vocabulary.\n",
    "\n",
    "Negative sampling idea is based on the concept of noise contrastive estimation, which indicates that a good model should differentiate fake signal from the real one by the means of logistic regression. Also, the motivation behind negative sampling objective is similar to stochastic gradient descent: instead of changing all of the weights each time with taking into account all of the thousands of observations we‚Äôre having, we‚Äôre using only K of them and increasing computational efficiency.\n",
    "\n",
    "Negative Sampling uses this formula to simulate condition probability: $P(w_o\\mid w_c)=\\frac{\\exp(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}{\\sum_{i\\in\\mathcal{V}}\\exp(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}$Ôºö\n",
    "\n",
    "$$P(w_o\\mid w_c)=P(D=1\\mid w_c,w_o)\\prod_{k=1,w_k\\sim P(w)}^K P(D=0\\mid w_c,w_k)$$\n",
    "\n",
    "Where $P(D=1\\mid w_c,w_o)=\\sigma(\\boldsymbol{u}_o^\\top\\boldsymbol{v}_c)$Ôºå$\\sigma(\\cdot)$ is the sigmoid function\n",
    "\n",
    "For a pair of center and contexts,  we randomly sample $K$ negatives from vocab (here, $K=5$), a common practice is to set the negative sampling prob $P(w)$ to the $0.75$ power of the ratio of word freq $w$ to the total word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_st81puo",
    "id": "C6A1BDB699EB49AA8EEB49C295EACBF7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    '''\n",
    "    @params:\n",
    "        all_contexts: [[w_o1, w_o2, ...], [...], ... ]\n",
    "        sampling_weights: all word's negative sampling probability\n",
    "        K: num of random samples\n",
    "    @return:\n",
    "        all_negatives: [[w_n1, w_n2, ...], [...], ...]\n",
    "    '''\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                # generate indices of k negatives according to each word's sampling_weights\n",
    "                # use larger k for computation efficiency\n",
    "                i, neg_candidates = 0, random.choices(\n",
    "                    population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            \n",
    "            # negatives cannot be contexts\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_g7431va",
    "id": "43F68EDDF24849BE8791D07F0618F9DD",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* Apart from Negative Sampling, **Hierarchical softmax** can also be used to solve the computation problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_jhrav4e",
    "id": "F991A9C6E28C42848DD393E4F891D49D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Load batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "graffitiCellId": "id_shkut5w",
    "id": "FF3BDA1024C94EB3A760EBA5FD583B4A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "def batchify(data):\n",
    "    '''\n",
    "    param collate_fn for DataLoader\n",
    "    @params:\n",
    "        data: list length of batch_size, each element is output from __getitem__\n",
    "    @outputs:\n",
    "        batch: output (centers, contexts_negatives, masks, labels) tuple\n",
    "            centers: idx of center, shape (n, 1) int tensor\n",
    "            contexts_negatives: idx of contexts and negativesÔºåshape (n, m) int tensor\n",
    "            masks: mask corresponding to padding in loss function, shape is (n, m), 0/1 int tensor\n",
    "            labels: label for centersÔºåshape is (n, m), 0/1 int tensor\n",
    "    '''\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)] # use mask variable to cover padding in loss function\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "        batch = (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n",
    "    return batch\n",
    "\n",
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify, \n",
    "                            num_workers=num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_hf5q360",
    "id": "3AC214600B9F4A73B87CD4B57384BB3B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Model Training\n",
    "\n",
    "- Loss function\n",
    "\n",
    "After applying the negative sampling, we can use the log-equivalent form of MLE (Maximum Likelihood Estimation) to define the loss function as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "\\sum_{t=1}^T\\sum_{-m\\le j\\le m,j\\ne 0} [-\\log P(D=1\\mid w^{(t)},w^{(t+j)})-\\sum_{k=1,w_k\\sim P(w)^K}\\log P(D=0\\mid w^{(t)},w_k)]\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "According to its definition, we then use the binary cross entropy loss function to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "graffitiCellId": "id_ap2woj6",
    "id": "6BDDED9801FF43B98CD51ED86B12D450",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8740, 1.2100])\n",
      "0.8740\n",
      "1.2100\n"
     ]
    }
   ],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: the prob of D=1 after sigmoid function\n",
    "            targets: 0/1 vectorÔºå1 is backgroundÔºå0 is negative\n",
    "        @return:\n",
    "            res: the avg loss per label\n",
    "        '''\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        res = res.sum(dim=1) / mask.float().sum(dim=1)\n",
    "        return res\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]]) # label 1: context; 0: negatives\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # mask variable\n",
    "print(loss(pred, label, mask))\n",
    "\n",
    "def sigmd(x):\n",
    "    return - math.log(1 / (1 + math.exp(-x)))\n",
    "    \n",
    "print('%.4f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4)) # 1-sigmoid(x) = sigmoid(-x)\n",
    "print('%.4f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "graffitiCellId": "id_k9ax2h6",
    "id": "EE565E07272B40C691196EED5695BC5D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "embed_size = 100\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_9ahavii",
    "id": "F178519AEE504029B3603E4D006BA839",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "graffitiCellId": "id_9fx6rj4",
    "id": "2DA9D996EE3E44B49DE0D2FAC370E1DD",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cpu\n",
      "epoch 1, loss 1.98, time 672.59s\n",
      "epoch 2, loss 0.62, time 670.56s\n",
      "epoch 3, loss 0.45, time 670.54s\n",
      "epoch 4, loss 0.40, time 671.01s\n",
      "epoch 5, loss 0.37, time 669.49s\n"
     ]
    }
   ],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            \n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            \n",
    "            l = loss(pred.view(label.shape), label, mask).mean() # avg loss for one batch\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        \n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))\n",
    "\n",
    "train(net, 0.01, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_bw1dtd1",
    "id": "A5DAB2B7CC6A41668D2F5A0061C54728",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "graffitiCellId": "id_wm2rrhl",
    "id": "838B4878856C457889DAA35A29948029",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.453: counseling\n",
      "cosine sim=0.441: software\n",
      "cosine sim=0.434: center\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    '''\n",
    "    @params:\n",
    "        query_token: token\n",
    "        k: num of similar words\n",
    "        embed: pre-trained word vector\n",
    "    '''\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt() # eps for smoothing\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # remove input word\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "        \n",
    "get_similar_tokens('chip', 3, net[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "785F38C28C7046178662366D6B21549E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.645: exchange\n",
      "cosine sim=0.627: shares\n",
      "cosine sim=0.587: friday\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('stock', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "20A4C97CE3C9461B8AA65E62B851F9DF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.399: glamorous\n",
      "cosine sim=0.387: advised\n",
      "cosine sim=0.380: affected\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('girl', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "27F75BB03FA646F1844957A7AA71ABFE",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.526: says\n",
      "cosine sim=0.513: <unk>\n",
      "cosine sim=0.470: politburo\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('man', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "0710927B9705435F83F5CD71C3239AE9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.409: magazines\n",
      "cosine sim=0.394: coffee\n",
      "cosine sim=0.394: personnel\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('shopping', 3, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B108429B80D94918909C28F298E9AD78",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "From above examples, we can see that the language model is not perfect and not generalized to other themes due to my CPU trainging limit and the size and content of our text corpus. However, it gave us some sort of sense of how word embedding like word2vec can achieve in NLP. In future, it is good to start from here and implement more complex word embedding on larger corpus as well as to explore the combinnation with other modern advanced language models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_g9tjvnn",
    "id": "17C8660147A846FEB9389CEF51AEC536",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### References\n",
    "* [Dive into Deep Learning](https://d2l.ai/chapter_natural-language-processing/word2vec.html)\n",
    "* [CS224n: Natural Language Processing with Deep Learning 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)\n",
    "* [Dive-into-DL-PyTorch on GitHub](https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/code/chapter10_natural-language-processing/10.3_word2vec-pytorch.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
