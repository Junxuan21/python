{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_lkex7dv",
    "id": "37A7E3ADB1994B758F375C5C3A7E2030",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Image Classification（CIFAR-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_38m3agm",
    "id": "0F4A7F611AA246518453975A0CA577CA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This notebook explores the image classification task on Kaggle called [CIFAR-10](https://www.kaggle.com/c/cifar-10). For practice purpose, I only use the mini dataset of the original dataset and a simple [ResNet-18 model](https://arxiv.org/abs/1512.03385) based on Kaiming He's [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/abs/1512.03385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "graffitiCellId": "id_xyhmvui",
    "id": "935DE49BF1454AE499FC879EE46D32A8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "graffitiCellId": "id_mjtodka",
    "id": "866894DD50BB44BCA7D0A9C3600515B8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_unbk8ki",
    "id": "58073EA55E7F432EA640CF1522235159",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Dataset is divided to training set and test set. Training set contains 50,000 images. Test set contains 300,000 images. All PNG format with 32\\*32 pixel, and 3 color channels RGB. The original dataset's images cover 10 categories: plane, car, bird, cat, deer, dog, frog, horse, ship and truck.\n",
    "\n",
    "To save the time and costs, this notebook only uses the mini set of the original dataset with 80 training samples, and 100 test samples under \"train_tiny\" and \"test_tiny\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_km2fwpa",
    "id": "69D16EA3180142EF868392C4C5362322",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "graffitiCellId": "id_ki9x99g",
    "id": "639B99C0F85A493086469FE70C801094",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(40),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "trainset = torchvision.datasets.ImageFolder(root='/home/kesci/input/CIFAR102891/cifar-10/train'\n",
    "                                            , transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "graffitiCellId": "id_3585c9q",
    "id": "30B45E88083C4DF989B85CC3E860C5CD",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "graffitiCellId": "id_13vulw7",
    "id": "AB757AC8BFF3432F8C80A137D762D771",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46735394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [d[0].data.cpu().numpy() for d in trainset]\n",
    "np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_u9936db",
    "id": "AFD88C0E557E4D99831625B9DFF777BF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23921667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_c9e4spf",
    "id": "199E596E421E45148A91745BBC1B706C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # pad first then crop to 32*32\n",
    "    transforms.RandomHorizontalFlip(),  # half flip, half do not\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4731, 0.4822, 0.4465), (0.2212, 0.1994, 0.2010)), # mean&std for standardization in R G B channel\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4731, 0.4822, 0.4465), (0.2212, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_3hupcua",
    "id": "7A5D89FDA67B409CBD509498688062F0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = '/home/kesci/input/CIFAR102891/cifar-10/train'\n",
    "test_dir = '/home/kesci/input/CIFAR102891/cifar-10/test'\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=test_dir, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'forg', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_5hudo8c",
    "id": "05E364AFFFA84D298131742D605FB0A7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Define Model\n",
    "\n",
    "I am gonna use ResNet-18 model in this project. The ResNet or Residual Network is developed in Kaiming He's \"Deep Residual Learning for Image Recognition\" \n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5x9kusfpk.png?imageView2/0/w/960/h/960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "graffitiCellId": "id_0j6bi2e",
    "id": "2DC7354A46994C5281CB8C090150AE51",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):  # build ResNet using torch.nn.Module\n",
    "\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.left = nn.Sequential( # use Sequential() to add layers based on above structure\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False), # first conv layer\n",
    "            nn.BatchNorm2d(outchannel), # batch norm for conv layer output\n",
    "            nn.ReLU(inplace=True), # activation use ReLU\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel) # batch norm for 2nd conv layer\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential() # for simple and quick use in model building \n",
    "        if stride != 1 or inchannel != outchannel: # check if Y = self.left(X) has the same shape as X\n",
    "            self.shortcut = nn.Sequential( \n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x): # stack two blocks，add ReLu in the end\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential( # use three 3x3 kernel size instead of 7x7, reduce params\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        ) \n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)  # 1st Res Block stride is from make_layer fn's param: stride\n",
    "        # the rest (num_blocks-1) Res Block has stride: 1\n",
    "        layers = []\n",
    "        for stride in strides: # loop to add blocks\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_iq3nppt",
    "id": "534E5D09D3B54639AE6D80EF9158EA74",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8B0ED73E06C64D928714D2DA2D54122E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training, Resnet-18!\n",
      "\n",
      "Epoch: 1\n",
      "[epoch:1, iter:1] Loss: 596.250 | Acc: 9.766% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 2\n",
      "[epoch:2, iter:177] Loss: 344.395 | Acc: 51.562% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 3\n",
      "[epoch:3, iter:353] Loss: 264.417 | Acc: 65.625% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 4\n",
      "[epoch:4, iter:529] Loss: 231.419 | Acc: 69.141% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 5\n",
      "[epoch:5, iter:705] Loss: 197.945 | Acc: 76.953% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 6\n",
      "[epoch:6, iter:881] Loss: 132.735 | Acc: 82.422% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 7\n",
      "[epoch:7, iter:1057] Loss: 141.950 | Acc: 81.250% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 8\n",
      "[epoch:8, iter:1233] Loss: 136.229 | Acc: 81.641% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 9\n",
      "[epoch:9, iter:1409] Loss: 132.046 | Acc: 78.906% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 10\n",
      "[epoch:10, iter:1585] Loss: 97.200 | Acc: 85.938% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 11\n",
      "[epoch:11, iter:1761] Loss: 94.208 | Acc: 87.500% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 12\n",
      "[epoch:12, iter:1937] Loss: 99.082 | Acc: 85.547% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 13\n",
      "[epoch:13, iter:2113] Loss: 94.444 | Acc: 89.453% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 14\n",
      "[epoch:14, iter:2289] Loss: 82.347 | Acc: 90.234% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 15\n",
      "[epoch:15, iter:2465] Loss: 106.834 | Acc: 84.766% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 16\n",
      "[epoch:16, iter:2641] Loss: 90.226 | Acc: 86.719% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 17\n",
      "[epoch:17, iter:2817] Loss: 89.968 | Acc: 88.672% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 18\n",
      "[epoch:18, iter:2993] Loss: 76.781 | Acc: 88.672% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 19\n",
      "[epoch:19, iter:3169] Loss: 79.292 | Acc: 89.062% \n",
      "Training Finished, TotalEPOCH=20\n",
      "\n",
      "Epoch: 20\n",
      "[epoch:20, iter:3345] Loss: 60.729 | Acc: 92.969% \n",
      "Training Finished, TotalEPOCH=20\n"
     ]
    }
   ],
   "source": [
    "# mini dataset use cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define hyperparams\n",
    "EPOCH = 20  \n",
    "pre_epoch = 0  \n",
    "LR = 0.1       \n",
    "\n",
    "net = ResNet18().to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # cross entropy for classification\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) \n",
    "# use mini-batch momentum-SGD and L2 regularization\n",
    "\n",
    "# train model\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training, Resnet-18!\")\n",
    "    \n",
    "    num_iters = 0\n",
    "    for epoch in range(pre_epoch, EPOCH):\n",
    "        print('\\nEpoch: %d' % (epoch + 1))\n",
    "        net.train() # tell BatchNorm in traning mode\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0\n",
    "        for i, data in enumerate(trainloader, 0): \n",
    "            num_iters += 1\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # reset grad to zero\n",
    "\n",
    "            # forward + backward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1) # use max value in output as prediction\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # print loss and accuracy\n",
    "            if i/20 == 0:\n",
    "                print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "                    % (epoch + 1, num_iters, sum_loss / (i + 1), 100. * correct / total))\n",
    "\n",
    "        print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)\n",
    "    \n",
    "        # print(\"Waiting Test!\")\n",
    "        # with torch.no_grad():\n",
    "        #     correct = 0\n",
    "        #     total = 0\n",
    "        #     for data in testloader:\n",
    "        #         net.eval() # tell BatchNorm in eval mode\n",
    "        #         images, labels = data\n",
    "        #         images, labels = images.to(device), labels.to(device)\n",
    "        #         outputs = net(images)\n",
    "                \n",
    "        #         _, predicted = torch.maxtorch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A6EEB0B50DC54D8EBF8B693F7158C9E1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## we can clearly see that the loss decreased and accuracy imrpoved as training more epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
