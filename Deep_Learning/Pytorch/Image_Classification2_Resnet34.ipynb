{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_9pjlvx1",
    "id": "36C1910C967E43B1B111E5E53259C1E2",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##  Dog Breed Identification (ImageNet Dogs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_nnzrhp8",
    "id": "C843FCEE167D421B8A497FB92B60FAB6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This notebook is for Kaggle's [Dog Breed Identification](https://www.kaggle.com/c/dog-breed-identification) contest and can be found here: https://www.kaggle.com/c/dog-breed-identification \n",
    "\n",
    "The purposeis to identify 120 different breeds using data from the famous ImageNet Dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "graffitiCellId": "id_fypnm12",
    "id": "2B43D410B3AE4BF18C12E4719E86D5C2",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FE7EE6AB8B95437784CEDB3668B6FC69",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(210)\n",
    "torch.manual_seed(21)\n",
    "torch.cuda.manual_seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAC79BE16D3E4EEC85AE5824764583DF",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Download and load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAABF1FBFDA740CC8C4FA4C2AB4EF063",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Downloaded dataset look like this:\n",
    "```\n",
    "| Dog Breed Identification\n",
    "    | train\n",
    "    |   | 000bec180eb18c7604dcecc8fe0dba07.jpg\n",
    "    |   | 00a338a92e4e7bf543340dc849230e75.jpg\n",
    "    |   | ...\n",
    "    | test\n",
    "    |   | 00a3edd22dc7859c487a64777fc8d093.jpg\n",
    "    |   | 00a6892e5c7f92c1f465e213fd904582.jpg\n",
    "    |   | ...\n",
    "    | labels.csv\n",
    "    | sample_submission.csv\n",
    "```\n",
    "\n",
    "The data is divided into a training set and testing set. The training set contains $10,222$ images and the testing set contains $10,357$ images. The images in both sets are in JPEG format. These images contain three RGB channels and they have different heights and widths. The file name of each image is a unique id. *labels.csv* contains the labels of the training set images. The file contains 10,222 rows, each row contains two columns, the first column is the image id, and the second column is the dog breed. There are 120 breeds of dogs in the training set. \n",
    "\n",
    "* Split the validation dataset from the training set to tune the hyperparams. After partitioning, the dataset should contain 4 parts: the partitioned training set, the partitioned validation set, the full training set, and the full test set\n",
    "\n",
    "* For 4 parts, create 4 folders: train, valid, train_valid, test. In the above folders, a folder is created for each category, and images belonging to the category are stored therein. The labels of the first three parts are known, so there are 120 subfolders each, and the labels of the test set are unknown, so only one subfolder named unknown is created to store all test data.\n",
    "\n",
    "\n",
    "So the sorted dataset structure would be like:\n",
    "\n",
    "```\n",
    "| train_valid_test\n",
    "    | train\n",
    "    |   | affenpinscher\n",
    "    |   |   | 00ca18751837cd6a22813f8e221f7819.jpg\n",
    "    |   |   | ...\n",
    "    |   | afghan_hound\n",
    "    |   |   | 0a4f1e17d720cdff35814651402b7cf4.jpg\n",
    "    |   |   | ...\n",
    "    |   | ...\n",
    "    | valid\n",
    "    |   | affenpinscher\n",
    "    |   |   | 56af8255b46eb1fa5722f37729525405.jpg\n",
    "    |   |   | ...\n",
    "    |   | afghan_hound\n",
    "    |   |   | 0df400016a7e7ab4abff824bf2743f02.jpg\n",
    "    |   |   | ...\n",
    "    |   | ...\n",
    "    | train_valid\n",
    "    |   | affenpinscher\n",
    "    |   |   | 00ca18751837cd6a22813f8e221f7819.jpg\n",
    "    |   |   | ...\n",
    "    |   | afghan_hound\n",
    "    |   |   | 0a4f1e17d720cdff35814651402b7cf4.jpg\n",
    "    |   |   | ...\n",
    "    |   | ...\n",
    "    | test\n",
    "    |   | unknown\n",
    "    |   |   | 00a3edd22dc7859c487a64777fc8d093.jpg\n",
    "    |   |   | ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8EE3AFAC71D143C98206EF76C4A7F576",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/kesci/input/Kaggle_Dog6357/dog-breed-identification' \n",
    "label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'  \n",
    "new_data_dir = './train_valid_test'  \n",
    "\n",
    "valid_ratio = 0.1  # for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "749C44BBB8434C16AC8EAEB22AA1C9ED",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))\n",
    "        \n",
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, new_data_dir, valid_ratio):\n",
    "    # load labels for training data\n",
    "    labels = pd.read_csv(os.path.join(data_dir, label_file))\n",
    "    id2label = {Id: label for Id, label in labels.values}  # (key: value): (id: label)\n",
    "\n",
    "    # shuffle traning data\n",
    "    train_files = os.listdir(os.path.join(data_dir, train_dir))\n",
    "    random.shuffle(train_files)    \n",
    "\n",
    "    valid_ds_size = int(len(train_files) * valid_ratio) \n",
    "    \n",
    "    for i, file in enumerate(train_files):\n",
    "        img_id = file.split('.')[0]  # file is string with id.jpg type\n",
    "        img_label = id2label[img_id]\n",
    "        if i < valid_ds_size:\n",
    "            mkdir_if_not_exist([new_data_dir, 'valid', img_label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, file),\n",
    "                        os.path.join(new_data_dir, 'valid', img_label))\n",
    "        else:\n",
    "            mkdir_if_not_exist([new_data_dir, 'train', img_label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, file),\n",
    "                        os.path.join(new_data_dir, 'train', img_label))\n",
    "        mkdir_if_not_exist([new_data_dir, 'train_valid', img_label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, file),\n",
    "                    os.path.join(new_data_dir, 'train_valid', img_label))\n",
    "\n",
    "    # test set\n",
    "    mkdir_if_not_exist([new_data_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(new_data_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E16AEEFA70874141961C0FD88895B9EF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reorg_dog_data(data_dir, label_file, train_dir, test_dir, new_data_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_nib4akb",
    "id": "6C567C4F30C3475DB4628406B698CBEB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_pdfj7om",
    "id": "750224C414DC431CB769A8702E255EAB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # scale size of 0.08~1 of original images,\n",
    "    # keep height and width ratio at 3/4~4/3，\n",
    "    # crop to 2248224 pixel new image\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0),  \n",
    "                                 ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    # flip half of the data\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # change params randomly to add noise to data \n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.ToTensor(), # change to tensor\n",
    "    # using mean(0.485, 0.456, 0.406) and std(0.229, 0.224, 0.225) normalize on channels\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "\n",
    "# no random noise on test set\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224), # crop the center square image with 224*224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_xtlsjze",
    "id": "52DB31C52D0E49C58B1269EA8D251F45",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train'),\n",
    "                                            transform=transform_train)\n",
    "valid_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'valid'),\n",
    "                                            transform=transform_test)\n",
    "train_valid_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train_valid'),\n",
    "                                            transform=transform_train)\n",
    "test_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'test'),\n",
    "                                            transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "13D59A2CAD1444AE84BAAC3C293EF681",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "train_valid_iter = torch.utils.data.DataLoader(train_valid_ds, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)  # shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1A0DA4055D14E17A3AD0C04C99E204A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Build Model\n",
    "\n",
    "Since this dataset belongs to a subset of the ImageNet dataset. We will use the fine-tuning method, selecting the pre-trained model on the complete ImageNet dataset to extract image features as input to a custom small-scale output network.\n",
    "\n",
    "Here, we use the pre-trained **ResNet-34 model** to directly reuse the input of the pre-trained model at the output layer, that is, the extracted features, and then we redefine the output layer. Then we only train the params of the redefined output layer. For the part used for feature extraction, we retain the params of the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "475191DACA434C1F865914AD665041FA",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_net(device): # build model using pre-trained resnet-34\n",
    "    finetune_net = models.resnet34(pretrained=False)  # get pre-trained resnet-34 model\n",
    "    finetune_net.load_state_dict(torch.load('/home/kesci/input/resnet347742/resnet34-333f7ec4.pth'))\n",
    "    for param in finetune_net.parameters():  # freeze params\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # the original finetune_net.fc is a FC with 512 input and 1000 output\n",
    "    # reset params in finetuen_net.fc to match with our features and labels\n",
    "    finetune_net.fc = nn.Sequential(\n",
    "        nn.Linear(in_features=512, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=120)  # 120 is output label class\n",
    "    )\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D5A6471E5B7C492C86D66BA578F7A1BE",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_loss_acc(data_iter, net, device): # calculate avg loss and accuracy on data_iter\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    is_training = net.training  # check if Bool net in train mode\n",
    "    net.eval()\n",
    "    l_sum, acc_sum, n = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "\n",
    "    net.train(is_training)  # reset net back to train/eval mode\n",
    "    \n",
    "    return l_sum / n, acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "01092780EA624512BCB57A85B5ADBC2A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period,\n",
    "          lr_decay):\n",
    "              \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.fc.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, n, start = 0.0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:  # decay lr every (lr_period) epoch\n",
    "            lr = lr * lr_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "                \n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "            \n",
    "        time_s = \"time %.2f sec\" % (time.time() - start)\n",
    "        \n",
    "        if valid_iter is not None:\n",
    "            valid_loss, valid_acc = evaluate_loss_acc(valid_iter, net, device)\n",
    "            epoch_s = (\"epoch %d, train loss %f, valid loss %f, valid acc %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n, valid_loss, valid_acc))\n",
    "        else:\n",
    "            epoch_s = (\"epoch %d, train loss %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n))\n",
    "                       \n",
    "        print(epoch_s + time_s + ', lr ' + str(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1B48EF6702B14C1E9FBC9E16BA3EE4A4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params setting\n",
    "num_epochs, lr_period, lr_decay = 20, 10, 0.1\n",
    "lr, wd = 0.03, 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9828385FE5B84A3A8E0246F2FC41DF55",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_net(device)\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "24E7689CCE9948E18114BDCEE88D7F1D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 3.231950, time 872.22 sec, lr 0.03\n",
      "epoch 2, train loss 1.471955, time 867.12 sec, lr 0.03\n",
      "epoch 3, train loss 1.274351, time 852.87 sec, lr 0.03\n",
      "epoch 4, train loss 1.208349, time 861.00 sec, lr 0.03\n",
      "epoch 5, train loss 1.157397, time 876.11 sec, lr 0.03\n",
      "epoch 6, train loss 1.116189, time 879.29 sec, lr 0.03\n",
      "epoch 7, train loss 1.113411, time 882.93 sec, lr 0.03\n",
      "epoch 8, train loss 1.085275, time 879.36 sec, lr 0.03\n",
      "epoch 9, train loss 1.070930, time 874.22 sec, lr 0.03\n",
      "epoch 10, train loss 1.027964, time 858.18 sec, lr 0.03\n",
      "epoch 11, train loss 0.931459, time 877.53 sec, lr 0.003\n",
      "epoch 12, train loss 0.891806, time 877.98 sec, lr 0.003\n",
      "epoch 13, train loss 0.885535, time 879.91 sec, lr 0.003\n",
      "epoch 14, train loss 0.870792, time 878.42 sec, lr 0.003\n",
      "epoch 15, train loss 0.878356, time 877.15 sec, lr 0.003\n",
      "epoch 16, train loss 0.847840, time 856.64 sec, lr 0.003\n",
      "epoch 17, train loss 0.873435, time 877.29 sec, lr 0.003\n",
      "epoch 18, train loss 0.844666, time 860.16 sec, lr 0.003\n",
      "epoch 19, train loss 0.885120, time 855.92 sec, lr 0.003\n",
      "epoch 20, train loss 0.876750, time 855.94 sec, lr 0.003\n"
     ]
    }
   ],
   "source": [
    "net = get_net(device)\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, device, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8454359E77EA4653B30FCA248E209666",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "38D8D088E69B48349899C01802C0439D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for X, _ in test_iter:\n",
    "    X = X.to(device)\n",
    "    output = net(X)\n",
    "    output = torch.softmax(output, dim=1)\n",
    "    preds += output.tolist()\n",
    "\n",
    "ids = sorted(os.listdir(os.path.join(new_data_dir, 'test/unknown')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "123E29CC1E1C4055890FC55ACFE4A84B",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write and save our model output\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
