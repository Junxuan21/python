{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_on4eyh2",
    "id": "9E73B21214794A1B8F92AF2169A6F409",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Text Sentiment Analysis - 2\n",
    "\n",
    "Text classification is a common task in natural language processing, which transforms a sequence of text of indefinite length into a category of text. It is similar to the image classification. The only difference is that, rather than an image, text classification's example is a text sentence.\n",
    "\n",
    "This notebook will focus on one of the sub problems in this field: using text sentiment classification to analyze the emotions of the text's author. This problem is also called sentiment analysis and has a wide range of applications. For example, we can analyze user reviews of products to obtain user satisfaction statistics in E-commerce, or analyze user sentiments about market conditions and use it to predict future trends in Finance.\n",
    "\n",
    "Following the first part, this second notebook will use pre-trained GloVe and **TextCNN (Convolutional Neural Network)** to determine whether a certain length of text sequence contains positive or negative emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A1256CCF9CFA4DF380CA46E53F7BDE74",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "graffitiCellId": "id_qpkl0v3",
    "id": "4C34DE8574664EBC876799AFFD97FACC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_75c1s0z",
    "id": "ECA9EC64CB6B4C0C8CCDB65ACD18D2BE",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The Sentiment Analysis Dataset\n",
    "\n",
    "We use [Stanford's Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) as the dataset for sentiment analysis. This dataset is divided into two datasets for training and testing purposes, each containing 25,000 movie reviews downloaded from IMDb. In each dataset, the number of comments labeled as \"positive\" and \"negative\" is equal, making it a quite balanced dataset.\n",
    "\n",
    "\n",
    "- Dataset file structure\n",
    "\n",
    "```\n",
    "| aclImdb_v1\n",
    "    | train\n",
    "    |   | pos\n",
    "    |   |   | 0_9.txt  \n",
    "    |   |   | 1_7.txt\n",
    "    |   |   | ...\n",
    "    |   | neg\n",
    "    |   |   | 0_3.txt\n",
    "    |   |   | 1_1.txt\n",
    "    |   | ...\n",
    "    | test\n",
    "    |   | pos\n",
    "    |   | neg\n",
    "    |   | ...\n",
    "    | ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "graffitiCellId": "id_nxrjw92",
    "id": "2D650272040243AA8AC04AA8BCC96645",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:00<00:00, 52698.83it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 51241.03it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 53923.67it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 53180.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t certainly nomad has some of the best horse riding \n",
      "1 \t this movie is really good. the plot, which works l\n",
      "0 \t who could possibly have wished for a sequel to ber\n",
      "1 \t saw this today with my 8 year old. i thought it wa\n",
      "0 \t unlike endemol usa's two other current game shows \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def read_imdb(folder='train', data_root=\"/home/kesci/input/IMDB2578/aclImdb_v1/aclImdb\"):\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "DATA_ROOT = \"/home/kesci/input/IMDB2578/aclImdb_v1/\"\n",
    "data_root = os.path.join(DATA_ROOT, \"aclImdb\")\n",
    "train_data, test_data = read_imdb('train', data_root), read_imdb('test', data_root)\n",
    "\n",
    "# print first 5 samples in the training set\n",
    "for sample in train_data[:5]:\n",
    "    print(sample[1], '\\t', sample[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "02FB2DCA103B4F22B1CC45DE98123CA7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trainings: 2\n",
      "label: this movie is really good. the plot, which works like puzzle forces viewer to think and guess, what will happen next. such a trick brings a lot of surprises and makes a viewer really looking forward to solution of a riddle. fighting scenes are very good. there's a lot of different combat styles (although one of styles was a bit unreal for me, but it's only my opinion) to watch and it's fascinating show. the only thing which may be irritating is actors look. a bit too effeminate (at least for me). hong kong was always good at kung-fu movies especially in the 70's and 80's, so \"five venoms\" (or other its versions) is great choice. review: certainly nomad has some of the best horse riding scenes, sw\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec12c9876622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'review:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# previe of the data\n",
    "print('# trainings:', len(train_data[0]))\n",
    "\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('label:', y, 'review:', x[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_q12pdj4",
    "id": "27D596E884AD48778032BE5876EA56C7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing:  Tokenization and Vocabulary\n",
    "\n",
    "After reading the data, we first tokenize the text, and then use a word as a token to create a dictionary based on the training set using [`torchtext.vocab.Vocab`](https://torchtext.readthedocs.io/en/latest/vocab.html#vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "graffitiCellId": "id_f6u98c3",
    "id": "501A5FC9F17D49B084DE9C15B04233A3",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in vocab: 46152\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized_imdb(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: each element is [string, 0/1 label] \n",
    "    @return: list after tokenization, each element is token sequence\n",
    "    '''\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')] # split by space\n",
    "    \n",
    "    return [tokenizer(review) for review, _ in data]\n",
    "\n",
    "def get_vocab_imdb(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: same as above\n",
    "    @return: vocab created on the dataset，instance as（freqs, stoi, itos）\n",
    "    '''\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "print('# words in vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_9s26xhr",
    "id": "B6CDF2CC978D48F980E835392BAC8CF0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "After the vocab and the mapping of token to idx are created, the text can be converted from string to a sequence with idx for later use. Because the reviews have different lengths, so they cannot be directly combined into mini batches. Here we fix the length of each comment to 500 by truncating or adding \"<unk>\" indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_3ejykvx",
    "id": "B8BDA6B5EE364F538022F97E8A1114FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        data: raw input data\n",
    "        vocab: dictionary we just created\n",
    "    @return:\n",
    "        features: idx seq of token, shape is (n, max_l) int tensor\n",
    "        labels: emotions labels, shape is (n,) 0/1 int tensor\n",
    "    '''\n",
    "    max_l = 500  # sequence length we defined: 500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x)) \n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_yge4ncq",
    "id": "D4189672985F4DA781725897C3395EBB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Creating the data iterator\n",
    "\n",
    "Using [`torch.utils.data.TensorDataset`](https://pytorch.org/docs/stable/data.html?highlight=tensor%20dataset#torch.utils.data.TensorDataset) can create PyTorch format dataset and data iterator. Each iteration will return a minibatch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_q2o053r",
    "id": "46970B8D972042009CC3393C59BCD4B6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 500]) y torch.Size([64])\n",
      "#batches: 391\n"
     ]
    }
   ],
   "source": [
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n",
    "\n",
    "# below is the code for same function as above\n",
    "# train_features, train_labels = preprocess_imdb(train_data, vocab)\n",
    "# test_features, test_labels = preprocess_imdb(test_data, vocab)\n",
    "# train_set = Data.TensorDataset(train_features, train_labels)\n",
    "# test_set = Data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "# preview \n",
    "# len(train_set) = features.shape[0] or labels.shape[0] \n",
    "# train_set[index] = (features[index], labels[index])\n",
    "\n",
    "batch_size = 64\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True) # shuffle the training set\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "# preview\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ksnuxvt",
    "id": "331B28609E2F4FCD81A30F5E665DB7EF",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Load pre-trained word vectors\n",
    "\n",
    "Since the vocab and idx token of the pre-trained word vectors are not the same as the dataset we use, the pre-trained word vector needs to be loaded according to the current order of the vocab and idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_1x32tei",
    "id": "E1E904AB724240589BAF01DA77485092",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 398244/400000 [00:14<00:00, 27457.92it/s]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6abdbc24e28e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_pretrained_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# load pre-trained, no need to update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 398244/400000 [00:30<00:00, 27457.92it/s]"
     ]
    }
   ],
   "source": [
    "cache_dir = \"/home/kesci/input/GloVe6B5429\"\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir) # 100dim is good enough\n",
    "#glove_vocab = Vocab.GloVe(name='6B', dim=300, cache=cache_dir) \n",
    "\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        words: list of word vectors to be loaded，as itos dictionary type\n",
    "        pretrained_vocab: pre-trained word vectors\n",
    "    @return:\n",
    "        embed: word vector loaded\n",
    "    '''\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # initialize to 0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False # load pre-trained, no need to update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_rfun1jp",
    "id": "6C1065F141F041D68C843ECD13DB5193",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Model training\n",
    "\n",
    "Define `train` and `evaluate_accuracy` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "graffitiCellId": "id_jv4ye1d",
    "id": "6E7F2873372E42F0AD93F544A6DEEBDC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    \n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) # calculate loss\n",
    "            optimizer.zero_grad() # reset grad to zero\n",
    "            l.backward() # backprop\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69C53E3038C14C6C8E6F50107816AFF9",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In the previous language models and text classification tasks, we treated text data as a time series data with only one dimension, and naturally, we used RNN models to process such data. In fact, we can also treat text as a one-dimensional image, so that we can apply one-dimensional Convolutional Neural Networks to capture associations between adjacent words. \n",
    "\n",
    "This notebook describes a such approach that applies CNN model to sentiment analysis: **textCNN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_h9301aq",
    "id": "467567D6536D47B887CF000BF0B9E7B1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convolution Neural Network\n",
    "\n",
    "### One-dimensional Convolution layer\n",
    "\n",
    "Before introducing the model, let's have a look at the principle of the 1-dim convolutional layer. Like 2-dim conv layers, 1-dim conv layers use 1-dim cross-correlation operations. In the 1d cross-correlation operation, the convolution window starts from the leftmost side of the input array and slides on the input array in order from left to right. When the convolution window slides to a certain position, the input subarray in the window and kernel array are multiplied and summed by element to get the element at the corresponding location in the output array. \n",
    "\n",
    "\n",
    "![Image Name](https://github.com/d2l-ai/d2l-en/raw/master/img/conv1d.svg?sanitize=true)\n",
    "\n",
    "\n",
    "As shown in the figure below, the input is a 1-dim array with a width of 7 and the kernel array has a width of 2. So the output width is $7-2+1=6$ and the 1st element is obtained by performing multiplication by element on the leftmost input subarray with a width of 2 and kernel array and then summing the results: $0×1+1×2 = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "graffitiCellId": "id_vf1wafb",
    "id": "F25448093E6340D181FE33A95E9F6FFB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  5.,  8., 11., 14., 17.])\n"
     ]
    }
   ],
   "source": [
    "# define the 1-dim croos-correlation function\n",
    "def corr1d(X, K):\n",
    "    '''\n",
    "    @params:\n",
    "        X: input array, (seq_len,) shape tensor\n",
    "        K: kernel size, (w,) tensor\n",
    "    @return:\n",
    "        Y: output array, (seq_len - w + 1,) shape tensor\n",
    "    '''\n",
    "    w = K.shape[0] # kernal window size\n",
    "    Y = torch.zeros((X.shape[0] - w + 1))\n",
    "    for i in range(Y.shape[0]): # sliding window\n",
    "        Y[i] = (X[i: i + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "X, K = torch.tensor([0, 1, 2, 3, 4, 5, 6]), torch.tensor([1, 2])\n",
    "print(corr1d(X, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_e67uju2",
    "id": "6AA00E61C69A4E42865B78FEC87E757D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The 1-dim cross-correlation operation for multiple input channels is also similar to the 2-dim cross-correlation operation for multiple input channels. On each channel, it performs the 1-dim cross-correlation operation on the kernel and its corresponding input and adds the results of the channels to get the output. Below figure shows a 1-dim cross-correlation operation with three input channels, where the blue part is the first output element and the input and kernel array elements used in calculation：$0×1+1×2+1×3+2×4+2×(−1)+3×(−3)=2$.\n",
    "\n",
    "\n",
    "![Image Name](https://github.com/d2l-ai/d2l-en/raw/master/img/conv1d-channel.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "graffitiCellId": "id_480pc9k",
    "id": "4E0F6D6AEB7344F080E9D8B0BFECFAD7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  8., 14., 20., 26., 32.])\n"
     ]
    }
   ],
   "source": [
    "def corr1d_multi_in(X, K):\n",
    "    # First, traverse along the 0th dim (channel dimension) of X and K,\n",
    "    # calculate the 1-dim cross-correlation result, \n",
    "    # stack all results together and add up along the 0th dim\n",
    "    \n",
    "    return torch.stack([corr1d(x, k) for x, k in zip(X, K)]).sum(dim=0)\n",
    "    # [corr1d(X[i], K[i]) for i in range(X.shape[0])]\n",
    "\n",
    "X = torch.tensor([[0, 1, 2, 3, 4, 5, 6],\n",
    "              [1, 2, 3, 4, 5, 6, 7],\n",
    "              [2, 3, 4, 5, 6, 7, 8]])\n",
    "K = torch.tensor([[1, 2], [3, 4], [-1, -3]])\n",
    "\n",
    "print(corr1d_multi_in(X, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_owwaz9l",
    "id": "FE1904E0DCC34AEB939DFE0B911985A6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The definition of a 2-dim cross-correlation operation shows that a 1-dim cross-correlation operation with multiple input channels can be regarded as a 2-dim cross-correlation operation with a single input channel. \n",
    "\n",
    "In other words, we can also present the 1-dim cross-correlation operation with multiple input channels below as the equivalent 2-dim cross-correlation operation with a single input channel. Here, the height of the kernel is equal to the height of the input. As the blue part shows the calculation：$2×(−1)+3×(−3)+1×3+2×4+0×1+1×2=2$.\n",
    "\n",
    "![Image Name](https://github.com/d2l-ai/d2l-en/raw/master/img/conv1d-2d.svg?sanitize=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Both the examples beofre have only one output channel. Similarly, we can also specify multiple output channels in the 1-dim convolutional layer to extend the model params in the convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_sp8o70s",
    "id": "64F1FB72318E45CC8EC2E974216BA59D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Max-Over-Time Pooling Layer\n",
    "\n",
    "Similarly, we have a 1-dim pooling layer. The **max-over-time pooling layer** used in TextCNN actually corresponds to a *1-dim global maximum pooling layer*. Assuming that the input contains multiple channels, and each channel consists of values on different timesteps, the output of each channel will be the largest value of all timesteps in the channel. Therefore, the input of the max-over-time pooling layer can have different timesteps on each channel.\n",
    "\n",
    "\n",
    "\n",
    "To improve computation performance, we often combine timing examples of different lengths into a minibatch and make the lengths of each timing example in the batch consistent by padding shorter samples with special char (likes 0). Naturally, these added special chars have no intrinsic meaning. Because the main purpose of the max-over-time pooling layer is to capture the most important features of timing, it usually allows the model to be unaffected by the manually added characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "graffitiCellId": "id_3r1xhqh",
    "id": "CB9416F229DB46038A3FC9AD670096E3",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @params:\n",
    "            x: input，(batch_size, n_channels, seq_len) tensor\n",
    "        @return: output, (batch_size, n_channels, 1) tensor\n",
    "        '''\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) # kenerl_size=seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_lhc1gs8",
    "id": "D697DE419B87437780D66FB3DD6D6C89",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### TextCNN Model\n",
    "\n",
    "\n",
    "TextCNN mainly uses a **1-dim convolutional layer** and **max-over-time pooling layer**. Suppose the input text sequence consists of $n$ words, and each word is represented by a $d$-dim word vector. Then the input sample has a width of $n$, a height of 1, and $d$ input channels. The calculation of textCNN can be mainly divided into the following steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define multiple one-dimensional convolution kernels and use them to perform convolution calculations on the inputs. Convolution kernels with different widths may capture the correlation of different numbers of adjacent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Perform max-over-time pooling on all output channels, and then concatenate the pooling output values of these channels in a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. The concatenated vector is transformed into the output for each category through the fully connected layer. A dropout layer can be used in this step to deal with overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Name](https://github.com/d2l-ai/d2l-en/raw/master/img/textcnn.svg?sanitize=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This figure shows an example to illustrate the textCNN. The input here is a sentence with 11 words, with each word represented by a 6-dim word vector. Therefore, the input sequence has a width of 11, and 6 input channels. \n",
    "\n",
    "Assuming we are using two 1-dim convolution kernels with widths of 2 and 4, and 4 and 5 as output channels, respectively. Therefore, after 1-dim convolution calculation, the width of the 4 output channels is $11-2+1=10$, while the width of the other 5 channels is $11-4+1=8$. Even though the width of each channel is different, we can still perform max-over-time pooling for each channel and concatenate the pooling outputs of the 9 channels into a 9-dim vector. \n",
    "\n",
    "Finally, we use a fully connected layer to transform the 9-dim vector into a 2-dim output: positive sentiment and negative sentiment predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5469E8C8A251426589FC84976BCFDFE5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Next, we will implement our textCNN model. Compared with the LSTM model, here we will use a 1-dim convolutional layer, two embedding layers, one with a fixed weight and another that participates in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "graffitiCellId": "id_9mqnlf7",
    "id": "43E465F855D841EEA2ABD3F0B16F6429",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: vocab\n",
    "            embed_size: embedding dim for word vectors\n",
    "            kernel_sizes: kernel size list\n",
    "            num_channels: channel number for kernel\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size) # embedding layer that participates in training\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size) # embedding layer that does not participate in training\n",
    "        \n",
    "        self.pool = GlobalMaxPool1d() # layer has no weight, so it can share an instance\n",
    "        self.convs = nn.ModuleList()  # create multiple 1-dim convolutional layers\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = 2*embed_size, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "            \n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        self.dropout = nn.Dropout(0.5) # add dropout layer\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: idx sequence for token, (batch_size, seq_len) int tensor\n",
    "        @return:\n",
    "            outputs: binary prediction, (batch_size, 2) tensor\n",
    "        '''\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), \n",
    "            self.constant_embedding(inputs)), dim=2)  # (batch_size, seq_len, 2*embed_size)\n",
    "        # concatenate the output of two embedding layers,\n",
    "        # transform into the channel dimension of the 1-dim convolutional layer,\n",
    "        # according to the input format required by Conv1D, the word vector dimension\n",
    "        embeddings = embeddings.permute(0, 2, 1) # (batch_size, 2*embed_size, seq_len)\n",
    "        \n",
    "        \n",
    "        # for each 1-dim convolutional layer, after max-over-time pooling,\n",
    "        # get tensor shape of (batch size, channel size, 1)\n",
    "        # use squeeze to remove the last dim and then concatenate on the channel dim\n",
    "        encoding = torch.cat([\n",
    "            self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "    \n",
    "        # below is the step-by-step code for the same function\n",
    "        # encoding = []\n",
    "        # for conv in self.convs:\n",
    "        #     out = conv(embeddings) # (batch_size, out_channels, seq_len-kernel_size+1)\n",
    "        #     out = self.pool(F.relu(out)) # (batch_size, out_channels, 1)\n",
    "        #     encoding.append(out.squeeze(-1)) # (batch_size, out_channels)\n",
    "        # encoding = torch.cat(encoding) # (batch_size, out_channels_sum)\n",
    "        \n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        # after dropout, use a fully connected layer get the output\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_leupmp8",
    "id": "374844FBE56B43668639A9C07D0E2DD0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "graffitiCellId": "id_g74v4mq",
    "id": "BC1674BE51704FBA9F46925EE338B813",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.6358, train acc 0.663, test acc 0.783, time 371.6 sec\n",
      "epoch 2, loss 0.2478, train acc 0.758, test acc 0.830, time 367.9 sec\n",
      "epoch 3, loss 0.1367, train acc 0.814, test acc 0.851, time 367.1 sec\n",
      "epoch 4, loss 0.0822, train acc 0.857, test acc 0.851, time 367.4 sec\n",
      "epoch 5, loss 0.0499, train acc 0.896, test acc 0.860, time 367.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_b5cbyuk",
    "id": "8BEB86488C464EB09629F94A5198DCDB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "training on  cuda\n",
    "epoch 1, loss 0.6314, train acc 0.666, test acc 0.803, time 15.9 sec\n",
    "epoch 2, loss 0.2416, train acc 0.766, test acc 0.807, time 15.9 sec\n",
    "epoch 3, loss 0.1330, train acc 0.821, test acc 0.849, time 15.9 sec\n",
    "epoch 4, loss 0.0825, train acc 0.858, test acc 0.860, time 16.0 sec\n",
    "epoch 5, loss 0.0494, train acc 0.898, test acc 0.865, time 15.9 sec\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "971D6CA7B6EE476BB6716B12617BC7DB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    '''\n",
    "    @params：\n",
    "        net: trained model\n",
    "        vocab: vocab on the dataset，mapping token to idx\n",
    "        sentence: text in word tokens seq\n",
    "    @return: pred outcome: positive for postive emotions，negative else\n",
    "    '''\n",
    "    device = list(net.parameters())[0].device # load the device location\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "graffitiCellId": "id_me4atgv",
    "id": "4817955A29B1426BA9AF6B9493EC83F0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CCFE2B727C6845D1834D109009A4758E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "graffitiCellId": "id_6qea9rq",
    "id": "EF6C2C490A2C49DC8F49794FB5BCD7ED",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "\n",
    "* We can use one-dimensional convolution to process and analyze timing data.\n",
    "\n",
    "* A one-dimensional cross-correlation operation with multiple input channels can be regarded as a two-dimensional cross-correlation operation with a single input channel.\n",
    "\n",
    "* The input of the max-over-time pooling layer can have different numbers of timesteps on each channel.\n",
    "\n",
    "* TextCNN mainly uses a one-dimensional convolutional layer and max-over-time pooling layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
