{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_rjm21dx",
    "id": "A5A4340181EA406E8EF192DFC1A66A96",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Advanced Word Embedding\n",
    "\n",
    "\n",
    "In the last notebook, I trained a Word2Vec word embedding model on a small-scale data set and searched for synonyms based on the cosine similarity of the word vectors. \n",
    "\n",
    "Although Word2Vec has been able to successfully convert discrete words into continuous word vectors, and to some extent preserve the approximate relationship between words, the Word2Vec model is still not perfect, and it can be further improved:\n",
    "\n",
    "\n",
    "1.  **Subword Embedding**：FastText, represent words more closely as a collection of subwords in a **fixed-size N-gram**, while BPE(Byte Pair Encoding) can automatically and dynamically generate a set of high-freq subwords based on the statistical information of the corpus\n",
    "\n",
    "\n",
    "2.  [GloVe(Global Vectors for Word Representation)](https://nlp.stanford.edu/pubs/glove.pdf) : By transforming the conditional probability formula of the Word2Vec model, we can obtain a function expression for global loss, and further optimize the model based on this.\n",
    "\n",
    "In practice, we often train these word embedding models on large-scale corpora, and apply the pre-trained word vectors to downstream NLP tasks. This notebook will use the **GloVe model** as an example to demonstrate how to use pre-trained word vectors to find synonyms and analogies.\n",
    "\n",
    "\n",
    "\n",
    "- [GloVe Model](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "Let's first look at the loss function for Word2Vec (using Skip-Gram and without negative samplin):\n",
    "\n",
    "\n",
    "$$-\\sum_{t=1}^T\\sum_{-m\\le j\\le m,j\\ne 0} \\log P(w^{(t+j)}\\mid w^{(t)})$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$P(w_j\\mid w_i) = \\frac{\\exp(\\boldsymbol{u}_j^\\top\\boldsymbol{v}_i)}{\\sum_{k\\in\\mathcal{V}}\\exp(\\boldsymbol{u}_k^\\top\\boldsymbol{v}_i)}$$\n",
    "\n",
    "\n",
    "where, $w_i$ is center，$w_j$ is context, the probability formula is $q_{ij}$。\n",
    "\n",
    "\n",
    "Note that the loss function contains two summation symbols, which respectively enumerate each center and its corresponding context words in the corpus. In fact, we can also use another counting method, which is to directly enumerate each word as the center and the context word:\n",
    "\n",
    "$$-\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} x_{ij}\\log q_{ij}$$\n",
    "\n",
    "\n",
    "Where $x_{ij}$  represents the total number of times $w_j$ is the context word of $w_i$ in the entire dataset. \n",
    "\n",
    "We can then rewrite the formula into the form of cross-entropy as follows:\n",
    "\n",
    "\n",
    "$$-\\sum_{i\\in\\mathcal{V}}x_i\\sum_{j\\in\\mathcal{V}}p_{ij} \\log q_{ij}$$\n",
    "\n",
    "\n",
    "where $x_i$ is the sum of the context window size $w_i$, $p_{ij}=x_{ij}/x_i$ is the proportion of $w_j$ in the context window of $w_i$.\n",
    "\n",
    "\n",
    "It is easy to understand as in fact our word embedding method would like the model to learn how likely it is that $w_j$ is the context word of $w_i$, while the ground truth labels are the actual data on the corpus. At the same time, each word in the corpus has a different weight in the loss function according to the difference of $x_i$.\n",
    "\n",
    "\n",
    "\n",
    "So far we have only rewritten Skip-Gram's loss function, and have not made any substantial changes to the model yet. The GloVe model has just made the following changes based on the previous:\n",
    "\n",
    "1. Use the non-probability distribution variables $p'_{ij}=x_{ij}$ and $q′_{ij}=\\exp(\\boldsymbol{u}^\\top_j\\boldsymbol{v}_i)$, and take their logarithm,\n",
    "\n",
    "2. Add two scalar model params for each word $w_i$: bias $b_i$ for the center and bias $c_i$ for the context, loosening the standard in the probability definition,\n",
    "\n",
    "3. Replace the weight of each loss term $x_i$ with the function $h(x_{ij})$, the weight function $h(x)$ is a monotonically increasing function with a range over $[0,1]$, loosening the implicit assumption that the context is linearly related to $x_i$,\n",
    "\n",
    "4. Use square loss function instead of the cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we arrived at the loss function for GloVe:\n",
    "\n",
    "\n",
    "$$\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} h(x_{ij}) (\\boldsymbol{u}^\\top_j\\boldsymbol{v}_i+b_i+c_j-\\log x_{ij})^2$$\n",
    "\n",
    "Since these non-zero $x_{ij}$ are calculated in advance using entire dataset, and contain the global info, thus received the name \"Global Vectors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D721CBA09684F36AEDBA1739EA84963",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Load pre-trained GloVe vectors\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/) Provides a variety of pre-trained word vectors. The corpus uses text from Wikipedia, CommonCrawl, and Twitter, with the total number of words in the corpus ranging from 6 billion to 840 billion. It also provides a variety of word vector dimensions for models to use.\n",
    "\n",
    "[`torchtext.vocab`](https://torchtext.readthedocs.io/en/latest/vocab.html) already supports GloVe, FastText, CharNGram and other commonly-used pre-trained word vectors. We can load pre-trained GloVe word vectors by calling [`torchtext.vocab.GloVe`](https://torchtext.readthedocs.io/en/latest/vocab.html#glove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "graffitiCellId": "id_su5pfc7",
    "id": "BD67D8BDC6374AFD8056B0587FFC3B7E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/400000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove.42B.300d', 'glove.840B.300d', 'glove.twitter.27B.25d', 'glove.twitter.27B.50d', 'glove.twitter.27B.100d', 'glove.twitter.27B.200d', 'glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399249/400000 [00:10<00:00, 38432.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contain 400000 words in total.\n",
      "3366 beautiful\n"
     ]
    }
   ],
   "source": [
    "# load torchtext module\n",
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "print([key for key in vocab.pretrained_aliases.keys() if \"glove\" in key])\n",
    "# 42B means 42 billion words in the vocab, 300-dim\n",
    "cache_dir = \"/home/kesci/input/GloVe6B5429\"\n",
    "glove = vocab.GloVe(name='6B', dim=50, cache=cache_dir) # initiate glove model we need by size and dim\n",
    "\n",
    "print(\"Contain %d words in total.\" % len(glove.stoi)) \n",
    "# bidirectional mapping: string to idx & idx to string\n",
    "print(glove.stoi['beautiful'], glove.itos[3366]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FBAF89D636D41E99BB95E8A3B7F8FE0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Find Synonyms and Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_a6xttki",
    "id": "0DCD1B715DE84E258D948974A5CE7F76",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Find Synonyms\n",
    "\n",
    "Since the cosine similarity of word vectors can measure the similarity of words' meaning, we can find the synonyms of a word by finding its K nearest neighbors in the vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "graffitiCellId": "id_hnlfnud",
    "id": "923F41A77F21415883D370F34EF15D33",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.921: lovely\n",
      "cosine sim=0.893: gorgeous\n",
      "cosine sim=0.830: wonderful\n",
      "cosine sim=0.825: charming\n"
     ]
    }
   ],
   "source": [
    "def knn(W, x, k):\n",
    "    '''\n",
    "    @params:\n",
    "        W: all vectors in space\n",
    "        x: a specific vector\n",
    "        k: neighbors number\n",
    "    @outputs:\n",
    "        topk: idx of top K vectors with maximum cosine similarity\n",
    "        [...]: Cosine similarity\n",
    "    '''\n",
    "    cos = torch.matmul(W, x.view((-1,)))/( # reshape \n",
    "        (torch.sum(W * W, dim=1) + 1e-9).sqrt() * torch.sum(x * x).sqrt()) # smoothing eps\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    topk = topk.cpu().numpy()\n",
    "    return topk, [cos[i].item() for i in topk]\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    '''\n",
    "    @params:\n",
    "        query_token: the input word \n",
    "        k: number of synonyms\n",
    "        embed: pre-trained word vectors\n",
    "    '''\n",
    "    topk, cos = knn(embed.vectors,\n",
    "                    embed.vectors[embed.stoi[query_token]], k+1) # k+1 to include input token itself\n",
    "    for i, c in zip(topk[1:], cos[1:]):  # remove the input token\n",
    "        print('cosine sim=%.3f: %s' % (c, (embed.itos[i])))\n",
    "\n",
    "get_similar_tokens('beautiful', 4, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "graffitiCellId": "id_u76vsiw",
    "id": "C20619ABAA0F4B8686AD294528BD87D8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.917: computers\n",
      "cosine sim=0.881: software\n",
      "cosine sim=0.853: technology\n",
      "cosine sim=0.813: electronic\n",
      "cosine sim=0.806: internet\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('computer', 5, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "A19D8BC3E87F4EE084BABCC5CD5451DD",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.819: drink\n",
      "cosine sim=0.818: drinks\n",
      "cosine sim=0.814: wine\n",
      "cosine sim=0.808: tea\n",
      "cosine sim=0.804: beer\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('coffee', 5, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "graffitiCellId": "id_q1w9uyd",
    "id": "875E409C49E44DDD877BC1D8BDFA3D45",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.810: awful\n",
      "cosine sim=0.788: sorry\n",
      "cosine sim=0.772: terrible\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('sad', 3, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_j4l1wko",
    "id": "E4F2C7332C054680887DE3E8F90795E7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Find Analogies\n",
    "\n",
    "\n",
    "In addition to finding synonyms, we can also use pre-trained word vectors to find analogies for a given word, for example, \"man\" to \"woman\" is equivalent to \"son\" to \"daughter\". The analogy problem can be defined as:  For the 4 words in the analogy relationship  ***\"$a$ to $b$ is equivalent to $c$ to $d$\"***,  given the first 3 words $a, b, c$, find $d$. \n",
    "\n",
    "The idea of ​​analogy is to search for the word vector that is most similar to the result vector of **$\\text{vec}(c)+\\text{vec}(b)-\\text{vec}(a)$**, where $\\text{vec}(w)$ is the word vector of $w$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "graffitiCellId": "id_8ovz8go",
    "id": "4DA8D296F91C4B2798C8EBF5D36F405E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    '''\n",
    "    @params:\n",
    "        token_a: word a\n",
    "        token_b: word b\n",
    "        token_c: word c\n",
    "        embed: pre-trained word vectors\n",
    "    @outputs:\n",
    "        res: analogy d\n",
    "    '''\n",
    "    vecs = [embed.vectors[embed.stoi[t]] \n",
    "                for t in [token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2] # find word vector d\n",
    "    topk, cos = knn(embed.vectors, x, 1) # use find synoynm function to find d itself\n",
    "    res = embed.itos[topk[0]] # get d from idx to string\n",
    "    return res\n",
    "\n",
    "get_analogy('man', 'woman', 'son', glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "graffitiCellId": "id_6r0q4tn",
    "id": "B75CCBF19161403EB5A5C47817A7AD36",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japan'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('beijing', 'china', 'tokyo', glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "graffitiCellId": "id_avf2squ",
    "id": "61E15B6E656E49ABAE8E64352D9C9360",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biggest'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('bad', 'worst', 'big', glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "graffitiCellId": "id_u115i98",
    "id": "F89CC50CA1F14F9586A174E8AFE670FE",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('do', 'did', 'go', glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "06A0959B011B40CE9B51026F1CF09DB5",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evening'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('coffee', 'morning', 'beer', glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7225993C0078460CB6F5ED9CA844DE9D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this notebook, we explored how word2vec can be further improved with more complex word embedding models and had a taste of using GloVe to find synonyms and analogies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
