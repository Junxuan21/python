{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple template for SkipGram implemented in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from collecitons import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text data\n",
    "with open(file_path) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "def preprocess(text, freq):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.replace('.', \" \")\n",
    "    #... more preprocessing\n",
    "    \n",
    "    # remove the low-freq words\n",
    "    words = text.split()\n",
    "    word_count = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_count[word] >= freq]\n",
    "    \n",
    "    return trimmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = preprocess(text, freq = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping table \n",
    "vocab = set(words)\n",
    "vocab2int = {w: c for c,w in enumerate(list(vocab))}\n",
    "int2vocab = {c: w for c,w in enumerate(list(vocab))}\n",
    "\n",
    "int_words = [vocab2int[w] for w in words]\n",
    "int_word_counts = Counter(int_words)\n",
    "total_count = len(int_word_counts)\n",
    "\n",
    "t = 1e-5\n",
    "word_freq = {w: c/total_count for w, c in int_word_counts.items()}\n",
    "prob_drop = {w: 1-np.sqrt(t/word_freq[w]) for w in int_word_counts)}\n",
    "train_word = [w for w in int_words if random.random() < (1-prob_drop[w])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a window to get a batch of words\n",
    "\n",
    "def get_window(words, idx, window_size=5):\n",
    "    # idx is the index of center word\n",
    "    target_window = np.random.randint(1, window_zise+1)\n",
    "    start_point = idx - target_window if idx - target_window > 0 else 0\n",
    "    end_point = idx + target_window\n",
    "    target = set(words[start_point:idx]+words[idx+1:end_point+1])\n",
    "    \n",
    "    return list(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch iterator\n",
    "\n",
    "def get_batch(words, batch_size, window_size=5):\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    words = word[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        batch_x, batch_y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for i in range(len(batch)):\n",
    "            x = batch[i]  # x is the center of each batch\n",
    "            y = get_window(batch, i, window_size)  # y is the surrouding words of x\n",
    "            batch_x.extend([x]*len(y))\n",
    "            batch_y.extend(y)\n",
    "            \n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build SkipGram language model using Negative Sampling\n",
    "\n",
    "class SkipGramNeg(nn.Module):\n",
    "    \n",
    "    def __init__ (self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist # used in negative sampling \n",
    "        \n",
    "        # define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        # initialize embedding tables with uniform distribution as this helps with covergence\n",
    "        self.in_embed.weight.data.uniform_(-1, 1)\n",
    "        self.out_embed.weight.data.uniform_(-1, 1)\n",
    "    \n",
    "    def forward_input(self, input_words):\n",
    "        input_vectors = self.in_embed(input_words) # input_words shape is like one hot\n",
    "        return input_vectors \n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        output_vectors = self.out_embed(output_words)\n",
    "        return output_vectors\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\"\n",
    "        generate noise vectors with shape(batch_size, n_samples, n_embed)\n",
    "        \"\"\"\n",
    "        if self.noise_dist is None:\n",
    "            # sample words uniformly\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist, batch_size*n_samples, replacement=True)\n",
    "        # resize noise_vectors to the matrix shape for operations\n",
    "        noise_vectors = self.out_embed(noise_words).view(\n",
    "                        batch_size, n_samples, self.n_embed) \n",
    "        \n",
    "        return noise_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loss function\n",
    "\n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "\n",
    "        # input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        # output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, \n",
    "                             input_vectors).sigmoid().log()\n",
    "        # resulting shape: batch_size*1\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), \n",
    "                               input_vectors).sigmoid().log()\n",
    "        # resulting shape: batch_size*n_samples*1\n",
    "        # sum the loss over the sample of noise vectors\n",
    "        noise_loss = noise_loss.squeeze().sum(1) \n",
    "        \n",
    "        # negate and sum correct and noisy log-sigmoid loss\n",
    "        # return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform dist for negative sampling\n",
    "\n",
    "word_freq = np.array(word_freq.values())\n",
    "unigram_dist = word_freq/word_freq.sum()\n",
    "noise_dist = torch.from_numpy(\n",
    "    unigram_dist ** (0.75)/np.sum(unigram_dist ** (0.75))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instantiating\n",
    "embedding_dim = 300\n",
    "model = SkipGramNeg(len(vocab2int), embedding_dim, noise_dist=noise_dist)\n",
    "\n",
    "# using the defined loss function\n",
    "criterion = NegativeSamplingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 1500\n",
    "steps = 0\n",
    "epochs = 5\n",
    "batch_size = 500\n",
    "n_samples = 5\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get input, target batches\n",
    "    for input_words, target_words in get_batch(train_words, batch_size):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
    "        \n",
    "        # input, output and noise vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.forward_noise(batch_size, n_samples)\n",
    "        \n",
    "        # negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "        if steps //print_every == 0:\n",
    "            print(loss)\n",
    "            \n",
    "        optimizer.zero_grad()  # reset gradient to zero\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
