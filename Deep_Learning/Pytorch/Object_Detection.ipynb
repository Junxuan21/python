{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_hkbldjq",
    "id": "8397DCD3D04243F88F25751AB221C99E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Object Detection and Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "graffitiCellId": "id_fm9iskh",
    "id": "1618756528DB4C518FAF47C7904A440A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/kesci/input/')\n",
    "import d2lzh1981 as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "graffitiCellId": "id_j7fclka",
    "id": "C9CAC60A6ADF4814AA1EFBB838B80254",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc84edff8d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/C9CAC60A6ADF4814AA1EFBB838B80254/q68lbsx8ck.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display an image as an example\n",
    "d2l.set_figsize()\n",
    "img = Image.open('/home/kesci/input/img2083/img/catdog.jpg')\n",
    "d2l.plt.imshow(img) # display the image and return the file output, add semicolon to only display image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_n61fsiz",
    "id": "4750E0470A374AAD89C5A983BF09EAD1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "graffitiCellId": "id_96o216d",
    "id": "B73D8B9880ED420FA06E1002AAC50981",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bbox stands for bouding box\n",
    "# takes (upper left x, upper left y, lower right x, loer right y) of the bbox\n",
    "dog_bbox, cat_bbox = [60, 45, 378, 516], [400, 112, 655, 493]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_y09udcv",
    "id": "DB52EF377A4C4F68873DFCF19D3C16CC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bbox_to_rect(bbox, color):  \n",
    "    # define the function to change the bbox representation from\n",
    "    # (upper left x, upper left y, lower right x, loer right y)to matplotlib format：\n",
    "    # ((upper left x, upper left y), width, height)\n",
    "    return d2l.plt.Rectangle(\n",
    "        xy = (bbox[0], bbox[1]), \n",
    "        width=bbox[2]-bbox[0], \n",
    "        height = bbox[3]-bbox[1],\n",
    "        fill = False, edgecolor = color, linewidth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "graffitiCellId": "id_kgphoy1",
    "id": "D3875332DE3140BA88D963B12739617F",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ad1d7f3b4565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_to_rect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_to_rect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd2l' is not defined"
     ]
    }
   ],
   "source": [
    "fig = d2l.plt.imshow(img)\n",
    "fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))\n",
    "fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_66i83p8",
    "id": "0B157D1FA2FF44FABC8BEAB55270954F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Anchor Box\n",
    "\n",
    "There are a wide array of algos in object detection field, basically by sampling spaces in the input images and to adjust the area edge to approach the ground-truth bounding box of obejcts.\n",
    "\n",
    "Here, in this notebook I will implement Anchor Box generated at different pixel point with different aspect ratio to solve our object detection problem!\n",
    "\n",
    "> Note: using PyTorch for Object Detection look at :   [a-PyTorch-Tutorial-to-Object-Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_1f8r8p0",
    "id": "F6A36EF6551F49309F60F53B6F2D23C7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "IMAGE_DIR = '/home/kesci/input/img2083/img/'\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_r7hb3z0",
    "id": "AA44CF027C5A422F865DAA109E773A4E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Create multiple anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_j0hr7at",
    "id": "E93C5C06547848018A990A9A1B0710E4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Suppose the input image has height\\*width of $h$ and $w$. We generate anchor boxes of different shapes with each pixel of the image as the center. If the size is $s\\in (0,1]$ and the aspect ratio is $r > 0$, then the width and height of the anchor box will be $ws\\sqrt{r}$ and $hs/\\sqrt{r}$. When the center position is given, the anchor frame with known width and height is determined.\n",
    "\n",
    "\n",
    "Below we set a set of sizes $s_1, \\ldots, s_n$ and a set of aspect ratios $r_1, \\ldots, r_m$. If using all the size and aspect ratio combinations with each pixel as the center, the input image will get a total of $whnm$ anchor boxes. Although these anchor boxes may cover all real bounding boxes, the computational complexity is way too high. Therefore, we are usually only interested in size and aspect ratio combinations that include $s_1$ or $r_1$, ie\n",
    "\n",
    "\n",
    "$$\n",
    "(s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1).\n",
    "$$\n",
    "\n",
    "\n",
    "That is, the number of anchor frames centered on the same pixel is $n+m-1$. For the whole input image, we will generate a total of $wh(n+m-1)$ anchor boxes. The above method of generating anchor boxes has been implemented in `MultiBoxPrior` function. Specifying an input, a set of sizes, and a set of aspect ratios, the function returns all anchor boxes of the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "graffitiCellId": "id_0m38c9r",
    "id": "27C845C20AB44B2FB7D40936199E72D2",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 728, h = 561\n"
     ]
    }
   ],
   "source": [
    "d2l.set_figsize(figsize=(8,5))\n",
    "img = Image.open(os.path.join(IMAGE_DIR, 'catdog.jpg'))\n",
    "w, h = img.size\n",
    "print(\"w = %d, h = %d\" % (w, h)) # print image's width&height\n",
    "\n",
    "#d2l.plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "graffitiCellId": "id_ddbxqxt",
    "id": "A1F0E2CCC9164DD184B5B896B7B2BC17",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the function\n",
    "def MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):\n",
    "    \"\"\"\n",
    "    anchor uses (xmin, ymin, xmax, ymax)\n",
    "    Args:\n",
    "        feature_map: torch tensor, Shape: [N, C, H, W]\n",
    "        sizes: List of sizes (0~1) of generated MultiBoxPriores  #the size of anchor box to the image\n",
    "        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores #height/width ratio\n",
    "    Returns:\n",
    "        anchors of shape (1, num_anchors, 4) \n",
    "        every batch is same, so the 1st-dim is 1\n",
    "    \"\"\"\n",
    "    pairs = [] # pair of (size, sqrt(ration))\n",
    "    \n",
    "    # generate (n + m -1) anchor boxes\n",
    "    for r in ratios:\n",
    "        pairs.append([sizes[0], math.sqrt(r)])\n",
    "    for s in sizes[1:]:\n",
    "        pairs.append([s, math.sqrt(ratios[0])])\n",
    "    \n",
    "    pairs = np.array(pairs)\n",
    "    \n",
    "    # generate box at the center of coordinates (x,y,x,y)\n",
    "    ss1 = pairs[:, 0] * pairs[:, 1] # size * sqrt(ration)\n",
    "    ss2 = pairs[:, 0] / pairs[:, 1] # size / sqrt(ration)\n",
    "    \n",
    "    base_anchors = np.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2\n",
    "    \n",
    "    # combine cooridnates and anchor to get hw(n+m-1) box output\n",
    "    h, w = feature_map.shape[-2:]\n",
    "    shifts_x = np.arange(0, w) / w # generate coordinates on x/y axis\n",
    "    shifts_y = np.arange(0, h) / h # divide width/height to standardize\n",
    "    shift_x, shift_y = np.meshgrid(shifts_x, shifts_y) # combine x,y coordinates together\n",
    "    \n",
    "    shift_x = shift_x.reshape(-1)\n",
    "    shift_y = shift_y.reshape(-1)\n",
    "    \n",
    "    shifts = np.stack((shift_x, shift_y, shift_x, shift_y), axis=1) # have all coordinates\n",
    "    # reshape to keep base_anchor last dim, expand for shifts and base_anchor\n",
    "    # 1st-dim is base_anchor, 2nd-dim is shifts\n",
    "    anchors = shifts.reshape((-1, 1, 4)) + base_anchors.reshape((1, -1, 4)) \n",
    "    \n",
    "    return torch.tensor(anchors, dtype=torch.float32).view(1, -1, 4) # .view() to reshape again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "graffitiCellId": "id_v691kg0",
    "id": "0A6F451B5C9D409BBF406F48B67A58EF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2042040, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor(1, 3, h, w)  # create input data to test\n",
    "Y = MultiBoxPrior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_jc7hpds",
    "id": "6C1B7D28BBCB4D9DA905F380EDA3BE1E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "We can see that, the return `y` shape is (1, total_num_anchors, 4). After we transform `y` to (h, w, num_anchors_for_pixel, 4 ), we can get all anchors centered at one pixel by any pixel points. The below example shows how to get the first anchor for pixel（250, 250）.\n",
    "\n",
    "It has 4 elements, upper left $x$ and $y$ coordinates and lower right $x$ and $y$ coordinates. The $x$ and $y$ coordinates are divided by image height and width, so the value is between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "graffitiCellId": "id_tlhm5sm",
    "id": "0A93AC47957A43868235C9431E757EA5",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0316,  0.0706,  0.7184,  0.8206])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display anchor of one pixel point\n",
    "boxes = Y.reshape((h, w, 5, 4)) # reshape to(h, w, anchor for every pixel)\n",
    "# 5 because we have 3 sizes and 3 ratios, total we have 3+3-1 anchors for every pixel \n",
    "\n",
    "boxes[250, 250, 0, :] # * torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "# the first size and ratio is 0.75 and 1\n",
    "# the width and height is 0.75 = 0.7184 + 0.0316 = 0.8206 - 0.0706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_8ix1b72",
    "id": "A61011D1BB9A4CC99B6B7638B4E41489",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "> To test the output, the size and ratio is 0.75 and 1, so after standardization, the height and width is both 0.75, so the output is correct（0.75 = 0.7184 + 0.0316 = 0.8206 - 0.0706\n",
    "\n",
    "To display all the anchor centered at one pixel, we define a function `show_bboxes` to draw multiple bboxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "graffitiCellId": "id_nc2s0ua",
    "id": "BE181FA7968F41619C3E86B1C5FEA8FA",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_bboxes(axes, bboxes, labels=None, colors=None):\n",
    "    \n",
    "    def _make_list(obj, default_values=None):\n",
    "        if obj is None:\n",
    "            obj = default_values\n",
    "        elif not isinstance(obj, (list, tuple)):\n",
    "            obj = [obj]\n",
    "        return obj\n",
    "\n",
    "    labels = _make_list(labels)\n",
    "    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])\n",
    "    \n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        color = colors[i % len(colors)]\n",
    "        rect = d2l.bbox_to_rect(bbox.detach().cpu().numpy(), color)\n",
    "        axes.add_patch(rect)\n",
    "        \n",
    "        if labels and len(labels) > i:\n",
    "            text_color = 'k' if color == 'w' else 'w'\n",
    "            axes.text(rect.xy[0], rect.xy[1], labels[i],\n",
    "                      va='center', ha='center', fontsize=6, color=text_color,\n",
    "                      bbox=dict(facecolor=color, lw=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the coordinates of the $x$ and $y$ axes in the variable `boxes` have been divided by the width and height of the image, respectively. When plotting, we need to restore the original coordinate value of the anchor box, and therefore define the variable `bbox_scale`. \n",
    "\n",
    "Now we can draw all the anchor boxes centered at (250, 250) in the image. It can be seen that the anchor box with a size of 0.75 and an aspect ratio of 1 covers the dog in the image pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "graffitiCellId": "id_47p177b",
    "id": "28FFDB57F6CB43A69163325A4D136A48",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/28FFDB57F6CB43A69163325A4D136A48/q68lc6yokl.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display anchor for 250*250 pixel points\n",
    "d2l.set_figsize()\n",
    "fig = d2l.plt.imshow(img)\n",
    "bbox_scale = torch.tensor([[w, h, w, h]], dtype=torch.float32)\n",
    "show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,\n",
    "            ['s=0.75, r=1', 's=0.75, r=2', 's=0.75, r=0.5', 's=0.5, r=1', 's=0.25, r=1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_kxinbbk",
    "id": "E97D198893014D269A056372142A806C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## IoU (Intersection over Union)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_0gx2u7a",
    "id": "219EF652C7BC4B84804B9055F9FAA551",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "How do we evaluate our anchor boxes if we know the ground truth bouding boxes for our objects?\n",
    "\n",
    "One obvious way is to measure the similarity between anchor and the real bbox, as we know the Jaccard index can measure the similarity of two sets. Given Set $\\mathcal{A}$ and $\\mathcal{B}$, the Jaccard index is the intersection divided by the union:\n",
    "\n",
    "\n",
    "$$\n",
    "J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In fact, we can think of the pixel area within the bounding box as a collection of pixels. In this way, we can use the Jaccard index of the pixel set of the two bounding boxes to measure the similarity of the two bounding boxes. When measuring the similarity of two bounding boxes, we usually refer to the Jaccard index as the Intersection over Union (IoU), that is, the ratio of the intersection and the union, as shown below. The value of the intersection ratio ranges between 0 and 1, with 0 means that the two bounding boxes have no overlapping pixels, and 1 means that the two bounding boxes are identical.\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5vs9jkw9f.png?imageView2/0/w/640/h/640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "graffitiCellId": "id_cyce0zq",
    "id": "D762F50137C345EB8B18418E11F9198C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_intersection(set_1, set_2):\n",
    "    \"\"\"\n",
    "    compute intersections between anchors\n",
    "    Args:\n",
    "        set_1: a tensor of dimensions (n1, 4), anchor as(xmin, ymin, xmax, ymax)\n",
    "        set_2: a tensor of dimensions (n2, 4), anchor as(xmin, ymin, xmax, ymax)\n",
    "    \n",
    "    Returns:\n",
    "        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\n",
    "    \"\"\"\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
    "\n",
    "\n",
    "def compute_jaccard(set_1, set_2):\n",
    "    \"\"\"\n",
    "    计算anchor之间的Jaccard系数(IoU)\n",
    "    Args:\n",
    "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\n",
    "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\n",
    "    Returns:\n",
    "        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\n",
    "    \"\"\"\n",
    "    # Find intersections\n",
    "    intersection = compute_intersection(set_1, set_2)  # (n1, n2)\n",
    "\n",
    "    # Find areas of each box in both sets\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
    "\n",
    "    # Find the union\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
    "\n",
    "    return intersection / union  # (n1, n2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_dexi436",
    "id": "BA19ABB6788D463E880223B6932C1769",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mark anchor for training set\n",
    "\n",
    "\n",
    "In the training set, we treat each anchor box as a training sample. In order to train the object detection model, we need to label two types of labels for each anchor box: \n",
    "\n",
    "- one is the category of the target contained in the anchor box, referred to as the category or class; \n",
    "- the second is the offset of the real bounding box from the anchor box, referred to as the offset \n",
    "\n",
    "In object detection, we first generate multiple anchors, then predict the category and offset for each anchor, then adjust the anchor position according to the predicted offset, and finally filter the predicted anchors to output.\n",
    "\n",
    "\n",
    "\n",
    "In the training set, each image has been labeled with the position of the ground truth bounding box and the category of the object it contains. Whereas for the anchors, how do we know and assign a ground truth bounding box similar to an anchor?\n",
    "\n",
    "\n",
    "Suppose the anchors in the image are $A_1, A_2, \\ldots, A_{n_a}$, the ground truth bboxes are $B_1, B_2, \\ldots, B_ {n_b}$, and $n_a \\geq n_b$. Define the matrix $\\boldsymbol{X} \\in \\mathbb{R}^{n_a \\times n_b}$, where the element $x_ {ij}$ in the $i$th row and $j$th column is the IoU of anchor $A_i$ and bbox $B_j$.\n",
    "\n",
    "\n",
    "First, we find the largest element in the matrix $\\boldsymbol{X}$, and set the row index and column index of the element as $i_1, j_1$, respectively. We assign the bbox $B_ {j_1}$ to the anchor $A_ {i_1}$. Obviously, the anchor $A_ {i_1}$ and the bbox $B_ {j_1}$ have the highest similarity among all pairs. Next, all elements on the $i_1$ row and $j_1$ column in the matrix $\\boldsymbol{X}$ are discarded. \n",
    "\n",
    "Find the largest remaining element in the matrix $\\boldsymbol{X}$, and set the element's row index and column index as $i_2, j_2$. Again assign the bbox $B_ {j_2}$ to the anchor $A_ {i_2}$, and then discard all elements in the $i_2$ row and $j_2$ column of the matrix $\\boldsymbol{X}$. At this point, the matrix $\\boldsymbol{X}$ have discarded two rows and two columns.\n",
    "\n",
    "Keep on looping until all $n_b$ column elements in the matrix $\\boldsymbol{X}$ are discarded. At this time, we have assigned a bbox for $n_b$_num anchors. Next, we only traverse the remaining $n_a-n_b$ anchors: given the anchor $A_i$, find the bbox $B_j$ that has the largest IoU with $A_i$ according to the $i$ row of the matrix $\\boldsymbol{X}$. And assign $B_j$ to anchor $A_i$, only when the IoU is greater than our pre-defined threshold.\n",
    "\n",
    "\n",
    "As shown in figure, assuming that the maximum value in the matrix $\\boldsymbol{X}$ is $x_{23}$, we will assign bbox $B_3$ to anchor $A_2$. Then, discard all elements in row 2 and col 3 of the matrix, find the largest remaining element $x_{71}$, and assign bbox $B_1$ to anchor $A_7$. Next, discard all elements in row 7 and col 1 of the matrix, find the largest remaining element $x_{54}$, and assign a bbox $B_4$ to anchor $A_5$. Finally, discard all elements in row 5 and col 4. Find the remaining largest element $x_{92}$, assign a bbox $B_2$ to anchor $A_9$. After that, we only need to traverse the remaining anchors except $A_2, A_5, A_7, A_9$, and decide whether to assign bboxes according to the threshold.\n",
    "\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5vsc1hcg8.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "\n",
    "\n",
    "Now we can label the category and offset of the anchors. If an anchor $A$ is assigned a bbox $B$, set the category of the anchor box $A$ same as $B$, and based on the relative position of the central coordinates of $B$ and $A$ and relative size of each box, we mark the offset of anchor $A$. \n",
    "\n",
    "Because the positions and sizes of the boxes are different, we usually require some special transformations to make the distribution of the offset more uniform and easier to fit. \n",
    "\n",
    "Let the center coordinates of the anchor box $A$ and its assigned bbox $B$ be $(x_a, y_a)$ and $(x_b, y_b)$, and the widths of $A$ and $B$ are $w_a$ and $w_b$, with heights of $h_a$ and $h_b$, respectively. A common way is to mark the offset of $A$ as\n",
    "\n",
    "$$\n",
    "\\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x},\n",
    "\\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y},\n",
    "\\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w},\n",
    "\\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),\n",
    "$$\n",
    "\n",
    "\n",
    "The default value of the constant is $ \\ mu_x = \\ mu_y = \\ mu_w = \\ mu_h = 0, \\ sigma_x = \\ sigma_y = 0.1, \\ sigma_w = \\ sigma_h = 0.2 $. If an anchor box is not assigned a true bounding box, we only need to set the anchor box category as the background. Anchor boxes with category as background are usually called negative anchor boxes, and the rest are called positive anchor boxes.\n",
    "\n",
    "\n",
    "A specific example is demonstrated below. We define real bounding boxes for cats and dogs in the read image. The first element is the category (0 is dog and 1 is cat). The remaining 4 elements are the $ x $ and $ y $ axes in the upper left corner. Coordinates and $ x $ and $ y $ axis coordinates in the lower right corner (with a range between 0 and 1). Here, five anchor boxes to be labeled are constructed by the coordinates of the upper left corner and the lower right corner, which are respectively marked as $ A_0, \\ ldots, A_4 $ (the index in the program starts from 0). First draw the positions of these anchor boxes and real bounding boxes in the image.\n",
    "\n",
    "The default value of the constant is $\\mu_x = \\mu_y = \\mu_w = \\mu_h = 0, \\sigma_x=\\sigma_y=0.1, \\sigma_w=\\sigma_h=0.2$\n",
    "\n",
    "If an anchor is not assigned with a bbox, we only need to set the anchor category as the background. Anchors with category as background are usually called negative anchors, and the rest are called positive anchors.\n",
    "\n",
    "Next is a specific example, defining bbox for cats and dogs in the image. The first element is the category (0 is dog and 1 is cat). The remaining 4 elements are the $x$ and $y$ coordinates in the upper left corner, $x$ and $y$ coordinates in the lower right corner (with a range between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "graffitiCellId": "id_dc3fn4d",
    "id": "EE440F7DF1044779979C9EE9F4906CDC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/EE440F7DF1044779979C9EE9F4906CDC/q68lkdvgi3.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bbox_scale = torch.tensor((w, h, w, h), dtype=torch.float32)\n",
    "\n",
    "ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n",
    "                            [1, 0.55, 0.2, 0.9, 0.88]])\n",
    "                            \n",
    "anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n",
    "                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n",
    "                    [0.57, 0.3, 0.92, 0.9]])\n",
    "\n",
    "fig = d2l.plt.imshow(img)\n",
    "show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')\n",
    "show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "graffitiCellId": "id_s0tnkxy",
    "id": "82C2532A165E49E38882A610B323F41C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0536, 0.0000],\n",
       "        [0.1417, 0.0000],\n",
       "        [0.0000, 0.5657],\n",
       "        [0.0000, 0.2059],\n",
       "        [0.0000, 0.7459]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_jaccard(anchors, ground_truth[:, 1:]) # test compute_jaccard function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_iwux97l",
    "id": "5DF900CB5F5C4E62B1A29F25C0631CA6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Next is the `MultiBoxTarget` function to mark class and bias for anchor boxes. The function set 0 for background, and set int index for obejct classes thereafter(1 for dog, 2 for cat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "graffitiCellId": "id_t0k8dlx",
    "id": "F752953425DE4B9FAFB90D799B98EFA4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_anchor(bb, anchor, jaccard_threshold=0.5):\n",
    "    \"\"\"\n",
    "    # assign ground truth bboxes for anchor, anchor as standardized(xmin, ymin, xmax, ymax).\n",
    "    Args:\n",
    "        bb: ground truth bounding box, shape:（nb, 4）\n",
    "        anchor: anchor to be assigned, shape:（na, 4）\n",
    "        jaccard_threshold: pre-defined threshold\n",
    "    Returns:\n",
    "        assigned_idx: shape: (na, ), the index of ground truth bb for every anchor assigned\n",
    "        returns -1 if no bb assigned\n",
    "    \"\"\"\n",
    "    \n",
    "    na = anchor.shape[0] \n",
    "    nb = bb.shape[0]\n",
    "    jaccard = compute_jaccard(anchor, bb).detach().cpu().numpy() # shape: (na, nb)\n",
    "    assigned_idx = np.ones(na) * -1  # initial index all set to -1\n",
    "    \n",
    "    # assign an anchor to bb (jaccard_threshold not required)\n",
    "    jaccard_cp = jaccard.copy()\n",
    "    for j in range(nb):\n",
    "        i = np.argmax(jaccard_cp[:, j])\n",
    "        assigned_idx[i] = j\n",
    "        jaccard_cp[i, :] = float(\"-inf\") # negative inf\n",
    "     \n",
    "    # deal with unassigned anchor, jaccard_threshold required\n",
    "    for i in range(na):\n",
    "        if assigned_idx[i] == -1:\n",
    "            j = np.argmax(jaccard[i, :])\n",
    "            if jaccard[i, j] >= jaccard_threshold:\n",
    "                assigned_idx[i] = j\n",
    "                \n",
    "    return torch.tensor(assigned_idx, dtype=torch.long)\n",
    "\n",
    "\n",
    "def xy_to_cxcy(xy):\n",
    "    \"\"\"\n",
    "    change (x_min, y_min, x_max, y_max) anchor to (center_x, center_y, w, h) format\n",
    "    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    \n",
    "    Args:\n",
    "        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
    "    Returns: \n",
    "        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
    "    \"\"\"\n",
    "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n",
    "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n",
    "\n",
    "\n",
    "def MultiBoxTarget(anchor, label):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        anchor: torch tensor, input anchor, generated by MultiBoxPrior, shape:（1，num_anchors，4）\n",
    "        label: label, shape (bn, max_num of true bbox for every image, 5)\n",
    "               in the 2nd-dim, if not enough anchor box, use -1 to fill blank\n",
    "               5 in the last dim is [class label, 4 coordinates]\n",
    "    Returns:\n",
    "        list, [bbox_offset, bbox_mask, cls_labels]\n",
    "        bbox_offset: bias for every anchor box, shape (batch_num，num_anchors*4)\n",
    "        bbox_mask: shape like bbox_offset, mask for every anchor, foreground or background, \n",
    "        match with bias, background is 0, foreground is 1\n",
    "        cls_labels: class for every anchor, 0 for background, shape (batch_num，num_anchors)\n",
    "    \"\"\"\n",
    "    assert len(anchor.shape) == 3 and len(label.shape) == 3\n",
    "    bn = label.shape[0]\n",
    "    \n",
    "    def MultiBoxTarget_one(anc, lab, eps=1e-6):\n",
    "        \"\"\"\n",
    "        MultiBoxTarget help function, deal with one in the batch\n",
    "        Args:\n",
    "            anc: shape of (num_anchors, 4)\n",
    "            lab: shape of (num_groundtruth_bbox, 5), 5 is[class_label, 4 coordinates]\n",
    "            eps: smoothing param, avoid log0\n",
    "        Returns:\n",
    "            offset: (num_anchors*4, )\n",
    "            bbox_mask: (num_anchors*4, ), 0 for background, 1 for non-background\n",
    "            cls_labels: (num_anchors, 4), 0 for background\n",
    "        \"\"\"\n",
    "        an = anc.shape[0]\n",
    "\n",
    "        assigned_idx = assign_anchor(lab[:, 1:], anc) # (num_anchors, )\n",
    "        print(\"a: \",  assigned_idx.shape)\n",
    "        print(assigned_idx)\n",
    "        bbox_mask = ((assigned_idx >= 0).float().unsqueeze(-1)).repeat(1, 4) # (num_anchors, 4)\n",
    "        print(\"b: \" , bbox_mask.shape)\n",
    "        print(bbox_mask)\n",
    "\n",
    "        cls_labels = torch.zeros(an, dtype=torch.long) # 0 for background\n",
    "        assigned_bb = torch.zeros((an, 4), dtype=torch.float32) # bb coordinates for matched anchor\n",
    "        for i in range(an):\n",
    "            bb_idx = assigned_idx[i]\n",
    "            if bb_idx >= 0: # non-background\n",
    "                cls_labels[i] = lab[bb_idx, 0].long().item() + 1 # add 1\n",
    "                assigned_bb[i, :] = lab[bb_idx, 1:]\n",
    "        \n",
    "        # calculate bias\n",
    "        center_anc = xy_to_cxcy(anc) # (center_x, center_y, w, h)\n",
    "        center_assigned_bb = xy_to_cxcy(assigned_bb)\n",
    "\n",
    "        offset_xy = 10.0 * (center_assigned_bb[:, :2] - center_anc[:, :2]) / center_anc[:, 2:]\n",
    "        offset_wh = 5.0 * torch.log(eps + center_assigned_bb[:, 2:] / center_anc[:, 2:])\n",
    "        offset = torch.cat([offset_xy, offset_wh], dim = 1) * bbox_mask # (num_anchors, 4)\n",
    "\n",
    "        return offset.view(-1), bbox_mask.view(-1), cls_labels\n",
    "    \n",
    "    # output\n",
    "    batch_offset = []\n",
    "    batch_mask = []\n",
    "    batch_cls_labels = []\n",
    "    for b in range(bn):\n",
    "        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[0, :, :], label[b, :, :])\n",
    "        \n",
    "        batch_offset.append(offset)\n",
    "        batch_mask.append(bbox_mask)\n",
    "        batch_cls_labels.append(cls_labels)\n",
    "    \n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    cls_labels = torch.stack(batch_cls_labels)\n",
    "    \n",
    "    return [bbox_offset, bbox_mask, cls_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_3c4rh47",
    "id": "EA40302D9DC54A038D0991E2B548F865",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Use `unsqueeze` to add sample dim for anchor and ground truth bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "graffitiCellId": "id_p5u2kh5",
    "id": "962B76B2F8774902AF4F0FC50CE0FE78",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  torch.Size([5])\n",
      "tensor([-1,  0,  1, -1,  1])\n",
      "b:  torch.Size([5, 4])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "labels = MultiBoxTarget(anchors.unsqueeze(dim=0),\n",
    "                        ground_truth.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_80n1mvs",
    "id": "D4F6080AAADB4E828BD2A0B23D5F4B76",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Return 3 values, all are `Tensor`. The last value is the marked class of anchor box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "graffitiCellId": "id_qq5gg2i",
    "id": "134F85B4431D4FD783496D7B5B33048C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 0, 2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[2] # class label, background, dog, cat, background, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "graffitiCellId": "id_h5f7kko",
    "id": "9719FFAC72674C3A84B5CC95479E7844",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "         1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1] # mask value, shape(batch_num, num_anchors*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "graffitiCellId": "id_473p2sy",
    "id": "61A3474A4EE44D8FAF829A23DDBAA208",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  1.4000e+00,\n",
       "          1.0000e+01,  2.5940e+00,  7.1754e+00, -1.2000e+00,  2.6882e-01,\n",
       "          1.6824e+00, -1.5655e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "         -0.0000e+00, -5.7143e-01, -1.0000e+00,  4.1723e-06,  6.2582e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0] # 4 bias for every anchor box, 0 for background anchor box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_y2qa1qt",
    "id": "4C21C5BE5BDC4D308E9C55E1D5250B16",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Output Prediction Bounding Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_80q8t71",
    "id": "11D878A176D5451193A08B7E46F331CB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In the prediction, we first generate multiple anchors for the image, and predict the categories and offsets for these anchors one by one. Then, we get the predicted boundary box based on the anchor and its predicted offset. When the number of anchors is large, a lot of similar predicted bounding boxes may show up on the same object. To make the results more concise, we can remove similar boundary box prediction. A commonly used method is called non-maximum suppression (NMS).\n",
    "\n",
    "For a prediction bounding box $B$, the model calculates the prediction probability for each category. Let the largest prediction probability be $p$, and the category corresponding to the probability is the prediction category of $B$. \n",
    "\n",
    "We also call $p$ the confidence of the predicted bounding box $B$. On the same image, we sort the prediction bounding boxes of the non-background prediction categories from high to low to get the list $L$. From $L$, select the most confident prediction bounding box $B_1$ as the benchmark, and remove all non-benchmarked bounding boxes whose IoU with $B_1$ is greater than a certain pre-set threshold from $L$. At this point, $L$ retains the most confident prediction bounding box and removes other prediction bounding boxes similar to it.\n",
    "\n",
    "Next, from $L$, select the prediction bounding box $B_2$ with the second highest confidence as the benchmark, and remove all non-benchmarked bounding boxes whose IoU with $B_2$ is greater than a threshold from $L$. This process is repeated until all predicted bounding boxes in $L$ have been used as the benchmark. At this time, the IoU ratio of any pair of predicted bounding boxes in $L$ is less than the threshold. Finally, all predicted bounding boxes in the list $L$ are output.\n",
    "\n",
    "Let's look at a specific example below. First create 4 anchors. For simplicity, we assume that the prediction offset is all 0: the prediction bounding box is the anchor. Finally, we construct the predicted probabilities for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "graffitiCellId": "id_7ob7ov2",
    "id": "CD3C5B1557B746119EC3C1FD04D7AF54",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n",
    "                        [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\n",
    "offset_preds = torch.tensor([0.0] * (4 * len(anchors)))\n",
    "cls_probs = torch.tensor([[0., 0., 0., 0.,],  # prob for background\n",
    "                          [0.9, 0.8, 0.7, 0.1],  # prob for dog\n",
    "                          [0.1, 0.2, 0.3, 0.9]])  # prob for cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "graffitiCellId": "id_rhs2pam",
    "id": "9A3523CA158741A9BE840976BD280FCD",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/9A3523CA158741A9BE840976BD280FCD/q68nosv3k5.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the bouding boxes and their prob\n",
    "fig = d2l.plt.imshow(img)\n",
    "show_bboxes(fig.axes, anchors * bbox_scale,\n",
    "            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_pz0xz8f",
    "id": "2D54C8E798D3435384131A4C2E8508B4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Use `MultiBoxDetection` to perform non-maximum suppression on image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "graffitiCellId": "id_rt5r686",
    "id": "A67A98C985F6464789960CD04D518F34",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Pred_BB_Info = namedtuple(\"Pred_BB_Info\", [\"index\", \"class_id\", \"confidence\", \"xyxy\"])\n",
    "\n",
    "def non_max_suppression(bb_info_list, nms_threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        bb_info_list: Pred_BB_Info list, has pred class id, prob\n",
    "        nms_threshold: threshold\n",
    "    Returns:\n",
    "        output: Pred_BB_Info list, only keep filtered bbox using threshold \n",
    "    \"\"\"\n",
    "    output = []\n",
    "    # sorting according to prob from high to low\n",
    "    sorted_bb_info_list = sorted(bb_info_list, key = lambda x: x.confidence, reverse=True)\n",
    "    \n",
    "    # iterate through list and remove unnecessary output\n",
    "    while len(sorted_bb_info_list) != 0:\n",
    "        best = sorted_bb_info_list.pop(0)\n",
    "        output.append(best)\n",
    "        \n",
    "        if len(sorted_bb_info_list) == 0:\n",
    "            break\n",
    "\n",
    "        bb_xyxy = []\n",
    "        for bb in sorted_bb_info_list:\n",
    "            bb_xyxy.append(bb.xyxy)\n",
    "        \n",
    "        iou = compute_jaccard(torch.tensor([best.xyxy]), \n",
    "                              torch.tensor(bb_xyxy))[0] # shape: (len(sorted_bb_info_list), )\n",
    "        \n",
    "        n = len(sorted_bb_info_list)\n",
    "        sorted_bb_info_list = [sorted_bb_info_list[i] for i in range(n) if iou[i] <= nms_threshold]\n",
    "    return output\n",
    "\n",
    "\n",
    "def MultiBoxDetection(cls_prob, loc_pred, anchor, nms_threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        cls_prob: get prob for every anchor box after softmax, shape:(bn, pred_class+1, num_anchors)\n",
    "        loc_pred: bias for every anchor predicted, shape:(bn, num_anchors*4)\n",
    "        anchor: MultiBoxPrior default anchor output, shape: (1, num_anchors, 4)\n",
    "        nms_threshold: threshold\n",
    "    Returns:\n",
    "        all anchor, shape: (bn, num_anchors, 6)\n",
    "        each anchor represented as[class_id, confidence, xmin, ymin, xmax, ymax]\n",
    "        class_id=-1 means bacjground get removed in NMS\n",
    "    \"\"\"\n",
    "    assert len(cls_prob.shape) == 3 and len(loc_pred.shape) == 2 and len(anchor.shape) == 3\n",
    "    bn = cls_prob.shape[0]\n",
    "   \n",
    "    \n",
    "    def MultiBoxDetection_one(c_p, l_p, anc, nms_threshold = 0.5):\n",
    "        \"\"\"\n",
    "        MultiBoxDetection help function, deal with one in a batch\n",
    "        Args:\n",
    "            c_p: (pred_class+1, num_anchors)\n",
    "            l_p: (num_anchors*4, )\n",
    "            anc: (num_anchors, 4)\n",
    "            nms_threshold: threshold\n",
    "        Return:\n",
    "            output: (num_anchors, 6)\n",
    "        \"\"\"\n",
    "        pred_bb_num = c_p.shape[1]\n",
    "        anc = (anc + l_p.view(pred_bb_num, 4)).detach().cpu().numpy() # add bias\n",
    "        \n",
    "        confidence, class_id = torch.max(c_p, 0)\n",
    "        confidence = confidence.detach().cpu().numpy()\n",
    "        class_id = class_id.detach().cpu().numpy()\n",
    "        \n",
    "        pred_bb_info = [Pred_BB_Info(\n",
    "                            index = i,\n",
    "                            class_id = class_id[i] - 1, # positive class label starts from 0\n",
    "                            confidence = confidence[i],\n",
    "                            xyxy=[*anc[i]]) # xyxy is a list\n",
    "                        for i in range(pred_bb_num)]\n",
    "        \n",
    "        # postive class index\n",
    "        obj_bb_idx = [bb.index for bb in non_max_suppression(pred_bb_info, nms_threshold)]\n",
    "        \n",
    "        output = []\n",
    "        for bb in pred_bb_info:\n",
    "            output.append([\n",
    "                (bb.class_id if bb.index in obj_bb_idx else -1.0),\n",
    "                bb.confidence,\n",
    "                *bb.xyxy\n",
    "            ])\n",
    "            \n",
    "        return torch.tensor(output) # shape: (num_anchors, 6)\n",
    "    \n",
    "    batch_output = []\n",
    "    \n",
    "    for b in range(bn):\n",
    "        batch_output.append(MultiBoxDetection_one(cls_prob[b], loc_pred[b], anchor[0], nms_threshold))\n",
    "    \n",
    "    return torch.stack(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "graffitiCellId": "id_jynblea",
    "id": "A29A219B8C7046648EF5AFE3F45E0CB6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.9000,  0.1000,  0.0800,  0.5200,  0.9200],\n",
       "         [-1.0000,  0.8000,  0.0800,  0.2000,  0.5600,  0.9500],\n",
       "         [-1.0000,  0.7000,  0.1500,  0.3000,  0.6200,  0.9100],\n",
       "         [ 1.0000,  0.9000,  0.5500,  0.2000,  0.9000,  0.8800]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = MultiBoxDetection(\n",
    "    cls_probs.unsqueeze(dim=0), offset_preds.unsqueeze(dim=0),\n",
    "    anchors.unsqueeze(dim=0), nms_threshold=0.5)\n",
    "\n",
    "output # output is info for a pred box, shape (bn, num_anchors, 6)\n",
    "# 1st element: pred_class(0:dog, 1:cat, -1 means background removed)\n",
    "# 2nd element: confidence interval\n",
    "# 3-6 element: upper left x, y and lower right x,y coordinates(between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "graffitiCellId": "id_9glop5q",
    "id": "20C0E67E283D4D4BBF20B9E2A4F46915",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/20C0E67E283D4D4BBF20B9E2A4F46915/q68novaopk.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = d2l.plt.imshow(img)\n",
    "for i in output[0].detach().cpu().numpy():\n",
    "    if i[0] == -1:\n",
    "        continue\n",
    "    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])\n",
    "    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_95p6zbf",
    "id": "E9BC5338559643EC93B278A8DFFD82C3",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multi-size Object Detection\n",
    "\n",
    "Another way to increase the accuracy and reduce computation, we can perform generating different size anchor boxes for objects of different sizes. For example, we can generate more small-size anchors for samll-size objects and larger and fewer anchors for larger-size objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "graffitiCellId": "id_cmjn92g",
    "id": "4B95651E081141D68C632D83EB64E9D4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728, 561)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, h = img.size\n",
    "w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "graffitiCellId": "id_lsu3iq6",
    "id": "4240391E34A44AD389138A6B7342C9C8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "\n",
    "def display_anchors(fmap_w, fmap_h, s): # fmap controls how many anchor to be generated\n",
    "    fmap = torch.zeros((1, 10, fmap_h, fmap_w), dtype=torch.float32)\n",
    "    \n",
    "    # move all anchors to equally distribute on the image\n",
    "    offset_x, offset_y = 1.0/fmap_w, 1.0/fmap_h\n",
    "    anchors = d2l.MultiBoxPrior(fmap, sizes=s, ratios=[1, 2, 0.5]) + \\\n",
    "        torch.tensor([offset_x/2, offset_y/2, offset_x/2, offset_y/2])\n",
    "    \n",
    "    bbox_scale = torch.tensor([[w, h, w, h]], dtype=torch.float32)\n",
    "    d2l.show_bboxes(d2l.plt.imshow(img).axes,\n",
    "                    anchors[0] * bbox_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "graffitiCellId": "id_0lu1o67",
    "id": "FFD6EFB60BAC460A83015115A93376B0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/FFD6EFB60BAC460A83015115A93376B0/q68nxwzj2.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_anchors(fmap_w=4, fmap_h=2, s=[0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "graffitiCellId": "id_hyhlrm3",
    "id": "B4223FD7BD5D4320B1FDB53445264F79",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/B4223FD7BD5D4320B1FDB53445264F79/q68nxx7t4m.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_anchors(fmap_w=2, fmap_h=1, s=[0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "graffitiCellId": "id_8nnyfb5",
    "id": "1A763C2BB5BA404092B31C190A963A17",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/1A763C2BB5BA404092B31C190A963A17/q68nxxxbx1.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_anchors(fmap_w=1, fmap_h=1, s=[0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBE55A824CB140118F472F552943ABBB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In practice, we can remove the lower-confidence prediction bounding box before performing non-maximum suppression, thereby reducing the amount of calculation for non-maximum suppression. We can also filter the output for non-maximum suppression, for example, to keep only those results with higher confidence as the final output.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "* Generate multiple anchors with different sizes and aspect ratios around each pixel.\n",
    "* The IoU is the ratio of the intersection area and the union area of ​​two bounding boxes.\n",
    "* In the training set, there are two types of labels for each anchor box: one is the category of the target contained in the anchor box; the other is the offset of the true bounding box from the anchor box.\n",
    "* When predicting, you can use non-maximum suppression to remove similar prediction bounding boxes to make the results concise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
