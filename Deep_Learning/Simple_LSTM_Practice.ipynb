{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J13lYG03gVVM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzU2pCrYgbh6"
   },
   "outputs": [],
   "source": [
    "# defining simple univariate LSTM model\n",
    "from numpy import array\n",
    " \n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return array(X), array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "BvbRKDQ7gbkN",
    "outputId": "d6214275-7fbf-4732-b38f-99b674041ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] 40\n",
      "[20 30 40] 50\n",
      "[30 40 50] 60\n",
      "[40 50 60] 70\n",
      "[50 60 70] 80\n",
      "[60 70 80] 90\n"
     ]
    }
   ],
   "source": [
    "# define a simple input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "  print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9oz9rr57hS5j"
   },
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8g8WzJwg3pW"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout, LSTM\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.python.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HTMmSk6gboq"
   },
   "outputs": [],
   "source": [
    "# define single layer LSTM model\n",
    "# the hidden layer should have the input like [samples, timesteps, features]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features))) # 50 units\n",
    "model.add(Dense(1)) # means only output one value\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cW90UsYGgbrJ",
    "outputId": "cf774ad0-1420-4df8-9ccb-306a165ab1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6 samples\n",
      "Epoch 1/300\n",
      "6/6 - 3s - loss: 4596.8613\n",
      "Epoch 2/300\n",
      "6/6 - 0s - loss: 4560.2124\n",
      "Epoch 3/300\n",
      "6/6 - 0s - loss: 4523.5142\n",
      "Epoch 4/300\n",
      "6/6 - 0s - loss: 4486.0649\n",
      "Epoch 5/300\n",
      "6/6 - 0s - loss: 4448.0679\n",
      "Epoch 6/300\n",
      "6/6 - 0s - loss: 4409.4214\n",
      "Epoch 7/300\n",
      "6/6 - 0s - loss: 4370.7778\n",
      "Epoch 8/300\n",
      "6/6 - 0s - loss: 4331.8716\n",
      "Epoch 9/300\n",
      "6/6 - 0s - loss: 4292.5186\n",
      "Epoch 10/300\n",
      "6/6 - 0s - loss: 4252.9165\n",
      "Epoch 11/300\n",
      "6/6 - 0s - loss: 4211.4116\n",
      "Epoch 12/300\n",
      "6/6 - 0s - loss: 4168.1987\n",
      "Epoch 13/300\n",
      "6/6 - 0s - loss: 4124.2632\n",
      "Epoch 14/300\n",
      "6/6 - 0s - loss: 4078.7717\n",
      "Epoch 15/300\n",
      "6/6 - 0s - loss: 4031.2334\n",
      "Epoch 16/300\n",
      "6/6 - 0s - loss: 3981.5203\n",
      "Epoch 17/300\n",
      "6/6 - 0s - loss: 3929.6836\n",
      "Epoch 18/300\n",
      "6/6 - 0s - loss: 3875.0696\n",
      "Epoch 19/300\n",
      "6/6 - 0s - loss: 3820.8406\n",
      "Epoch 20/300\n",
      "6/6 - 0s - loss: 3765.2507\n",
      "Epoch 21/300\n",
      "6/6 - 0s - loss: 3707.5461\n",
      "Epoch 22/300\n",
      "6/6 - 0s - loss: 3648.4258\n",
      "Epoch 23/300\n",
      "6/6 - 0s - loss: 3585.7566\n",
      "Epoch 24/300\n",
      "6/6 - 0s - loss: 3519.9929\n",
      "Epoch 25/300\n",
      "6/6 - 0s - loss: 3453.5654\n",
      "Epoch 26/300\n",
      "6/6 - 0s - loss: 3385.4988\n",
      "Epoch 27/300\n",
      "6/6 - 0s - loss: 3313.6077\n",
      "Epoch 28/300\n",
      "6/6 - 0s - loss: 3239.5137\n",
      "Epoch 29/300\n",
      "6/6 - 0s - loss: 3160.9832\n",
      "Epoch 30/300\n",
      "6/6 - 0s - loss: 3081.0771\n",
      "Epoch 31/300\n",
      "6/6 - 0s - loss: 3005.7449\n",
      "Epoch 32/300\n",
      "6/6 - 0s - loss: 2928.9866\n",
      "Epoch 33/300\n",
      "6/6 - 0s - loss: 2854.5723\n",
      "Epoch 34/300\n",
      "6/6 - 0s - loss: 2776.7937\n",
      "Epoch 35/300\n",
      "6/6 - 0s - loss: 2696.3679\n",
      "Epoch 36/300\n",
      "6/6 - 0s - loss: 2616.6340\n",
      "Epoch 37/300\n",
      "6/6 - 0s - loss: 2537.8997\n",
      "Epoch 38/300\n",
      "6/6 - 0s - loss: 2454.6143\n",
      "Epoch 39/300\n",
      "6/6 - 0s - loss: 2368.0020\n",
      "Epoch 40/300\n",
      "6/6 - 0s - loss: 2280.2839\n",
      "Epoch 41/300\n",
      "6/6 - 0s - loss: 2180.2537\n",
      "Epoch 42/300\n",
      "6/6 - 0s - loss: 2075.3879\n",
      "Epoch 43/300\n",
      "6/6 - 0s - loss: 1967.3412\n",
      "Epoch 44/300\n",
      "6/6 - 0s - loss: 1853.6547\n",
      "Epoch 45/300\n",
      "6/6 - 0s - loss: 1733.5052\n",
      "Epoch 46/300\n",
      "6/6 - 0s - loss: 1612.1637\n",
      "Epoch 47/300\n",
      "6/6 - 0s - loss: 1487.7831\n",
      "Epoch 48/300\n",
      "6/6 - 0s - loss: 1362.9552\n",
      "Epoch 49/300\n",
      "6/6 - 0s - loss: 1244.2568\n",
      "Epoch 50/300\n",
      "6/6 - 0s - loss: 1135.3859\n",
      "Epoch 51/300\n",
      "6/6 - 0s - loss: 1030.7399\n",
      "Epoch 52/300\n",
      "6/6 - 0s - loss: 931.4619\n",
      "Epoch 53/300\n",
      "6/6 - 0s - loss: 830.5256\n",
      "Epoch 54/300\n",
      "6/6 - 0s - loss: 728.0754\n",
      "Epoch 55/300\n",
      "6/6 - 0s - loss: 629.1539\n",
      "Epoch 56/300\n",
      "6/6 - 0s - loss: 532.3801\n",
      "Epoch 57/300\n",
      "6/6 - 0s - loss: 439.2235\n",
      "Epoch 58/300\n",
      "6/6 - 0s - loss: 354.7768\n",
      "Epoch 59/300\n",
      "6/6 - 0s - loss: 279.3474\n",
      "Epoch 60/300\n",
      "6/6 - 0s - loss: 214.4190\n",
      "Epoch 61/300\n",
      "6/6 - 0s - loss: 164.1374\n",
      "Epoch 62/300\n",
      "6/6 - 0s - loss: 130.6269\n",
      "Epoch 63/300\n",
      "6/6 - 0s - loss: 114.3439\n",
      "Epoch 64/300\n",
      "6/6 - 0s - loss: 109.8627\n",
      "Epoch 65/300\n",
      "6/6 - 0s - loss: 114.4619\n",
      "Epoch 66/300\n",
      "6/6 - 0s - loss: 124.0501\n",
      "Epoch 67/300\n",
      "6/6 - 0s - loss: 133.2760\n",
      "Epoch 68/300\n",
      "6/6 - 0s - loss: 140.3223\n",
      "Epoch 69/300\n",
      "6/6 - 0s - loss: 142.6519\n",
      "Epoch 70/300\n",
      "6/6 - 0s - loss: 140.2648\n",
      "Epoch 71/300\n",
      "6/6 - 0s - loss: 134.0904\n",
      "Epoch 72/300\n",
      "6/6 - 0s - loss: 125.0950\n",
      "Epoch 73/300\n",
      "6/6 - 0s - loss: 113.2318\n",
      "Epoch 74/300\n",
      "6/6 - 0s - loss: 100.0709\n",
      "Epoch 75/300\n",
      "6/6 - 0s - loss: 86.7695\n",
      "Epoch 76/300\n",
      "6/6 - 0s - loss: 74.7030\n",
      "Epoch 77/300\n",
      "6/6 - 0s - loss: 64.1476\n",
      "Epoch 78/300\n",
      "6/6 - 0s - loss: 56.3004\n",
      "Epoch 79/300\n",
      "6/6 - 0s - loss: 50.0568\n",
      "Epoch 80/300\n",
      "6/6 - 0s - loss: 45.2361\n",
      "Epoch 81/300\n",
      "6/6 - 0s - loss: 41.1648\n",
      "Epoch 82/300\n",
      "6/6 - 0s - loss: 37.8352\n",
      "Epoch 83/300\n",
      "6/6 - 0s - loss: 35.0135\n",
      "Epoch 84/300\n",
      "6/6 - 0s - loss: 32.6409\n",
      "Epoch 85/300\n",
      "6/6 - 0s - loss: 30.0956\n",
      "Epoch 86/300\n",
      "6/6 - 0s - loss: 27.5006\n",
      "Epoch 87/300\n",
      "6/6 - 0s - loss: 24.7269\n",
      "Epoch 88/300\n",
      "6/6 - 0s - loss: 21.9218\n",
      "Epoch 89/300\n",
      "6/6 - 0s - loss: 19.3226\n",
      "Epoch 90/300\n",
      "6/6 - 0s - loss: 17.0632\n",
      "Epoch 91/300\n",
      "6/6 - 0s - loss: 15.3483\n",
      "Epoch 92/300\n",
      "6/6 - 0s - loss: 14.2423\n",
      "Epoch 93/300\n",
      "6/6 - 0s - loss: 13.4493\n",
      "Epoch 94/300\n",
      "6/6 - 0s - loss: 12.3494\n",
      "Epoch 95/300\n",
      "6/6 - 0s - loss: 11.0070\n",
      "Epoch 96/300\n",
      "6/6 - 0s - loss: 9.6842\n",
      "Epoch 97/300\n",
      "6/6 - 0s - loss: 8.8371\n",
      "Epoch 98/300\n",
      "6/6 - 0s - loss: 8.4871\n",
      "Epoch 99/300\n",
      "6/6 - 0s - loss: 8.2578\n",
      "Epoch 100/300\n",
      "6/6 - 0s - loss: 7.9458\n",
      "Epoch 101/300\n",
      "6/6 - 0s - loss: 7.5961\n",
      "Epoch 102/300\n",
      "6/6 - 0s - loss: 7.2146\n",
      "Epoch 103/300\n",
      "6/6 - 0s - loss: 6.8241\n",
      "Epoch 104/300\n",
      "6/6 - 0s - loss: 6.4365\n",
      "Epoch 105/300\n",
      "6/6 - 0s - loss: 6.1166\n",
      "Epoch 106/300\n",
      "6/6 - 0s - loss: 5.8307\n",
      "Epoch 107/300\n",
      "6/6 - 0s - loss: 5.5539\n",
      "Epoch 108/300\n",
      "6/6 - 0s - loss: 5.2621\n",
      "Epoch 109/300\n",
      "6/6 - 0s - loss: 4.9405\n",
      "Epoch 110/300\n",
      "6/6 - 0s - loss: 4.6022\n",
      "Epoch 111/300\n",
      "6/6 - 0s - loss: 4.2929\n",
      "Epoch 112/300\n",
      "6/6 - 0s - loss: 4.0716\n",
      "Epoch 113/300\n",
      "6/6 - 0s - loss: 3.8728\n",
      "Epoch 114/300\n",
      "6/6 - 0s - loss: 3.6955\n",
      "Epoch 115/300\n",
      "6/6 - 0s - loss: 3.5343\n",
      "Epoch 116/300\n",
      "6/6 - 0s - loss: 3.3898\n",
      "Epoch 117/300\n",
      "6/6 - 0s - loss: 3.2418\n",
      "Epoch 118/300\n",
      "6/6 - 0s - loss: 3.0884\n",
      "Epoch 119/300\n",
      "6/6 - 0s - loss: 2.9220\n",
      "Epoch 120/300\n",
      "6/6 - 0s - loss: 2.7572\n",
      "Epoch 121/300\n",
      "6/6 - 0s - loss: 2.5981\n",
      "Epoch 122/300\n",
      "6/6 - 0s - loss: 2.4430\n",
      "Epoch 123/300\n",
      "6/6 - 0s - loss: 2.2935\n",
      "Epoch 124/300\n",
      "6/6 - 0s - loss: 2.1469\n",
      "Epoch 125/300\n",
      "6/6 - 0s - loss: 2.0026\n",
      "Epoch 126/300\n",
      "6/6 - 0s - loss: 1.8606\n",
      "Epoch 127/300\n",
      "6/6 - 0s - loss: 1.7225\n",
      "Epoch 128/300\n",
      "6/6 - 0s - loss: 1.5893\n",
      "Epoch 129/300\n",
      "6/6 - 0s - loss: 1.4704\n",
      "Epoch 130/300\n",
      "6/6 - 0s - loss: 1.3546\n",
      "Epoch 131/300\n",
      "6/6 - 0s - loss: 1.2424\n",
      "Epoch 132/300\n",
      "6/6 - 0s - loss: 1.1344\n",
      "Epoch 133/300\n",
      "6/6 - 0s - loss: 1.0306\n",
      "Epoch 134/300\n",
      "6/6 - 0s - loss: 0.9316\n",
      "Epoch 135/300\n",
      "6/6 - 0s - loss: 0.8381\n",
      "Epoch 136/300\n",
      "6/6 - 0s - loss: 0.7490\n",
      "Epoch 137/300\n",
      "6/6 - 0s - loss: 0.6622\n",
      "Epoch 138/300\n",
      "6/6 - 0s - loss: 0.5802\n",
      "Epoch 139/300\n",
      "6/6 - 0s - loss: 0.5074\n",
      "Epoch 140/300\n",
      "6/6 - 0s - loss: 0.4410\n",
      "Epoch 141/300\n",
      "6/6 - 0s - loss: 0.3806\n",
      "Epoch 142/300\n",
      "6/6 - 0s - loss: 0.3254\n",
      "Epoch 143/300\n",
      "6/6 - 0s - loss: 0.2744\n",
      "Epoch 144/300\n",
      "6/6 - 0s - loss: 0.2278\n",
      "Epoch 145/300\n",
      "6/6 - 0s - loss: 0.1860\n",
      "Epoch 146/300\n",
      "6/6 - 0s - loss: 0.1543\n",
      "Epoch 147/300\n",
      "6/6 - 0s - loss: 0.1301\n",
      "Epoch 148/300\n",
      "6/6 - 0s - loss: 0.1090\n",
      "Epoch 149/300\n",
      "6/6 - 0s - loss: 0.0914\n",
      "Epoch 150/300\n",
      "6/6 - 0s - loss: 0.0777\n",
      "Epoch 151/300\n",
      "6/6 - 0s - loss: 0.0664\n",
      "Epoch 152/300\n",
      "6/6 - 0s - loss: 0.0571\n",
      "Epoch 153/300\n",
      "6/6 - 0s - loss: 0.0493\n",
      "Epoch 154/300\n",
      "6/6 - 0s - loss: 0.0424\n",
      "Epoch 155/300\n",
      "6/6 - 0s - loss: 0.0364\n",
      "Epoch 156/300\n",
      "6/6 - 0s - loss: 0.0308\n",
      "Epoch 157/300\n",
      "6/6 - 0s - loss: 0.0258\n",
      "Epoch 158/300\n",
      "6/6 - 0s - loss: 0.0212\n",
      "Epoch 159/300\n",
      "6/6 - 0s - loss: 0.0172\n",
      "Epoch 160/300\n",
      "6/6 - 0s - loss: 0.0137\n",
      "Epoch 161/300\n",
      "6/6 - 0s - loss: 0.0109\n",
      "Epoch 162/300\n",
      "6/6 - 0s - loss: 0.0087\n",
      "Epoch 163/300\n",
      "6/6 - 0s - loss: 0.0072\n",
      "Epoch 164/300\n",
      "6/6 - 0s - loss: 0.0062\n",
      "Epoch 165/300\n",
      "6/6 - 0s - loss: 0.0056\n",
      "Epoch 166/300\n",
      "6/6 - 0s - loss: 0.0052\n",
      "Epoch 167/300\n",
      "6/6 - 0s - loss: 0.0050\n",
      "Epoch 168/300\n",
      "6/6 - 0s - loss: 0.0048\n",
      "Epoch 169/300\n",
      "6/6 - 0s - loss: 0.0047\n",
      "Epoch 170/300\n",
      "6/6 - 0s - loss: 0.0044\n",
      "Epoch 171/300\n",
      "6/6 - 0s - loss: 0.0042\n",
      "Epoch 172/300\n",
      "6/6 - 0s - loss: 0.0039\n",
      "Epoch 173/300\n",
      "6/6 - 0s - loss: 0.0035\n",
      "Epoch 174/300\n",
      "6/6 - 0s - loss: 0.0032\n",
      "Epoch 175/300\n",
      "6/6 - 0s - loss: 0.0029\n",
      "Epoch 176/300\n",
      "6/6 - 0s - loss: 0.0026\n",
      "Epoch 177/300\n",
      "6/6 - 0s - loss: 0.0024\n",
      "Epoch 178/300\n",
      "6/6 - 0s - loss: 0.0023\n",
      "Epoch 179/300\n",
      "6/6 - 0s - loss: 0.0021\n",
      "Epoch 180/300\n",
      "6/6 - 0s - loss: 0.0020\n",
      "Epoch 181/300\n",
      "6/6 - 0s - loss: 0.0019\n",
      "Epoch 182/300\n",
      "6/6 - 0s - loss: 0.0017\n",
      "Epoch 183/300\n",
      "6/6 - 0s - loss: 0.0016\n",
      "Epoch 184/300\n",
      "6/6 - 0s - loss: 0.0014\n",
      "Epoch 185/300\n",
      "6/6 - 0s - loss: 0.0012\n",
      "Epoch 186/300\n",
      "6/6 - 0s - loss: 0.0010\n",
      "Epoch 187/300\n",
      "6/6 - 0s - loss: 8.6618e-04\n",
      "Epoch 188/300\n",
      "6/6 - 0s - loss: 7.3663e-04\n",
      "Epoch 189/300\n",
      "6/6 - 0s - loss: 6.3745e-04\n",
      "Epoch 190/300\n",
      "6/6 - 0s - loss: 5.6550e-04\n",
      "Epoch 191/300\n",
      "6/6 - 0s - loss: 5.1441e-04\n",
      "Epoch 192/300\n",
      "6/6 - 0s - loss: 4.7643e-04\n",
      "Epoch 193/300\n",
      "6/6 - 0s - loss: 4.4420e-04\n",
      "Epoch 194/300\n",
      "6/6 - 0s - loss: 4.1234e-04\n",
      "Epoch 195/300\n",
      "6/6 - 0s - loss: 3.7776e-04\n",
      "Epoch 196/300\n",
      "6/6 - 0s - loss: 3.4002e-04\n",
      "Epoch 197/300\n",
      "6/6 - 0s - loss: 3.0059e-04\n",
      "Epoch 198/300\n",
      "6/6 - 0s - loss: 2.6184e-04\n",
      "Epoch 199/300\n",
      "6/6 - 0s - loss: 2.2672e-04\n",
      "Epoch 200/300\n",
      "6/6 - 0s - loss: 1.9747e-04\n",
      "Epoch 201/300\n",
      "6/6 - 0s - loss: 1.7555e-04\n",
      "Epoch 202/300\n",
      "6/6 - 0s - loss: 1.6112e-04\n",
      "Epoch 203/300\n",
      "6/6 - 0s - loss: 1.5303e-04\n",
      "Epoch 204/300\n",
      "6/6 - 0s - loss: 1.4957e-04\n",
      "Epoch 205/300\n",
      "6/6 - 0s - loss: 1.4816e-04\n",
      "Epoch 206/300\n",
      "6/6 - 0s - loss: 1.4686e-04\n",
      "Epoch 207/300\n",
      "6/6 - 0s - loss: 1.4406e-04\n",
      "Epoch 208/300\n",
      "6/6 - 0s - loss: 1.3916e-04\n",
      "Epoch 209/300\n",
      "6/6 - 0s - loss: 1.3228e-04\n",
      "Epoch 210/300\n",
      "6/6 - 0s - loss: 1.2397e-04\n",
      "Epoch 211/300\n",
      "6/6 - 0s - loss: 1.1521e-04\n",
      "Epoch 212/300\n",
      "6/6 - 0s - loss: 1.0691e-04\n",
      "Epoch 213/300\n",
      "6/6 - 0s - loss: 9.9525e-05\n",
      "Epoch 214/300\n",
      "6/6 - 0s - loss: 9.3327e-05\n",
      "Epoch 215/300\n",
      "6/6 - 0s - loss: 8.8096e-05\n",
      "Epoch 216/300\n",
      "6/6 - 0s - loss: 8.3506e-05\n",
      "Epoch 217/300\n",
      "6/6 - 0s - loss: 7.9053e-05\n",
      "Epoch 218/300\n",
      "6/6 - 0s - loss: 7.4296e-05\n",
      "Epoch 219/300\n",
      "6/6 - 0s - loss: 6.9025e-05\n",
      "Epoch 220/300\n",
      "6/6 - 0s - loss: 6.2977e-05\n",
      "Epoch 221/300\n",
      "6/6 - 0s - loss: 5.6364e-05\n",
      "Epoch 222/300\n",
      "6/6 - 0s - loss: 4.9388e-05\n",
      "Epoch 223/300\n",
      "6/6 - 0s - loss: 4.2419e-05\n",
      "Epoch 224/300\n",
      "6/6 - 0s - loss: 3.5823e-05\n",
      "Epoch 225/300\n",
      "6/6 - 0s - loss: 2.9852e-05\n",
      "Epoch 226/300\n",
      "6/6 - 0s - loss: 2.4773e-05\n",
      "Epoch 227/300\n",
      "6/6 - 0s - loss: 2.0527e-05\n",
      "Epoch 228/300\n",
      "6/6 - 0s - loss: 1.7071e-05\n",
      "Epoch 229/300\n",
      "6/6 - 0s - loss: 1.4267e-05\n",
      "Epoch 230/300\n",
      "6/6 - 0s - loss: 1.1879e-05\n",
      "Epoch 231/300\n",
      "6/6 - 0s - loss: 9.7991e-06\n",
      "Epoch 232/300\n",
      "6/6 - 0s - loss: 7.9485e-06\n",
      "Epoch 233/300\n",
      "6/6 - 0s - loss: 6.2482e-06\n",
      "Epoch 234/300\n",
      "6/6 - 0s - loss: 4.8000e-06\n",
      "Epoch 235/300\n",
      "6/6 - 0s - loss: 3.5713e-06\n",
      "Epoch 236/300\n",
      "6/6 - 0s - loss: 2.6297e-06\n",
      "Epoch 237/300\n",
      "6/6 - 0s - loss: 1.9679e-06\n",
      "Epoch 238/300\n",
      "6/6 - 0s - loss: 1.5908e-06\n",
      "Epoch 239/300\n",
      "6/6 - 0s - loss: 1.4596e-06\n",
      "Epoch 240/300\n",
      "6/6 - 0s - loss: 1.5119e-06\n",
      "Epoch 241/300\n",
      "6/6 - 0s - loss: 1.6559e-06\n",
      "Epoch 242/300\n",
      "6/6 - 0s - loss: 1.8415e-06\n",
      "Epoch 243/300\n",
      "6/6 - 0s - loss: 1.9912e-06\n",
      "Epoch 244/300\n",
      "6/6 - 0s - loss: 2.0869e-06\n",
      "Epoch 245/300\n",
      "6/6 - 0s - loss: 2.0940e-06\n",
      "Epoch 246/300\n",
      "6/6 - 0s - loss: 2.0354e-06\n",
      "Epoch 247/300\n",
      "6/6 - 0s - loss: 1.9255e-06\n",
      "Epoch 248/300\n",
      "6/6 - 0s - loss: 1.7938e-06\n",
      "Epoch 249/300\n",
      "6/6 - 0s - loss: 1.6616e-06\n",
      "Epoch 250/300\n",
      "6/6 - 0s - loss: 1.5349e-06\n",
      "Epoch 251/300\n",
      "6/6 - 0s - loss: 1.4315e-06\n",
      "Epoch 252/300\n",
      "6/6 - 0s - loss: 1.3294e-06\n",
      "Epoch 253/300\n",
      "6/6 - 0s - loss: 1.2325e-06\n",
      "Epoch 254/300\n",
      "6/6 - 0s - loss: 1.1304e-06\n",
      "Epoch 255/300\n",
      "6/6 - 0s - loss: 1.0140e-06\n",
      "Epoch 256/300\n",
      "6/6 - 0s - loss: 8.8377e-07\n",
      "Epoch 257/300\n",
      "6/6 - 0s - loss: 7.4605e-07\n",
      "Epoch 258/300\n",
      "6/6 - 0s - loss: 6.0988e-07\n",
      "Epoch 259/300\n",
      "6/6 - 0s - loss: 4.8125e-07\n",
      "Epoch 260/300\n",
      "6/6 - 0s - loss: 3.6642e-07\n",
      "Epoch 261/300\n",
      "6/6 - 0s - loss: 2.8106e-07\n",
      "Epoch 262/300\n",
      "6/6 - 0s - loss: 2.1694e-07\n",
      "Epoch 263/300\n",
      "6/6 - 0s - loss: 1.7789e-07\n",
      "Epoch 264/300\n",
      "6/6 - 0s - loss: 1.5881e-07\n",
      "Epoch 265/300\n",
      "6/6 - 0s - loss: 1.4931e-07\n",
      "Epoch 266/300\n",
      "6/6 - 0s - loss: 1.4447e-07\n",
      "Epoch 267/300\n",
      "6/6 - 0s - loss: 1.3875e-07\n",
      "Epoch 268/300\n",
      "6/6 - 0s - loss: 1.3695e-07\n",
      "Epoch 269/300\n",
      "6/6 - 0s - loss: 1.3427e-07\n",
      "Epoch 270/300\n",
      "6/6 - 0s - loss: 1.2769e-07\n",
      "Epoch 271/300\n",
      "6/6 - 0s - loss: 1.2632e-07\n",
      "Epoch 272/300\n",
      "6/6 - 0s - loss: 1.2291e-07\n",
      "Epoch 273/300\n",
      "6/6 - 0s - loss: 1.2286e-07\n",
      "Epoch 274/300\n",
      "6/6 - 0s - loss: 1.2528e-07\n",
      "Epoch 275/300\n",
      "6/6 - 0s - loss: 1.2605e-07\n",
      "Epoch 276/300\n",
      "6/6 - 0s - loss: 1.3088e-07\n",
      "Epoch 277/300\n",
      "6/6 - 0s - loss: 1.2810e-07\n",
      "Epoch 278/300\n",
      "6/6 - 0s - loss: 1.2636e-07\n",
      "Epoch 279/300\n",
      "6/6 - 0s - loss: 1.2168e-07\n",
      "Epoch 280/300\n",
      "6/6 - 0s - loss: 1.1355e-07\n",
      "Epoch 281/300\n",
      "6/6 - 0s - loss: 1.0147e-07\n",
      "Epoch 282/300\n",
      "6/6 - 0s - loss: 9.1939e-08\n",
      "Epoch 283/300\n",
      "6/6 - 0s - loss: 8.0171e-08\n",
      "Epoch 284/300\n",
      "6/6 - 0s - loss: 7.0325e-08\n",
      "Epoch 285/300\n",
      "6/6 - 0s - loss: 6.2069e-08\n",
      "Epoch 286/300\n",
      "6/6 - 0s - loss: 5.2952e-08\n",
      "Epoch 287/300\n",
      "6/6 - 0s - loss: 4.7570e-08\n",
      "Epoch 288/300\n",
      "6/6 - 0s - loss: 4.0942e-08\n",
      "Epoch 289/300\n",
      "6/6 - 0s - loss: 3.4697e-08\n",
      "Epoch 290/300\n",
      "6/6 - 0s - loss: 2.9608e-08\n",
      "Epoch 291/300\n",
      "6/6 - 0s - loss: 2.4918e-08\n",
      "Epoch 292/300\n",
      "6/6 - 0s - loss: 2.0077e-08\n",
      "Epoch 293/300\n",
      "6/6 - 0s - loss: 1.6107e-08\n",
      "Epoch 294/300\n",
      "6/6 - 0s - loss: 1.2430e-08\n",
      "Epoch 295/300\n",
      "6/6 - 0s - loss: 9.8104e-09\n",
      "Epoch 296/300\n",
      "6/6 - 0s - loss: 7.7440e-09\n",
      "Epoch 297/300\n",
      "6/6 - 0s - loss: 6.3762e-09\n",
      "Epoch 298/300\n",
      "6/6 - 0s - loss: 5.4642e-09\n",
      "Epoch 299/300\n",
      "6/6 - 0s - loss: 4.8264e-09\n",
      "Epoch 300/300\n",
      "6/6 - 0s - loss: 4.4359e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4d0cff8eb8>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=300, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8HF6KciKgbtU",
    "outputId": "8751e741-6ec7-4397-afff-8444ae91a686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s\n",
      "[[102.622086]]\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "x_input = array([70, 80, 90])\n",
    "\n",
    "# the model accpets input as [samples, timesteps, features]\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "yhat = model.predict(x_input, verbose=2)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vXtYcZyKh-Rr"
   },
   "outputs": [],
   "source": [
    "## from above we can see that the model predicts very close to the value we expected\n",
    "## given that the dataset is just a toy example\n",
    "## a single layer LSTM is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLvkCzNbiQCv"
   },
   "outputs": [],
   "source": [
    "# define univariate stacked LSTM model\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "# return_seq=True to add more value in the output that can use as input to next layer\n",
    "model2.add(LSTM(50, activation='relu'))\n",
    "model2.add(Dense(1)) # still output single value\n",
    "model2.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcGScR94iQFO"
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model2.fit(X, y, epochs=150, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nB4e-Ps_iQH-",
    "outputId": "18321142-956b-43fd-f1e8-fcd274d3d933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s\n",
      "[[102.17309]]\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model2.predict(x_input, verbose=2)\n",
    "\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tz9dSsgAi4Dh"
   },
   "outputs": [],
   "source": [
    "## we then compare the different perofrmances of these two model\n",
    "## given that the dataset is just a toy example\n",
    "## a single layer LSTM is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GxTF4yti4K5"
   },
   "outputs": [],
   "source": [
    "# define univariate bidirectional LSTM model\n",
    "from tensorflow.python.keras.layers import Bidirectional\n",
    "\n",
    "model3 = Sequential()\n",
    "# wrap the first hidden layer in bidirecitonal layer\n",
    "model3.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "model3.add(Dense(1))\n",
    "model3.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQ8soKVVl9Pq"
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model3.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zUtOxigPmG30",
    "outputId": "900653a8-c7e8-4624-f056-9153753fe42d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s\n",
      "[[102.622086]]\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=2)\n",
    "\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "hjPQ0pnfmpbh",
    "outputId": "c99e18ab-fb06-4c83-8d32-fa94115d676f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  15  25]\n",
      " [ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]]\n"
     ]
    }
   ],
   "source": [
    "# define multivariate LSTM model\n",
    "# dataset generation\n",
    "\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "from numpy import hstack\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-hNw-RpnYhE"
   },
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNaDjZ_dobTH"
   },
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "z_cL4hTvnYjg",
    "outputId": "eff200d8-4d09-4c84-da72-bf66dfbab1dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3, 2) (7,)\n",
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] 65\n",
      "[[20 25]\n",
      " [30 35]\n",
      " [40 45]] 85\n",
      "[[30 35]\n",
      " [40 45]\n",
      " [50 55]] 105\n",
      "[[40 45]\n",
      " [50 55]\n",
      " [60 65]] 125\n",
      "[[50 55]\n",
      " [60 65]\n",
      " [70 75]] 145\n",
      "[[60 65]\n",
      " [70 75]\n",
      " [80 85]] 165\n",
      "[[70 75]\n",
      " [80 85]\n",
      " [90 95]] 185\n"
     ]
    }
   ],
   "source": [
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRaiiFsHpLo1"
   },
   "outputs": [],
   "source": [
    "n_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxCmqQU0nYl9"
   },
   "outputs": [],
   "source": [
    "# define LSTM model for multivariate dataset\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6MjBhM2NnYoy",
    "outputId": "e0093fbe-1178-4a6a-df1b-48c5cc462375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples\n",
      "Epoch 1/200\n",
      "7/7 - 4s - loss: 17388.7500\n",
      "Epoch 2/200\n",
      "7/7 - 0s - loss: 17062.9023\n",
      "Epoch 3/200\n",
      "7/7 - 0s - loss: 16775.4844\n",
      "Epoch 4/200\n",
      "7/7 - 0s - loss: 16490.3477\n",
      "Epoch 5/200\n",
      "7/7 - 0s - loss: 16186.4434\n",
      "Epoch 6/200\n",
      "7/7 - 0s - loss: 15831.6230\n",
      "Epoch 7/200\n",
      "7/7 - 0s - loss: 15414.6533\n",
      "Epoch 8/200\n",
      "7/7 - 0s - loss: 14921.7676\n",
      "Epoch 9/200\n",
      "7/7 - 0s - loss: 14368.2900\n",
      "Epoch 10/200\n",
      "7/7 - 0s - loss: 13796.4043\n",
      "Epoch 11/200\n",
      "7/7 - 0s - loss: 13161.5830\n",
      "Epoch 12/200\n",
      "7/7 - 0s - loss: 12438.1465\n",
      "Epoch 13/200\n",
      "7/7 - 0s - loss: 11570.6787\n",
      "Epoch 14/200\n",
      "7/7 - 0s - loss: 10510.8115\n",
      "Epoch 15/200\n",
      "7/7 - 0s - loss: 9284.1514\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 7941.7021\n",
      "Epoch 17/200\n",
      "7/7 - 0s - loss: 6481.1284\n",
      "Epoch 18/200\n",
      "7/7 - 0s - loss: 4902.2905\n",
      "Epoch 19/200\n",
      "7/7 - 0s - loss: 3410.5391\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 1991.7932\n",
      "Epoch 21/200\n",
      "7/7 - 0s - loss: 780.7370\n",
      "Epoch 22/200\n",
      "7/7 - 0s - loss: 191.6523\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 479.5687\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 1159.6084\n",
      "Epoch 25/200\n",
      "7/7 - 0s - loss: 1621.2721\n",
      "Epoch 26/200\n",
      "7/7 - 0s - loss: 1667.1224\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 1404.4338\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 1012.5799\n",
      "Epoch 29/200\n",
      "7/7 - 0s - loss: 626.4152\n",
      "Epoch 30/200\n",
      "7/7 - 0s - loss: 337.6904\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 159.8429\n",
      "Epoch 32/200\n",
      "7/7 - 0s - loss: 84.5046\n",
      "Epoch 33/200\n",
      "7/7 - 0s - loss: 87.4471\n",
      "Epoch 34/200\n",
      "7/7 - 0s - loss: 135.9349\n",
      "Epoch 35/200\n",
      "7/7 - 0s - loss: 191.1869\n",
      "Epoch 36/200\n",
      "7/7 - 0s - loss: 226.0590\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 228.3553\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 192.7876\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 123.0240\n",
      "Epoch 40/200\n",
      "7/7 - 0s - loss: 33.8398\n",
      "Epoch 41/200\n",
      "7/7 - 0s - loss: 18.9635\n",
      "Epoch 42/200\n",
      "7/7 - 0s - loss: 96.0027\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 108.8472\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 68.1318\n",
      "Epoch 45/200\n",
      "7/7 - 0s - loss: 22.2879\n",
      "Epoch 46/200\n",
      "7/7 - 0s - loss: 4.5877\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 18.7548\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 40.4885\n",
      "Epoch 49/200\n",
      "7/7 - 0s - loss: 47.2192\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 38.3184\n",
      "Epoch 51/200\n",
      "7/7 - 0s - loss: 22.3863\n",
      "Epoch 52/200\n",
      "7/7 - 0s - loss: 8.1975\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 4.4901\n",
      "Epoch 54/200\n",
      "7/7 - 0s - loss: 11.1870\n",
      "Epoch 55/200\n",
      "7/7 - 0s - loss: 14.8051\n",
      "Epoch 56/200\n",
      "7/7 - 0s - loss: 12.1488\n",
      "Epoch 57/200\n",
      "7/7 - 0s - loss: 8.2432\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 4.8016\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 2.5232\n",
      "Epoch 60/200\n",
      "7/7 - 0s - loss: 2.1203\n",
      "Epoch 61/200\n",
      "7/7 - 0s - loss: 3.1394\n",
      "Epoch 62/200\n",
      "7/7 - 0s - loss: 4.5349\n",
      "Epoch 63/200\n",
      "7/7 - 0s - loss: 5.2608\n",
      "Epoch 64/200\n",
      "7/7 - 0s - loss: 4.7880\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 3.4535\n",
      "Epoch 66/200\n",
      "7/7 - 0s - loss: 2.0241\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 1.1340\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 1.1303\n",
      "Epoch 69/200\n",
      "7/7 - 0s - loss: 1.8133\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 2.4343\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 2.3812\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 1.7347\n",
      "Epoch 73/200\n",
      "7/7 - 0s - loss: 0.9701\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.5835\n",
      "Epoch 75/200\n",
      "7/7 - 0s - loss: 0.6417\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.9296\n",
      "Epoch 77/200\n",
      "7/7 - 0s - loss: 1.1411\n",
      "Epoch 78/200\n",
      "7/7 - 0s - loss: 1.0962\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.8197\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.4927\n",
      "Epoch 81/200\n",
      "7/7 - 0s - loss: 0.3147\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.3647\n",
      "Epoch 83/200\n",
      "7/7 - 0s - loss: 0.5393\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.6539\n",
      "Epoch 85/200\n",
      "7/7 - 0s - loss: 0.6045\n",
      "Epoch 86/200\n",
      "7/7 - 0s - loss: 0.4328\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.2696\n",
      "Epoch 88/200\n",
      "7/7 - 0s - loss: 0.2169\n",
      "Epoch 89/200\n",
      "7/7 - 0s - loss: 0.2744\n",
      "Epoch 90/200\n",
      "7/7 - 0s - loss: 0.3435\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.3438\n",
      "Epoch 92/200\n",
      "7/7 - 0s - loss: 0.2636\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.1589\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.0996\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.1078\n",
      "Epoch 96/200\n",
      "7/7 - 0s - loss: 0.1484\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.1653\n",
      "Epoch 98/200\n",
      "7/7 - 0s - loss: 0.1340\n",
      "Epoch 99/200\n",
      "7/7 - 0s - loss: 0.0777\n",
      "Epoch 100/200\n",
      "7/7 - 0s - loss: 0.0388\n",
      "Epoch 101/200\n",
      "7/7 - 0s - loss: 0.0384\n",
      "Epoch 102/200\n",
      "7/7 - 0s - loss: 0.0617\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.0767\n",
      "Epoch 104/200\n",
      "7/7 - 0s - loss: 0.0637\n",
      "Epoch 105/200\n",
      "7/7 - 0s - loss: 0.0366\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.0232\n",
      "Epoch 107/200\n",
      "7/7 - 0s - loss: 0.0322\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.0482\n",
      "Epoch 109/200\n",
      "7/7 - 0s - loss: 0.0513\n",
      "Epoch 110/200\n",
      "7/7 - 0s - loss: 0.0383\n",
      "Epoch 111/200\n",
      "7/7 - 0s - loss: 0.0230\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.0192\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.0263\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.0323\n",
      "Epoch 115/200\n",
      "7/7 - 0s - loss: 0.0290\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.0193\n",
      "Epoch 117/200\n",
      "7/7 - 0s - loss: 0.0124\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.0132\n",
      "Epoch 119/200\n",
      "7/7 - 0s - loss: 0.0178\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.0189\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.0144\n",
      "Epoch 122/200\n",
      "7/7 - 0s - loss: 0.0090\n",
      "Epoch 123/200\n",
      "7/7 - 0s - loss: 0.0075\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.0098\n",
      "Epoch 125/200\n",
      "7/7 - 0s - loss: 0.0118\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.0107\n",
      "Epoch 127/200\n",
      "7/7 - 0s - loss: 0.0075\n",
      "Epoch 128/200\n",
      "7/7 - 0s - loss: 0.0056\n",
      "Epoch 129/200\n",
      "7/7 - 0s - loss: 0.0065\n",
      "Epoch 130/200\n",
      "7/7 - 0s - loss: 0.0081\n",
      "Epoch 131/200\n",
      "7/7 - 0s - loss: 0.0082\n",
      "Epoch 132/200\n",
      "7/7 - 0s - loss: 0.0065\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.0049\n",
      "Epoch 134/200\n",
      "7/7 - 0s - loss: 0.0048\n",
      "Epoch 135/200\n",
      "7/7 - 0s - loss: 0.0057\n",
      "Epoch 136/200\n",
      "7/7 - 0s - loss: 0.0060\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.0051\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.0040\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.0036\n",
      "Epoch 140/200\n",
      "7/7 - 0s - loss: 0.0040\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.0043\n",
      "Epoch 142/200\n",
      "7/7 - 0s - loss: 0.0039\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.0032\n",
      "Epoch 144/200\n",
      "7/7 - 0s - loss: 0.0028\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.0030\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.0032\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 0.0031\n",
      "Epoch 148/200\n",
      "7/7 - 0s - loss: 0.0026\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 0.0024\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.0025\n",
      "Epoch 151/200\n",
      "7/7 - 0s - loss: 0.0026\n",
      "Epoch 152/200\n",
      "7/7 - 0s - loss: 0.0026\n",
      "Epoch 153/200\n",
      "7/7 - 0s - loss: 0.0023\n",
      "Epoch 154/200\n",
      "7/7 - 0s - loss: 0.0021\n",
      "Epoch 155/200\n",
      "7/7 - 0s - loss: 0.0022\n",
      "Epoch 156/200\n",
      "7/7 - 0s - loss: 0.0022\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.0022\n",
      "Epoch 158/200\n",
      "7/7 - 0s - loss: 0.0020\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.0019\n",
      "Epoch 160/200\n",
      "7/7 - 0s - loss: 0.0019\n",
      "Epoch 161/200\n",
      "7/7 - 0s - loss: 0.0019\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.0019\n",
      "Epoch 163/200\n",
      "7/7 - 0s - loss: 0.0018\n",
      "Epoch 164/200\n",
      "7/7 - 0s - loss: 0.0017\n",
      "Epoch 165/200\n",
      "7/7 - 0s - loss: 0.0017\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.0017\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.0016\n",
      "Epoch 168/200\n",
      "7/7 - 0s - loss: 0.0015\n",
      "Epoch 169/200\n",
      "7/7 - 0s - loss: 0.0015\n",
      "Epoch 170/200\n",
      "7/7 - 0s - loss: 0.0015\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.0015\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.0014\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.0014\n",
      "Epoch 174/200\n",
      "7/7 - 0s - loss: 0.0013\n",
      "Epoch 175/200\n",
      "7/7 - 0s - loss: 0.0013\n",
      "Epoch 176/200\n",
      "7/7 - 0s - loss: 0.0013\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.0013\n",
      "Epoch 178/200\n",
      "7/7 - 0s - loss: 0.0012\n",
      "Epoch 179/200\n",
      "7/7 - 0s - loss: 0.0012\n",
      "Epoch 180/200\n",
      "7/7 - 0s - loss: 0.0012\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.0012\n",
      "Epoch 182/200\n",
      "7/7 - 0s - loss: 0.0011\n",
      "Epoch 183/200\n",
      "7/7 - 0s - loss: 0.0011\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.0011\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.0010\n",
      "Epoch 186/200\n",
      "7/7 - 0s - loss: 0.0010\n",
      "Epoch 187/200\n",
      "7/7 - 0s - loss: 9.9707e-04\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 9.6820e-04\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 9.4655e-04\n",
      "Epoch 190/200\n",
      "7/7 - 0s - loss: 9.2808e-04\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 9.0760e-04\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 8.8314e-04\n",
      "Epoch 193/200\n",
      "7/7 - 0s - loss: 8.5974e-04\n",
      "Epoch 194/200\n",
      "7/7 - 0s - loss: 8.4076e-04\n",
      "Epoch 195/200\n",
      "7/7 - 0s - loss: 8.2337e-04\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 8.0387e-04\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 7.8258e-04\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 7.6263e-04\n",
      "Epoch 199/200\n",
      "7/7 - 0s - loss: 7.4540e-04\n",
      "Epoch 200/200\n",
      "7/7 - 0s - loss: 7.2847e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4d0c286ac8>"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VdtU5O_GpIsh",
    "outputId": "aad544f6-a8a8-4d14-c0a5-0acfc1990806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s\n",
      "[[207.91911]]\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "x_input = array([[80, 85], [90, 95], [100, 105]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=2)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKE3HAoSPIWM"
   },
   "outputs": [],
   "source": [
    "## LSTM can also be used to predict multiple timesteps in the future\n",
    "## one choice is to use multi-step vector-output LSTM model\n",
    "## or Encoder-Decoder LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMaev-OhPIZM"
   },
   "outputs": [],
   "source": [
    "# a univariate multi-step vector-output stacked lstm example\n",
    "# first, again split a univariate sequence into samples\n",
    "\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0FGrSkZ81Cq"
   },
   "outputs": [],
   "source": [
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose the number of time steps for input and output\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "\n",
    "# split into samples and reshape to the form of model input\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l06IgHW481FN"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aZlrp38o-OMz",
    "outputId": "039d72f2-7228-4416-809f-90863fed0a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples\n",
      "Epoch 1/100\n",
      "5/5 - 4s - loss: 4404.3711\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 4364.3667\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 4323.2446\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 4280.0566\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 4233.9272\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 4183.9414\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 4128.7734\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 4066.2292\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 3993.3511\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 3907.4727\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 3806.3687\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 3684.9609\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 3540.2629\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 3368.2324\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 3163.5454\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 2921.1023\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 2648.8379\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 2352.1421\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 2034.2800\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 1696.3828\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 1352.1844\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 1011.1064\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 700.9982\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 443.9359\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 278.2747\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 244.3753\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 353.4951\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 517.6158\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 613.4164\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 601.0681\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 512.4484\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 396.3399\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 286.8024\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 203.7042\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 152.9439\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 128.7898\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 123.5349\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 128.4996\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 135.6367\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 138.7855\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 134.8910\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 122.5624\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 102.7900\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 79.4294\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 57.4868\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 44.5587\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 46.5063\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 58.9176\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 64.9876\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 57.7128\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 44.3965\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 33.2485\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 28.2186\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 28.8912\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 32.2668\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 35.3322\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 36.1222\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 34.3150\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 30.6502\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 26.4364\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 22.9780\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 20.9980\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 20.4469\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 20.5384\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 20.3061\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 19.2507\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 17.3956\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 15.3115\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 13.7483\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 12.7792\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 12.4304\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 12.3798\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 12.1842\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 11.6289\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 10.8489\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 9.9848\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 9.2415\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 8.6864\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 8.2531\n",
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 7.7962\n",
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 7.2078\n",
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 6.4684\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 5.8015\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 5.3140\n",
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 4.8363\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 4.4015\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 4.0172\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 3.6669\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 3.3592\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 3.1255\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 2.9239\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 2.7139\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 2.4844\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 2.3195\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 2.0444\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 1.8451\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 1.6980\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 1.5580\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 1.4273\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 1.3131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4d0cf219e8>"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YKYMlK2y-SwH",
    "outputId": "dcca81b2-26d4-48a2-9b43-feed994679d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s\n",
      "[[112.23638 125.94245]]\n"
     ]
    }
   ],
   "source": [
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=2)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkyF2Ytr-Ypv"
   },
   "outputs": [],
   "source": [
    "# next the Encoder-Decoder LSTM model mostly deals with seq2seq prediction problem\n",
    "# needs RepeatVector and TimeDistributed modules to solve if input sequences and output sequences have different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfl5UOOi-rAK"
   },
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-oXFUyg-rCj"
   },
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return array(X), array(y)\n",
    " \n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "y = y.reshape((y.shape[0], y.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MT1RQ1LOBHht"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(RepeatVector(n_steps_out)) #\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p1k1_3SXBEzL",
    "outputId": "d8339344-8082-440a-e8a6-68a5087a7fd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 5s - loss: 4442.2295\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4416.1948\n",
      "Epoch 3/100\n",
      " - 0s - loss: 4385.6416\n",
      "Epoch 4/100\n",
      " - 0s - loss: 4353.1714\n",
      "Epoch 5/100\n",
      " - 0s - loss: 4318.2632\n",
      "Epoch 6/100\n",
      " - 0s - loss: 4281.2539\n",
      "Epoch 7/100\n",
      " - 0s - loss: 4240.9360\n",
      "Epoch 8/100\n",
      " - 0s - loss: 4195.8311\n",
      "Epoch 9/100\n",
      " - 0s - loss: 4145.0088\n",
      "Epoch 10/100\n",
      " - 0s - loss: 4087.7573\n",
      "Epoch 11/100\n",
      " - 0s - loss: 4020.7168\n",
      "Epoch 12/100\n",
      " - 0s - loss: 3942.5259\n",
      "Epoch 13/100\n",
      " - 0s - loss: 3849.7769\n",
      "Epoch 14/100\n",
      " - 0s - loss: 3739.9780\n",
      "Epoch 15/100\n",
      " - 0s - loss: 3609.9292\n",
      "Epoch 16/100\n",
      " - 0s - loss: 3456.8008\n",
      "Epoch 17/100\n",
      " - 0s - loss: 3276.3635\n",
      "Epoch 18/100\n",
      " - 0s - loss: 3065.3640\n",
      "Epoch 19/100\n",
      " - 0s - loss: 2827.7183\n",
      "Epoch 20/100\n",
      " - 0s - loss: 2562.4644\n",
      "Epoch 21/100\n",
      " - 0s - loss: 2272.4658\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1967.7123\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1656.9746\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1350.7693\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1063.0745\n",
      "Epoch 26/100\n",
      " - 0s - loss: 824.0496\n",
      "Epoch 27/100\n",
      " - 0s - loss: 684.4745\n",
      "Epoch 28/100\n",
      " - 0s - loss: 691.9389\n",
      "Epoch 29/100\n",
      " - 0s - loss: 840.6693\n",
      "Epoch 30/100\n",
      " - 0s - loss: 963.7077\n",
      "Epoch 31/100\n",
      " - 0s - loss: 968.1523\n",
      "Epoch 32/100\n",
      " - 0s - loss: 885.3682\n",
      "Epoch 33/100\n",
      " - 0s - loss: 771.8922\n",
      "Epoch 34/100\n",
      " - 0s - loss: 669.3152\n",
      "Epoch 35/100\n",
      " - 0s - loss: 598.3488\n",
      "Epoch 36/100\n",
      " - 0s - loss: 560.6788\n",
      "Epoch 37/100\n",
      " - 0s - loss: 547.2717\n",
      "Epoch 38/100\n",
      " - 0s - loss: 548.2461\n",
      "Epoch 39/100\n",
      " - 0s - loss: 554.4508\n",
      "Epoch 40/100\n",
      " - 0s - loss: 558.9860\n",
      "Epoch 41/100\n",
      " - 0s - loss: 557.7957\n",
      "Epoch 42/100\n",
      " - 0s - loss: 548.4510\n",
      "Epoch 43/100\n",
      " - 0s - loss: 531.4225\n",
      "Epoch 44/100\n",
      " - 0s - loss: 508.4582\n",
      "Epoch 45/100\n",
      " - 0s - loss: 481.8693\n",
      "Epoch 46/100\n",
      " - 0s - loss: 455.7091\n",
      "Epoch 47/100\n",
      " - 0s - loss: 436.4088\n",
      "Epoch 48/100\n",
      " - 0s - loss: 430.4268\n",
      "Epoch 49/100\n",
      " - 0s - loss: 435.8895\n",
      "Epoch 50/100\n",
      " - 0s - loss: 436.9609\n",
      "Epoch 51/100\n",
      " - 0s - loss: 423.6660\n",
      "Epoch 52/100\n",
      " - 0s - loss: 403.2729\n",
      "Epoch 53/100\n",
      " - 0s - loss: 385.2637\n",
      "Epoch 54/100\n",
      " - 0s - loss: 373.6657\n",
      "Epoch 55/100\n",
      " - 0s - loss: 367.4237\n",
      "Epoch 56/100\n",
      " - 0s - loss: 363.4857\n",
      "Epoch 57/100\n",
      " - 0s - loss: 358.7206\n",
      "Epoch 58/100\n",
      " - 0s - loss: 351.4057\n",
      "Epoch 59/100\n",
      " - 0s - loss: 341.2496\n",
      "Epoch 60/100\n",
      " - 0s - loss: 328.9986\n",
      "Epoch 61/100\n",
      " - 0s - loss: 316.0810\n",
      "Epoch 62/100\n",
      " - 0s - loss: 304.6728\n",
      "Epoch 63/100\n",
      " - 0s - loss: 295.1460\n",
      "Epoch 64/100\n",
      " - 0s - loss: 287.3273\n",
      "Epoch 65/100\n",
      " - 0s - loss: 279.7948\n",
      "Epoch 66/100\n",
      " - 0s - loss: 270.4782\n",
      "Epoch 67/100\n",
      " - 0s - loss: 258.7773\n",
      "Epoch 68/100\n",
      " - 0s - loss: 244.7200\n",
      "Epoch 69/100\n",
      " - 0s - loss: 230.4067\n",
      "Epoch 70/100\n",
      " - 0s - loss: 216.4829\n",
      "Epoch 71/100\n",
      " - 0s - loss: 202.0615\n",
      "Epoch 72/100\n",
      " - 0s - loss: 186.9631\n",
      "Epoch 73/100\n",
      " - 0s - loss: 170.1835\n",
      "Epoch 74/100\n",
      " - 0s - loss: 151.3650\n",
      "Epoch 75/100\n",
      " - 0s - loss: 131.8889\n",
      "Epoch 76/100\n",
      " - 0s - loss: 116.0987\n",
      "Epoch 77/100\n",
      " - 0s - loss: 102.9474\n",
      "Epoch 78/100\n",
      " - 0s - loss: 88.8141\n",
      "Epoch 79/100\n",
      " - 0s - loss: 75.4669\n",
      "Epoch 80/100\n",
      " - 0s - loss: 63.1852\n",
      "Epoch 81/100\n",
      " - 0s - loss: 52.1088\n",
      "Epoch 82/100\n",
      " - 0s - loss: 41.1632\n",
      "Epoch 83/100\n",
      " - 0s - loss: 31.1788\n",
      "Epoch 84/100\n",
      " - 0s - loss: 22.4625\n",
      "Epoch 85/100\n",
      " - 0s - loss: 16.2297\n",
      "Epoch 86/100\n",
      " - 0s - loss: 12.0546\n",
      "Epoch 87/100\n",
      " - 0s - loss: 9.8999\n",
      "Epoch 88/100\n",
      " - 0s - loss: 10.6477\n",
      "Epoch 89/100\n",
      " - 0s - loss: 18.6825\n",
      "Epoch 90/100\n",
      " - 0s - loss: 10.6047\n",
      "Epoch 91/100\n",
      " - 0s - loss: 17.3789\n",
      "Epoch 92/100\n",
      " - 0s - loss: 13.7670\n",
      "Epoch 93/100\n",
      " - 0s - loss: 7.5301\n",
      "Epoch 94/100\n",
      " - 0s - loss: 9.7370\n",
      "Epoch 95/100\n",
      " - 0s - loss: 9.9307\n",
      "Epoch 96/100\n",
      " - 0s - loss: 5.4596\n",
      "Epoch 97/100\n",
      " - 0s - loss: 6.3173\n",
      "Epoch 98/100\n",
      " - 0s - loss: 8.2081\n",
      "Epoch 99/100\n",
      " - 0s - loss: 6.3085\n",
      "Epoch 100/100\n",
      " - 0s - loss: 4.2169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d0b8a1cc0>"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yEn_PWm1BE17",
    "outputId": "ef20e961-98a6-4ea7-a4e9-96094bdce318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[103.17423]\n",
      "  [125.56825]]]\n"
     ]
    }
   ],
   "source": [
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=2)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKy31nMz-Ysj"
   },
   "outputs": [],
   "source": [
    "# multivariate multi-step data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHagqW1fIa8p"
   },
   "outputs": [],
   "source": [
    "# generate input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "#print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SyXfgbPMImqw"
   },
   "outputs": [],
   "source": [
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxJrrO5xIqe6"
   },
   "outputs": [],
   "source": [
    "n_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-e9_mxBkIqju"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nIPSa1_qIqh_",
    "outputId": "fa202fb0-0600-4cb4-8377-d7e41fd862ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 5s - loss: 16794.9551\n",
      "Epoch 2/200\n",
      " - 0s - loss: 16585.8516\n",
      "Epoch 3/200\n",
      " - 0s - loss: 16397.5527\n",
      "Epoch 4/200\n",
      " - 0s - loss: 16205.6250\n",
      "Epoch 5/200\n",
      " - 0s - loss: 15988.0234\n",
      "Epoch 6/200\n",
      " - 0s - loss: 15738.9609\n",
      "Epoch 7/200\n",
      " - 0s - loss: 15450.4697\n",
      "Epoch 8/200\n",
      " - 0s - loss: 15110.4736\n",
      "Epoch 9/200\n",
      " - 0s - loss: 14705.7783\n",
      "Epoch 10/200\n",
      " - 0s - loss: 14236.9424\n",
      "Epoch 11/200\n",
      " - 0s - loss: 13679.0947\n",
      "Epoch 12/200\n",
      " - 0s - loss: 12996.7500\n",
      "Epoch 13/200\n",
      " - 0s - loss: 12183.6787\n",
      "Epoch 14/200\n",
      " - 0s - loss: 11213.1260\n",
      "Epoch 15/200\n",
      " - 0s - loss: 10115.4580\n",
      "Epoch 16/200\n",
      " - 0s - loss: 8845.4209\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7480.5337\n",
      "Epoch 18/200\n",
      " - 0s - loss: 6086.4731\n",
      "Epoch 19/200\n",
      " - 0s - loss: 4695.7754\n",
      "Epoch 20/200\n",
      " - 0s - loss: 3310.9316\n",
      "Epoch 21/200\n",
      " - 0s - loss: 2053.6101\n",
      "Epoch 22/200\n",
      " - 0s - loss: 1087.5088\n",
      "Epoch 23/200\n",
      " - 0s - loss: 519.7864\n",
      "Epoch 24/200\n",
      " - 0s - loss: 528.3585\n",
      "Epoch 25/200\n",
      " - 0s - loss: 1056.0919\n",
      "Epoch 26/200\n",
      " - 0s - loss: 1576.5482\n",
      "Epoch 27/200\n",
      " - 0s - loss: 1736.1703\n",
      "Epoch 28/200\n",
      " - 0s - loss: 1557.1029\n",
      "Epoch 29/200\n",
      " - 0s - loss: 1221.1129\n",
      "Epoch 30/200\n",
      " - 0s - loss: 867.8121\n",
      "Epoch 31/200\n",
      " - 0s - loss: 573.8219\n",
      "Epoch 32/200\n",
      " - 0s - loss: 370.7997\n",
      "Epoch 33/200\n",
      " - 0s - loss: 258.9225\n",
      "Epoch 34/200\n",
      " - 0s - loss: 219.3273\n",
      "Epoch 35/200\n",
      " - 0s - loss: 228.0885\n",
      "Epoch 36/200\n",
      " - 0s - loss: 259.3613\n",
      "Epoch 37/200\n",
      " - 0s - loss: 287.5715\n",
      "Epoch 38/200\n",
      " - 0s - loss: 295.1494\n",
      "Epoch 39/200\n",
      " - 0s - loss: 276.2691\n",
      "Epoch 40/200\n",
      " - 0s - loss: 230.7421\n",
      "Epoch 41/200\n",
      " - 0s - loss: 163.6547\n",
      "Epoch 42/200\n",
      " - 0s - loss: 91.7682\n",
      "Epoch 43/200\n",
      " - 0s - loss: 39.1967\n",
      "Epoch 44/200\n",
      " - 0s - loss: 42.9066\n",
      "Epoch 45/200\n",
      " - 0s - loss: 100.3506\n",
      "Epoch 46/200\n",
      " - 0s - loss: 108.7812\n",
      "Epoch 47/200\n",
      " - 0s - loss: 73.5148\n",
      "Epoch 48/200\n",
      " - 0s - loss: 36.6473\n",
      "Epoch 49/200\n",
      " - 0s - loss: 21.9327\n",
      "Epoch 50/200\n",
      " - 0s - loss: 26.3064\n",
      "Epoch 51/200\n",
      " - 0s - loss: 37.8935\n",
      "Epoch 52/200\n",
      " - 0s - loss: 46.4408\n",
      "Epoch 53/200\n",
      " - 0s - loss: 46.4214\n",
      "Epoch 54/200\n",
      " - 0s - loss: 38.8026\n",
      "Epoch 55/200\n",
      " - 0s - loss: 27.1277\n",
      "Epoch 56/200\n",
      " - 0s - loss: 16.4900\n",
      "Epoch 57/200\n",
      " - 0s - loss: 11.1304\n",
      "Epoch 58/200\n",
      " - 0s - loss: 12.7375\n",
      "Epoch 59/200\n",
      " - 0s - loss: 18.8593\n",
      "Epoch 60/200\n",
      " - 0s - loss: 23.0677\n",
      "Epoch 61/200\n",
      " - 0s - loss: 21.1217\n",
      "Epoch 62/200\n",
      " - 0s - loss: 14.4467\n",
      "Epoch 63/200\n",
      " - 0s - loss: 8.0113\n",
      "Epoch 64/200\n",
      " - 0s - loss: 5.2292\n",
      "Epoch 65/200\n",
      " - 0s - loss: 5.9427\n",
      "Epoch 66/200\n",
      " - 0s - loss: 8.0632\n",
      "Epoch 67/200\n",
      " - 0s - loss: 9.3101\n",
      "Epoch 68/200\n",
      " - 0s - loss: 8.5196\n",
      "Epoch 69/200\n",
      " - 0s - loss: 6.0666\n",
      "Epoch 70/200\n",
      " - 0s - loss: 3.2813\n",
      "Epoch 71/200\n",
      " - 0s - loss: 2.1648\n",
      "Epoch 72/200\n",
      " - 0s - loss: 3.3280\n",
      "Epoch 73/200\n",
      " - 0s - loss: 4.7900\n",
      "Epoch 74/200\n",
      " - 0s - loss: 4.3150\n",
      "Epoch 75/200\n",
      " - 0s - loss: 2.3623\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.8689\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.9216\n",
      "Epoch 78/200\n",
      " - 0s - loss: 1.9943\n",
      "Epoch 79/200\n",
      " - 0s - loss: 2.8157\n",
      "Epoch 80/200\n",
      " - 0s - loss: 2.6488\n",
      "Epoch 81/200\n",
      " - 0s - loss: 1.6905\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.7246\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.5573\n",
      "Epoch 84/200\n",
      " - 0s - loss: 1.0742\n",
      "Epoch 85/200\n",
      " - 0s - loss: 1.3583\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.9931\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.4116\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.2496\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.5514\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.8376\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.7420\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.3763\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.1382\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.2242\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.4397\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.4816\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.3140\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.1579\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.1698\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.2655\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.2792\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.1846\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.0978\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.1102\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.1776\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.1929\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.1290\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.0613\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.0585\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.0989\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.1134\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.0790\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.0383\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.0343\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.0568\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.0650\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.0464\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.0271\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.0293\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.0422\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.0422\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.0261\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.0129\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.0158\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.0259\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.0270\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.0180\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.0114\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.0137\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.0185\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.0173\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.0118\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.0091\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.0116\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.0140\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.0122\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.0086\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.0081\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.0105\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.0118\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.0102\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.0080\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.0079\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.0090\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.0091\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.0078\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.0069\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.0075\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.0081\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.0066\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.0065\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.0066\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.0064\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.0063\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.0059\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.0060\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.0060\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.0057\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.0056\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.0056\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.0053\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.0053\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.0053\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.0053\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.0050\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.0048\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.0048\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.0048\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.0045\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.0041\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.0041\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.0039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d0b2653c8>"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eGI1rK2qIa5r",
    "outputId": "e5e2773f-5401-4118-ce90-93fb2f339966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[186.3042  207.68997]]\n"
     ]
    }
   ],
   "source": [
    "x_input = array([[70, 75], [80, 85], [90, 95]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRUZDec7JGK0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "simple LSTM practice",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
