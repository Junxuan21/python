{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the fifth part in the data competition for predicting used car prices. This notebook will focus on the stacking and ensemble models and their respective performances regarding the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- simple example of averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random dataset\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "y_test_true = [1, 3, 2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple weighted average function\n",
    "def weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]):\n",
    "    weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n",
    "    return weighted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred1 MAE: 0.1750000000000001\n",
      "Pred2 MAE: 0.07499999999999993\n",
      "Pred3 MAE: 0.10000000000000009\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Pred1 MAE:',metrics.mean_absolute_error(y_test_true, test_pre1))\n",
    "print('Pred2 MAE:',metrics.mean_absolute_error(y_test_true, test_pre2))\n",
    "print('Pred3 MAE:',metrics.mean_absolute_error(y_test_true, test_pre3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_test MAE: 0.05750000000000027\n"
     ]
    }
   ],
   "source": [
    "w = [0.3,0.4,0.3] # init weights\n",
    "weighted_test = weighted_method(test_pre1, test_pre2, test_pre3, w)\n",
    "print('Weighted_test MAE:', metrics.mean_absolute_error(y_test_true, weighted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## similarly, we can define weighted mean/median function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_method(test_pre1,test_pre2,test_pre3):\n",
    "    mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).mean(axis=1)\n",
    "    return mean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_method(test_pre1,test_pre2,test_pre3):\n",
    "    median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1)\n",
    "    return median_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_test MAE: 0.06666666666666693\n"
     ]
    }
   ],
   "source": [
    "mean_test = mean_method(test_pre1,test_pre2,test_pre3)\n",
    "print('Mean_test MAE:', metrics.mean_absolute_error(y_test_true, mean_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median_pre MAE: 0.07500000000000007\n"
     ]
    }
   ],
   "source": [
    "median_test = median_method(test_pre1,test_pre2,test_pre3)\n",
    "print('Median_pre MAE:', metrics.mean_absolute_error(y_test_true, median_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use sklearn to build a simple stacking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def stacking_method(train_reg1, train_reg2, train_reg3, y_train_true, \n",
    "                    test_pre1, test_pre2, test_pre3, \n",
    "                    model_L2 = linear_model.LinearRegression()):\n",
    "    \n",
    "    model_L2.fit(pd.concat([pd.Series(train_reg1), pd.Series(train_reg2), \n",
    "                            pd.Series(train_reg3)], axis=1).values,y_train_true)\n",
    "    stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1), pd.Series(test_pre2),\n",
    "                                                  pd.Series(test_pre3)], axis=1).values)\n",
    "    return stacking_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reg1 = [3.2, 8.2, 9.1, 5.2]\n",
    "train_reg2 = [2.9, 8.1, 9.0, 4.9]\n",
    "train_reg3 = [3.1, 7.9, 9.2, 5.0]\n",
    "\n",
    "y_train_true = [3, 8, 9, 5] \n",
    "\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "y_test_true = [1, 3, 2, 6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking_pre MAE: 0.04213483146067476\n"
     ]
    }
   ],
   "source": [
    "model_L2= linear_model.LinearRegression()\n",
    "stacking_test = stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,\n",
    "                               test_pre1, test_pre2, test_pre3, model_L2)\n",
    "\n",
    "print('Stacking_pre MAE:',metrics.mean_absolute_error(y_test_true, stacking_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of ensemble, such as the voting classifier as well as advanced ensemble models,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "#clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.7,\n",
    "#                     colsample_bytree=0.6, objective='binary:logistic')\n",
    "clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n",
    "                              min_samples_leaf=63,oob_score=True)\n",
    "clf3 = SVC(C=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.95 (+/- 0.03) [SVM]\n",
      "Accuracy: 0.65 (+/- 0.18) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "# hard voting system \n",
    "eclf = VotingClassifier(estimators=[ ('rf', clf2), ('svc', clf3)], voting='hard')\n",
    "\n",
    "\n",
    "for clf, label in zip([clf2, clf3, eclf], ['Random Forest', 'SVM', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.95 (+/- 0.03) [SVM]\n",
      "Accuracy: 0.95 (+/- 0.03) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "# soft voting\n",
    "clf3 = SVC(C=0.1, probability=True)\n",
    "eclf = VotingClassifier(estimators=[('rf', clf2), ('svc', clf3)], voting='soft', weights=[1,2])\n",
    "\n",
    "for clf, label in zip([clf2, clf3, eclf], ['Random Forest', 'SVM', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare two ways: Stacking & Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val auc Score: 1.000000\n",
      "val auc Score: 0.500000\n",
      "val auc Score: 0.500000\n",
      "val auc Score: 0.500000\n",
      "val auc Score: 0.500000\n",
      "Val auc Score of Stacking: 1.000000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "5-Fold Stacking\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier\n",
    "\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    "\n",
    "clfs = [LogisticRegression(solver='lbfgs'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    " \n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits)\n",
    "skf = skf.split(X, y)\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    \n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], 5))\n",
    "    \n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        # 5-fold cross training, use ith as prediction, \n",
    "        # train with the rest, output as new features to ith  \n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
    "    \n",
    "    # for testing, use the pred of k models as new feature\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(dataset_blend_train, y)\n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "print(\"Val auc Score of Stacking: %f\" % (roc_auc_score(y_predict, y_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "Val auc Score of Blending: 1.000000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Blending\n",
    "'''\n",
    " \n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    " \n",
    "clfs = [LogisticRegression(solver='lbfgs'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        #ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    " \n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n",
    "\n",
    "# split training set into d1, d2\n",
    "X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2020)\n",
    "dataset_d1 = np.zeros((X_d2.shape[0], len(clfs)))\n",
    "dataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))\n",
    " \n",
    "for j, clf in enumerate(clfs):\n",
    "    clf.fit(X_d1, y_d1)\n",
    "    y_submission = clf.predict_proba(X_d2)[:, 1]\n",
    "    dataset_d1[:, j] = y_submission\n",
    "    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1]\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j]))\n",
    "\n",
    "clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(dataset_d1, y_d2)\n",
    "\n",
    "y_submission = clf.predict_proba(dataset_d2)[:, 1]\n",
    "print(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we can also put all features into the model to make predictions, and transform the predicted results and add them back as new features, and then pass to the model to get new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ensemble_add_feature(train,test,target,clfs):\n",
    "    \n",
    "    # n_flods = 5\n",
    "    # skf = list(StratifiedKFold(y, n_folds=n_flods))\n",
    "\n",
    "    train_ = np.zeros((train.shape[0],len(clfs*2)))\n",
    "    test_ = np.zeros((test.shape[0],len(clfs*2)))\n",
    "\n",
    "    for j,clf in enumerate(clfs):\n",
    "\n",
    "        # print(j, clf)\n",
    "        # X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "\n",
    "        clf.fit(train,target)\n",
    "        y_train = clf.predict(train)\n",
    "        y_test = clf.predict(test)\n",
    "\n",
    "        ## create new features\n",
    "        train_[:,j*2] = y_train**2\n",
    "        test_[:,j*2] = y_test**2\n",
    "        train_[:, j+1] = np.exp(y_train)\n",
    "        test_[:, j+1] = np.exp(y_test)\n",
    "        # print(\"val auc Score: %f\" % r2_score(y_predict, dataset_d2[:, j]))\n",
    "        print('Method ',j)\n",
    "    \n",
    "    train_ = pd.DataFrame(train_)\n",
    "    test_ = pd.DataFrame(test_)\n",
    "    return train_,test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  0\n",
      "Method  1\n",
      "Method  2\n",
      "Method  3\n",
      "Method  4\n",
      "Val auc Score of stacking: 1.000000\n"
     ]
    }
   ],
   "source": [
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3)\n",
    "x_train = pd.DataFrame(x_train) ; x_test = pd.DataFrame(x_test)\n",
    "\n",
    "clfs = [LogisticRegression(),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    "\n",
    "New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs)\n",
    "\n",
    "# clf = LogisticRegression()\n",
    "clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(New_train, y_train)\n",
    "y_emb = clf.predict_proba(New_test)[:, 1]\n",
    "\n",
    "print(\"Val auc Score of stacking: %f\" % (roc_auc_score(y_test, y_emb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 31)\n",
      "(50000, 30)\n"
     ]
    }
   ],
   "source": [
    "# load the used car dataset\n",
    "Train_data = pd.read_csv('./data/used_car_train_20200313.csv', sep=' ')\n",
    "Test_data = pd.read_csv('./data/used_car_testA_20200313.csv', sep=' ')\n",
    "\n",
    "print(Train_data.shape)\n",
    "print(Test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SaleID', 'power', 'kilometer', 'price', 'v_0', 'v_1', 'v_2', 'v_3',\n",
      "       'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12',\n",
      "       'v_13', 'v_14', 'miss_info', 'used_time', 'notRepairedDamage_0.0',\n",
      "       'notRepairedDamage_1.0', 'notRepairedDamage_nan', 'fuelType_0.0',\n",
      "       'fuelType_1.0', 'fuelType_2.0', 'fuelType_3.0', 'fuelType_4.0',\n",
      "       'fuelType_5.0', 'fuelType_6.0', 'fuelType_nan', 'gearbox_0.0',\n",
      "       'gearbox_1.0', 'gearbox_nan', 'bodyType_0.0', 'bodyType_1.0',\n",
      "       'bodyType_2.0', 'bodyType_3.0', 'bodyType_4.0', 'bodyType_5.0',\n",
      "       'bodyType_6.0', 'bodyType_7.0', 'bodyType_nan'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = Train_data.select_dtypes(exclude = 'object').columns\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14', 'miss_info', 'used_time', 'notRepairedDamage_0.0', 'notRepairedDamage_1.0', 'notRepairedDamage_nan', 'fuelType_0.0', 'fuelType_1.0', 'fuelType_2.0', 'fuelType_3.0', 'fuelType_4.0', 'fuelType_5.0', 'fuelType_6.0', 'fuelType_nan', 'gearbox_0.0', 'gearbox_1.0', 'gearbox_nan', 'bodyType_0.0', 'bodyType_1.0', 'bodyType_2.0', 'bodyType_3.0', 'bodyType_4.0', 'bodyType_5.0', 'bodyType_6.0', 'bodyType_7.0', 'bodyType_nan']\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in numerical_cols if col not in ['SaleID','name','regDate','price']]\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(df):\n",
    "\n",
    "    Output = df.copy()\n",
    "\n",
    "    category_cols = ['brand', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage']\n",
    "    Output.replace('-', np.nan, inplace=True)\n",
    "    Output.replace({'Not Available': np.nan}, inplace=True)\n",
    "    Output['miss_info'] = Output.isnull().sum(axis=1)\n",
    "\n",
    "    for col in list(Output.columns):\n",
    "        if ('notRepairedDamage' in col):\n",
    "            Output[col] = Output[col].astype(float)\n",
    "    \n",
    "    Output['used_time'] = (pd.to_datetime(Output['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                            pd.to_datetime(Output['regDate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "    \n",
    "    \n",
    "    data_notRepairedDamage = pd.get_dummies(Output['notRepairedDamage'], prefix='notRepairedDamage', dummy_na=True)\n",
    "    data_fuelType = pd.get_dummies(Output['fuelType'], prefix='fuelType', dummy_na=True)\n",
    "    data_gearbox = pd.get_dummies(Output['gearbox'], prefix='gearbox', dummy_na=True)\n",
    "    data_bodyType = pd.get_dummies(Output['bodyType'], prefix='bodyType', dummy_na=True)\n",
    "    \n",
    "    Output = pd.concat([Output, data_notRepairedDamage, data_fuelType, data_gearbox, data_bodyType], axis=1)\n",
    "    \n",
    "    del Output[\"seller\"]\n",
    "    del Output[\"offerType\"]\n",
    "    del Output['name']\n",
    "    del Output['model']\n",
    "    del Output['regionCode']\n",
    "    del Output['regDate']\n",
    "    del Output['brand']\n",
    "    del Output['bodyType']\n",
    "    del Output['fuelType']\n",
    "    del Output['gearbox']\n",
    "    del Output['notRepairedDamage']\n",
    "    del Output['creatDate']\n",
    "    \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (150000, 44)\n",
      "Test data shape: (50000, 43)\n"
     ]
    }
   ],
   "source": [
    "Train_data = data_processing(Train_data)\n",
    "Test_data = data_processing(Test_data)\n",
    "\n",
    "print('Train data shape:',Train_data.shape)\n",
    "print('Test data shape:',Test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 44 columns):\n",
      "SaleID                   150000 non-null int64\n",
      "power                    150000 non-null int64\n",
      "kilometer                150000 non-null float64\n",
      "price                    150000 non-null int64\n",
      "v_0                      150000 non-null float64\n",
      "v_1                      150000 non-null float64\n",
      "v_2                      150000 non-null float64\n",
      "v_3                      150000 non-null float64\n",
      "v_4                      150000 non-null float64\n",
      "v_5                      150000 non-null float64\n",
      "v_6                      150000 non-null float64\n",
      "v_7                      150000 non-null float64\n",
      "v_8                      150000 non-null float64\n",
      "v_9                      150000 non-null float64\n",
      "v_10                     150000 non-null float64\n",
      "v_11                     150000 non-null float64\n",
      "v_12                     150000 non-null float64\n",
      "v_13                     150000 non-null float64\n",
      "v_14                     150000 non-null float64\n",
      "miss_info                150000 non-null int64\n",
      "used_time                138653 non-null float64\n",
      "notRepairedDamage_0.0    150000 non-null uint8\n",
      "notRepairedDamage_1.0    150000 non-null uint8\n",
      "notRepairedDamage_nan    150000 non-null uint8\n",
      "fuelType_0.0             150000 non-null uint8\n",
      "fuelType_1.0             150000 non-null uint8\n",
      "fuelType_2.0             150000 non-null uint8\n",
      "fuelType_3.0             150000 non-null uint8\n",
      "fuelType_4.0             150000 non-null uint8\n",
      "fuelType_5.0             150000 non-null uint8\n",
      "fuelType_6.0             150000 non-null uint8\n",
      "fuelType_nan             150000 non-null uint8\n",
      "gearbox_0.0              150000 non-null uint8\n",
      "gearbox_1.0              150000 non-null uint8\n",
      "gearbox_nan              150000 non-null uint8\n",
      "bodyType_0.0             150000 non-null uint8\n",
      "bodyType_1.0             150000 non-null uint8\n",
      "bodyType_2.0             150000 non-null uint8\n",
      "bodyType_3.0             150000 non-null uint8\n",
      "bodyType_4.0             150000 non-null uint8\n",
      "bodyType_5.0             150000 non-null uint8\n",
      "bodyType_6.0             150000 non-null uint8\n",
      "bodyType_7.0             150000 non-null uint8\n",
      "bodyType_nan             150000 non-null uint8\n",
      "dtypes: float64(17), int64(4), uint8(23)\n",
      "memory usage: 27.3 MB\n"
     ]
    }
   ],
   "source": [
    "Train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>price</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>...</th>\n",
       "      <th>gearbox_nan</th>\n",
       "      <th>bodyType_0.0</th>\n",
       "      <th>bodyType_1.0</th>\n",
       "      <th>bodyType_2.0</th>\n",
       "      <th>bodyType_3.0</th>\n",
       "      <th>bodyType_4.0</th>\n",
       "      <th>bodyType_5.0</th>\n",
       "      <th>bodyType_6.0</th>\n",
       "      <th>bodyType_7.0</th>\n",
       "      <th>bodyType_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1850</td>\n",
       "      <td>43.357796</td>\n",
       "      <td>3.966344</td>\n",
       "      <td>0.050257</td>\n",
       "      <td>2.159744</td>\n",
       "      <td>1.143786</td>\n",
       "      <td>0.235676</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3600</td>\n",
       "      <td>45.305273</td>\n",
       "      <td>5.236112</td>\n",
       "      <td>0.137925</td>\n",
       "      <td>1.380657</td>\n",
       "      <td>-1.422165</td>\n",
       "      <td>0.264777</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6222</td>\n",
       "      <td>45.978359</td>\n",
       "      <td>4.823792</td>\n",
       "      <td>1.319524</td>\n",
       "      <td>-0.998467</td>\n",
       "      <td>-0.996911</td>\n",
       "      <td>0.251410</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>193</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2400</td>\n",
       "      <td>45.687478</td>\n",
       "      <td>4.492574</td>\n",
       "      <td>-0.050616</td>\n",
       "      <td>0.883600</td>\n",
       "      <td>-2.228079</td>\n",
       "      <td>0.274293</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5200</td>\n",
       "      <td>44.383511</td>\n",
       "      <td>2.031433</td>\n",
       "      <td>0.572169</td>\n",
       "      <td>-1.571239</td>\n",
       "      <td>2.246088</td>\n",
       "      <td>0.228036</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SaleID  power  kilometer  price        v_0       v_1       v_2       v_3  \\\n",
       "0       0     60       12.5   1850  43.357796  3.966344  0.050257  2.159744   \n",
       "1       1      0       15.0   3600  45.305273  5.236112  0.137925  1.380657   \n",
       "2       2    163       12.5   6222  45.978359  4.823792  1.319524 -0.998467   \n",
       "3       3    193       15.0   2400  45.687478  4.492574 -0.050616  0.883600   \n",
       "4       4     68        5.0   5200  44.383511  2.031433  0.572169 -1.571239   \n",
       "\n",
       "        v_4       v_5  ...  gearbox_nan  bodyType_0.0  bodyType_1.0  \\\n",
       "0  1.143786  0.235676  ...            0             0             1   \n",
       "1 -1.422165  0.264777  ...            0             0             0   \n",
       "2 -0.996911  0.251410  ...            0             0             1   \n",
       "3 -2.228079  0.274293  ...            0             1             0   \n",
       "4  2.246088  0.228036  ...            0             0             1   \n",
       "\n",
       "   bodyType_2.0  bodyType_3.0  bodyType_4.0  bodyType_5.0  bodyType_6.0  \\\n",
       "0             0             0             0             0             0   \n",
       "1             1             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   bodyType_7.0  bodyType_nan  \n",
       "0             0             0  \n",
       "1             0             0  \n",
       "2             0             0  \n",
       "3             0             0  \n",
       "4             0             0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SaleID', 'power', 'kilometer', 'price', 'v_0', 'v_1', 'v_2', 'v_3',\n",
       "       'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12',\n",
       "       'v_13', 'v_14', 'miss_info', 'used_time', 'notRepairedDamage_0.0',\n",
       "       'notRepairedDamage_1.0', 'notRepairedDamage_nan', 'fuelType_0.0',\n",
       "       'fuelType_1.0', 'fuelType_2.0', 'fuelType_3.0', 'fuelType_4.0',\n",
       "       'fuelType_5.0', 'fuelType_6.0', 'fuelType_nan', 'gearbox_0.0',\n",
       "       'gearbox_1.0', 'gearbox_nan', 'bodyType_0.0', 'bodyType_1.0',\n",
       "       'bodyType_2.0', 'bodyType_3.0', 'bodyType_4.0', 'bodyType_5.0',\n",
       "       'bodyType_6.0', 'bodyType_7.0', 'bodyType_nan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SaleID', 'power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4',\n",
       "       'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13',\n",
       "       'v_14', 'miss_info', 'used_time', 'notRepairedDamage_0.0',\n",
       "       'notRepairedDamage_1.0', 'notRepairedDamage_nan', 'fuelType_0.0',\n",
       "       'fuelType_1.0', 'fuelType_2.0', 'fuelType_3.0', 'fuelType_4.0',\n",
       "       'fuelType_5.0', 'fuelType_6.0', 'fuelType_nan', 'gearbox_0.0',\n",
       "       'gearbox_1.0', 'gearbox_nan', 'bodyType_0.0', 'bodyType_1.0',\n",
       "       'bodyType_2.0', 'bodyType_3.0', 'bodyType_4.0', 'bodyType_5.0',\n",
       "       'bodyType_6.0', 'bodyType_7.0', 'bodyType_nan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in numerical_cols if col not in ['SaleID','name','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = Train_data[feature_cols]\n",
    "Y_data = Train_data['price']\n",
    "\n",
    "X_test  = Test_data[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>...</th>\n",
       "      <th>gearbox_nan</th>\n",
       "      <th>bodyType_0.0</th>\n",
       "      <th>bodyType_1.0</th>\n",
       "      <th>bodyType_2.0</th>\n",
       "      <th>bodyType_3.0</th>\n",
       "      <th>bodyType_4.0</th>\n",
       "      <th>bodyType_5.0</th>\n",
       "      <th>bodyType_6.0</th>\n",
       "      <th>bodyType_7.0</th>\n",
       "      <th>bodyType_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>12.5</td>\n",
       "      <td>43.357796</td>\n",
       "      <td>3.966344</td>\n",
       "      <td>0.050257</td>\n",
       "      <td>2.159744</td>\n",
       "      <td>1.143786</td>\n",
       "      <td>0.235676</td>\n",
       "      <td>0.101988</td>\n",
       "      <td>0.129549</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>45.305273</td>\n",
       "      <td>5.236112</td>\n",
       "      <td>0.137925</td>\n",
       "      <td>1.380657</td>\n",
       "      <td>-1.422165</td>\n",
       "      <td>0.264777</td>\n",
       "      <td>0.121004</td>\n",
       "      <td>0.135731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>12.5</td>\n",
       "      <td>45.978359</td>\n",
       "      <td>4.823792</td>\n",
       "      <td>1.319524</td>\n",
       "      <td>-0.998467</td>\n",
       "      <td>-0.996911</td>\n",
       "      <td>0.251410</td>\n",
       "      <td>0.114912</td>\n",
       "      <td>0.165147</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193</td>\n",
       "      <td>15.0</td>\n",
       "      <td>45.687478</td>\n",
       "      <td>4.492574</td>\n",
       "      <td>-0.050616</td>\n",
       "      <td>0.883600</td>\n",
       "      <td>-2.228079</td>\n",
       "      <td>0.274293</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.121964</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.383511</td>\n",
       "      <td>2.031433</td>\n",
       "      <td>0.572169</td>\n",
       "      <td>-1.571239</td>\n",
       "      <td>2.246088</td>\n",
       "      <td>0.228036</td>\n",
       "      <td>0.073205</td>\n",
       "      <td>0.091880</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   power  kilometer        v_0       v_1       v_2       v_3       v_4  \\\n",
       "0     60       12.5  43.357796  3.966344  0.050257  2.159744  1.143786   \n",
       "1      0       15.0  45.305273  5.236112  0.137925  1.380657 -1.422165   \n",
       "2    163       12.5  45.978359  4.823792  1.319524 -0.998467 -0.996911   \n",
       "3    193       15.0  45.687478  4.492574 -0.050616  0.883600 -2.228079   \n",
       "4     68        5.0  44.383511  2.031433  0.572169 -1.571239  2.246088   \n",
       "\n",
       "        v_5       v_6       v_7  ...  gearbox_nan  bodyType_0.0  bodyType_1.0  \\\n",
       "0  0.235676  0.101988  0.129549  ...            0             0             1   \n",
       "1  0.264777  0.121004  0.135731  ...            0             0             0   \n",
       "2  0.251410  0.114912  0.165147  ...            0             0             1   \n",
       "3  0.274293  0.110300  0.121964  ...            0             1             0   \n",
       "4  0.228036  0.073205  0.091880  ...            0             0             1   \n",
       "\n",
       "   bodyType_2.0  bodyType_3.0  bodyType_4.0  bodyType_5.0  bodyType_6.0  \\\n",
       "0             0             0             0             0             0   \n",
       "1             1             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   bodyType_7.0  bodyType_nan  \n",
       "0             0             0  \n",
       "1             0             0  \n",
       "2             0             0  \n",
       "3             0             0  \n",
       "4             0             0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1850\n",
       "1    3600\n",
       "2    6222\n",
       "3    2400\n",
       "4    5200\n",
       "Name: price, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(X_data, Y_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To constuct individual model, here in this notebook I just used several basic models ranging from linear models to nonlinear models, perhaps try catboost or other combinations in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lr(x_train,y_train):\n",
    "    reg_model = linear_model.LinearRegression()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_ridge(x_train,y_train):\n",
    "    reg_model = linear_model.Ridge(alpha=0.8) #alphas=range(1,100,5)\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_lasso(x_train,y_train):\n",
    "    reg_model = linear_model.LassoCV()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_gbdt(x_train,y_train):\n",
    "    estimator =GradientBoostingRegressor(loss='ls',subsample= 0.85,max_depth= 5,n_estimators = 100)\n",
    "    param_grid = { \n",
    "            'learning_rate': [0.05,0.08,0.1,0.2],\n",
    "            }\n",
    "    gbdt = GridSearchCV(estimator, param_grid,cv=3)\n",
    "    gbdt.fit(x_train,y_train)\n",
    "    print(gbdt.best_params_)\n",
    "    # print(gbdt.best_estimator_ )\n",
    "    return gbdt\n",
    "\n",
    "def build_model_xgb(x_train,y_train):\n",
    "    model = xgb.XGBRegressor(n_estimators=120, learning_rate=0.08, gamma=0, subsample=0.8,\\\n",
    "        colsample_bytree=0.9, max_depth=5) #, objective ='reg:squarederror'\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "def build_model_lgb(x_train,y_train):\n",
    "    estimator = lgb.LGBMRegressor(num_leaves=63,n_estimators = 100)\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "    }\n",
    "    gbm = GridSearchCV(estimator, param_grid)\n",
    "    gbm.fit(x_train, y_train)\n",
    "    return gbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xgb & 5fold cross validation\n",
    "xgr = xgb.XGBRegressor(n_estimators=120, learning_rate=0.1, subsample=0.8,\\\n",
    "        colsample_bytree=0.9, max_depth=7) # ,objective ='reg:squarederror'\n",
    "\n",
    "scores_train = []\n",
    "scores = []\n",
    "\n",
    "sk = StratifiedKFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "for train_ind,val_ind in sk.split(X_data,Y_data):\n",
    "    \n",
    "    train_x=X_data.iloc[train_ind].values\n",
    "    train_y=Y_data.iloc[train_ind]\n",
    "    val_x=X_data.iloc[val_ind].values\n",
    "    val_y=Y_data.iloc[val_ind]\n",
    "    \n",
    "    xgr.fit(train_x,train_y)\n",
    "    pred_train_xgb=xgr.predict(train_x)\n",
    "    pred_xgb=xgr.predict(val_x)\n",
    "    \n",
    "    score_train = mean_absolute_error(train_y,pred_train_xgb)\n",
    "    scores_train.append(score_train)\n",
    "    score = mean_absolute_error(val_y,pred_xgb)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Train mae:',np.mean(score_train))\n",
    "print('Val mae',np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model training \n",
    "\n",
    "print('Predict LR...')\n",
    "model_lr = build_model_lr(x_train,y_train)\n",
    "val_lr = model_lr.predict(x_val)\n",
    "subA_lr = model_lr.predict(X_test)\n",
    "\n",
    "print('Predict Ridge...')\n",
    "model_ridge = build_model_ridge(x_train,y_train)\n",
    "val_ridge = model_ridge.predict(x_val)\n",
    "subA_ridge = model_ridge.predict(X_test)\n",
    "\n",
    "print('Predict Lasso...')\n",
    "model_lasso = build_model_lasso(x_train,y_train)\n",
    "val_lasso = model_lasso.predict(x_val)\n",
    "subA_lasso = model_lasso.predict(X_test)\n",
    "\n",
    "print('Predict GBDT...')\n",
    "model_gbdt = build_model_gbdt(x_train,y_train)\n",
    "val_gbdt = model_gbdt.predict(x_val)\n",
    "subA_gbdt = model_gbdt.predict(X_test)\n",
    "\n",
    "\n",
    "print('predict XGB...')\n",
    "model_xgb = build_model_xgb(x_train,y_train)\n",
    "val_xgb = model_xgb.predict(x_val)\n",
    "subA_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "print('predict lgb...')\n",
    "model_lgb = build_model_lgb(x_train,y_train)\n",
    "val_lgb = model_lgb.predict(x_val)\n",
    "subA_lgb = model_lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with weighted averaging\n",
    "def weighted_method(test_pre1, test_pre2, test_pre3, w=[1/3,1/3,1/3]):\n",
    "    weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n",
    "    return weighted_result\n",
    "\n",
    "w = [0.3,0.4,0.3]\n",
    "\n",
    "# test accuracy on validation set\n",
    "val_pre = weighted_method(val_lgb, val_xgb, val_gbdt, w)\n",
    "MAE_Weighted = mean_absolute_error(y_val,val_pre)\n",
    "print('MAE of Weighted of val:',MAE_Weighted)\n",
    "\n",
    "# test on the predictions\n",
    "subA = weighted_method(subA_lgb, subA_xgb, subA_gbdt, w)\n",
    "print('Sta inf:')\n",
    "Sta_inf(subA)\n",
    "\n",
    "# save output\n",
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] = X_test.index\n",
    "sub['price'] = subA\n",
    "sub.to_csv('./sub_Weighted.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## similarly, we can also use stacking/blending to combine the predicted results together \n",
    "## to get the final output to have a better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
